[
  {
    "objectID": "TMDB_scraper/hw2blog.html",
    "href": "TMDB_scraper/hw2blog.html",
    "title": "HW 2, Web Scarpping",
    "section": "",
    "text": "In this blog we will go through on how to scrap a website. In this example we will formulate a list of all the actors who acted in the first Hunger Game movie and create a data fram with all the other movies in which they acted in to help recomand new movies to others.\n\n\nLocate the Url that you would be scrapping information from. For this blog we will use this website https://www.themoviedb.org/ and will scarpe a list of actors from the Hunger Game to create recomandtion system that gives users a list of the other movies the actors played in. \nWe First want to make sure that we are in the right enviroment and create a TMDB_scraper folder with all the files we will need by running the following lines in your terminal.\n\nPIC16B-24W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\n\n\n\n\n#### Part 1 We create a new file inside the spider directory and named it tmdb_spider.py and run the following lines\nTo run the code in the terminal we need to run this line with the correct added subdir for the movie page we chose\n“scrapy crawl tmdb_spider -o movies.csv -a subdir=”\n\n\nNext we will create and implemnt three diffrent parsing methods to get the url for each actor page and then to scrape their page and collect a list of all the movies they have acted in\nOnce we have create the intial function to be able to explore the movie we chose for we need to add the correct subdir .\nWhen going to the hunger game movie page the url is as follow : https://www.themoviedb.org/movie/70160-the-hunger-games When running the code in our terminal we can also just write 70160 for the subdir instead of also inclduing the movie name\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n    '''\n     Initialize the TmdbSpider instance.\n     '''\n    \n\nIn this parse method we create a way to navigate to our cast and crew page by adding /cast to our orginal url. This can be more easily visualize by going to the movie webiste. Once you are there select the full cast and crew page and you will get redirected to a new page. If you look at the url you would notice that it now changed to :https://www.themoviedb.org/movie/70160-the-hunger-games/cast\n\ndef parse(self, response):\n    ''''\n    We start on a movie page, and then  navigate to the Full Cast & Crew page\n    ''''\n            # Navigate to the Full Cast & Crew page\n            full_cast_url = f\"{response.url + '/cast/'}\"\n            # Yield a scrapy.Request for the page of each actor listed on the Full Cast & Crew page\n            yield scrapy.Request(full_cast_url, callback=self.parse_full_credits)\n            \n            \n        \n  \n\nNow that we are at the full cast and crew page the nect step is to go to the actors main page where we can get a list of all the Movies and TV shows they acted in. In order to understand how to find all the information we need we need to inspect the webpage we are at. When we inspect it we are able to use css or xpath to explore and navigate to the right urls. To locate the cast members we will use css to search within the h3 aspect. We will then go through the list of actors by examining ol. To get the actors url we ill examine the href attribute under the div class info\n\n  def parse_full_credits(self, response):\n        '''\n        Following the previous parse we start off at the main page with all the list of the cast and crew \n        \n        We will navigate to get a list of all the actors in The Hunger Games movie \n        \n        This will yiled a request that will get us to each of the actors main page where we \n        could get a list of the movies and TV shows they have been in \n        '''\n        actors_url=response.css('h3:contains(\"Cast\") + ol')\n        #actor_name=actors_url.css(\":: text\")\n        list_a=actors_url.css(\"div.info a::attr(href)\").getall()\n        \n        for url in list_a:\n            full_url = response.urljoin(url)\n            \n            yield scrapy.Request(full_url,callback=self.parse_actor_page) \n           \n\nNow that we are at the actors webapge we will use css to get a list of all the movies they were in. We also used the h2 element to collect all the actors name that were in the title of each of their webpage. Once we collect all the information we created a for loop that yoield a dictionary with the name of the actor and the movies and TV shows that they have been in.\n\ndef parse_actor_page(self, response):\n        \n        '''\n        In this section we assume that we are starting in the actore webapge\n    \n        We will be grabbing and formulating a list of all the movies they were in \n        \n        '''\n        # get the actor name, in this case we located in the title of the page \n        actor_name=response.css('h2.title &gt; a::text').get()\n        # this lines grabs the list of all the movies they have been in \n    \n        card_credit=response.xpath('//h3[text()=\"Acting\"]/following-sibling::table[1]')\n        movie_id=card_credit.css('td.role.true.account_adult_false.item_adult_false &gt; a.tooltip &gt; bdi::text').getall()\n        \n         #This will go through the list of all the movies we have collected and create a dictionary with all the movies and actors \n       \n        for movie in movie_id:\n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie}\n       \n\n\n\n\nOnce we have finished writing all the parsing for the class TmdbSpider we will run the following line with the correct subdir to create a .csv file for all of our actors and moves/ TV they have been in.\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=70160-the-hunger-games"
  },
  {
    "objectID": "TMDB_scraper/hw2blog.html#first-step",
    "href": "TMDB_scraper/hw2blog.html#first-step",
    "title": "HW 2, Web Scarpping",
    "section": "",
    "text": "Locate the Url that you would be scrapping information from. For this blog we will use this website https://www.themoviedb.org/ and will scarpe a list of actors from the Hunger Game to create recomandtion system that gives users a list of the other movies the actors played in. \nWe First want to make sure that we are in the right enviroment and create a TMDB_scraper folder with all the files we will need by running the following lines in your terminal.\n\nPIC16B-24W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper"
  },
  {
    "objectID": "TMDB_scraper/hw2blog.html#second-step",
    "href": "TMDB_scraper/hw2blog.html#second-step",
    "title": "HW 2, Web Scarpping",
    "section": "",
    "text": "#### Part 1 We create a new file inside the spider directory and named it tmdb_spider.py and run the following lines\nTo run the code in the terminal we need to run this line with the correct added subdir for the movie page we chose\n“scrapy crawl tmdb_spider -o movies.csv -a subdir=”\n\n\nNext we will create and implemnt three diffrent parsing methods to get the url for each actor page and then to scrape their page and collect a list of all the movies they have acted in\nOnce we have create the intial function to be able to explore the movie we chose for we need to add the correct subdir .\nWhen going to the hunger game movie page the url is as follow : https://www.themoviedb.org/movie/70160-the-hunger-games When running the code in our terminal we can also just write 70160 for the subdir instead of also inclduing the movie name\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n    '''\n     Initialize the TmdbSpider instance.\n     '''\n    \n\nIn this parse method we create a way to navigate to our cast and crew page by adding /cast to our orginal url. This can be more easily visualize by going to the movie webiste. Once you are there select the full cast and crew page and you will get redirected to a new page. If you look at the url you would notice that it now changed to :https://www.themoviedb.org/movie/70160-the-hunger-games/cast\n\ndef parse(self, response):\n    ''''\n    We start on a movie page, and then  navigate to the Full Cast & Crew page\n    ''''\n            # Navigate to the Full Cast & Crew page\n            full_cast_url = f\"{response.url + '/cast/'}\"\n            # Yield a scrapy.Request for the page of each actor listed on the Full Cast & Crew page\n            yield scrapy.Request(full_cast_url, callback=self.parse_full_credits)\n            \n            \n        \n  \n\nNow that we are at the full cast and crew page the nect step is to go to the actors main page where we can get a list of all the Movies and TV shows they acted in. In order to understand how to find all the information we need we need to inspect the webpage we are at. When we inspect it we are able to use css or xpath to explore and navigate to the right urls. To locate the cast members we will use css to search within the h3 aspect. We will then go through the list of actors by examining ol. To get the actors url we ill examine the href attribute under the div class info\n\n  def parse_full_credits(self, response):\n        '''\n        Following the previous parse we start off at the main page with all the list of the cast and crew \n        \n        We will navigate to get a list of all the actors in The Hunger Games movie \n        \n        This will yiled a request that will get us to each of the actors main page where we \n        could get a list of the movies and TV shows they have been in \n        '''\n        actors_url=response.css('h3:contains(\"Cast\") + ol')\n        #actor_name=actors_url.css(\":: text\")\n        list_a=actors_url.css(\"div.info a::attr(href)\").getall()\n        \n        for url in list_a:\n            full_url = response.urljoin(url)\n            \n            yield scrapy.Request(full_url,callback=self.parse_actor_page) \n           \n\nNow that we are at the actors webapge we will use css to get a list of all the movies they were in. We also used the h2 element to collect all the actors name that were in the title of each of their webpage. Once we collect all the information we created a for loop that yoield a dictionary with the name of the actor and the movies and TV shows that they have been in.\n\ndef parse_actor_page(self, response):\n        \n        '''\n        In this section we assume that we are starting in the actore webapge\n    \n        We will be grabbing and formulating a list of all the movies they were in \n        \n        '''\n        # get the actor name, in this case we located in the title of the page \n        actor_name=response.css('h2.title &gt; a::text').get()\n        # this lines grabs the list of all the movies they have been in \n    \n        card_credit=response.xpath('//h3[text()=\"Acting\"]/following-sibling::table[1]')\n        movie_id=card_credit.css('td.role.true.account_adult_false.item_adult_false &gt; a.tooltip &gt; bdi::text').getall()\n        \n         #This will go through the list of all the movies we have collected and create a dictionary with all the movies and actors \n       \n        for movie in movie_id:\n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie}\n       \n\n\n\n\nOnce we have finished writing all the parsing for the class TmdbSpider we will run the following line with the correct subdir to create a .csv file for all of our actors and moves/ TV they have been in.\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=70160-the-hunger-games"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/HW4/index.html",
    "href": "posts/HW4/index.html",
    "title": "HW 4, Heat Diffusion",
    "section": "",
    "text": "Below we set up and intalizes our N and epsilon variables as well as set up the main conditions for the heat difussion starting with the point in the middle\n\nN represents the size of the gride as well as the number of points that we will have on the dimension that the simulation is run on. Since we are working on a 2D gride the N gride will be N x N. The larger the vlaue of N is , the finer the gride will be. For this blog we will set up the N value to be 101\nEpsilon is the parameter that represents the stability constant or time step size in the simulation. Since we are working with heat difussion it can also be used to represent a scaling factor for the diffusion rate (this impacts the spead). Here we set the epsilon to be 0.2\n\n\nN = 101\nepsilon = 0.2\n\nBelow we imported some of the main packages we will need to use to both measure the time each method took and to present the heat difussion. we also constructed a gride with the initial condition to use as a starting point.\n\nimport time #so I could caluclate the time each method took\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# construct initial condition: 1 unit of heat at midpoint.\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n&lt;matplotlib.image.AxesImage at 0x7ec2cd0da6e0&gt;"
  },
  {
    "objectID": "posts/HW4/index.html#set-up",
    "href": "posts/HW4/index.html#set-up",
    "title": "HW 4, Heat Diffusion",
    "section": "",
    "text": "Below we set up and intalizes our N and epsilon variables as well as set up the main conditions for the heat difussion starting with the point in the middle\n\nN represents the size of the gride as well as the number of points that we will have on the dimension that the simulation is run on. Since we are working on a 2D gride the N gride will be N x N. The larger the vlaue of N is , the finer the gride will be. For this blog we will set up the N value to be 101\nEpsilon is the parameter that represents the stability constant or time step size in the simulation. Since we are working with heat difussion it can also be used to represent a scaling factor for the diffusion rate (this impacts the spead). Here we set the epsilon to be 0.2\n\n\nN = 101\nepsilon = 0.2\n\nBelow we imported some of the main packages we will need to use to both measure the time each method took and to present the heat difussion. we also constructed a gride with the initial condition to use as a starting point.\n\nimport time #so I could caluclate the time each method took\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# construct initial condition: 1 unit of heat at midpoint.\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n&lt;matplotlib.image.AxesImage at 0x7ec2cd0da6e0&gt;"
  },
  {
    "objectID": "posts/HW4/index.html#visualization",
    "href": "posts/HW4/index.html#visualization",
    "title": "HW 4, Heat Diffusion",
    "section": "Visualization",
    "text": "Visualization\nNow that we created out two functions we are finally ready to start visualizing our heat difussion maps.\n\nSet up before visualization\nBefore we can start using pyplot we need to creat snapshots of the gride state every 300 iteration to make it easier to graph.\n\n\nWe start by intializing the Gride state and Parameters\nwe use u0 for the gride state to use the intial state we created at the begining of the blog post. We also make sure to use the epsilon and N parameters that we established at the begining\n\n\nThen we construct the Laplace Operator Matrix A\nhere we call our get_A function to get our A matrix #### lastly, we run the simulation We created a loop following i to iterate 2700 times. Each time we iterate it represents a timestep in the simulation. It is importnat that we make sure to also update the gride state to represents the new iteration. We can update the gride by applying the matrix A to the current state u through the function advance_time_matvecmul. For the last part of this section, it is important that we store the intermediate solutions. Since 2700 is a lot of iterations we only want to keep track of every 300 iterations starting at 300 which we indicated in our if loop. We then append the iteration to our list ” solutions1 and we are now finally ready to start making cool heat difussions !!!\n\n# Initialize your grid state 'u' and stability constant 'epsilon'\nu = u0  #  initial grid state\nepsilon = 0.2  # stability constant\nN = 101\n\n# Get the matrix A using the function get_A(N)\nA = get_A(N)\n\n# Initialize a list to store the intermediate solutions\nsolutions1 = []\n\ns_time1=time.time()\n\n# Run the simulation for 2700 iterations\nfor i in range(2701):\n    u = advance_time_matvecmul(A, u, epsilon)\n\n    # Store the solution every 300 iterations\n    if i % 300 ==0 and i !=0:\n        solutions1.append(u.copy())\n#to calculate the total time it took to create it\ned_time1=time.time()\ntotal_time1=ed_time1- s_time1\n\n# Now, 'solutions' contains the grid state every 300 iterations\n\n\n\nGraphing\nWe will use matplot to create our visualizations. First, we create a 3x# grid of the subplots to show each of heat difussions that we captured at the diffrent iterations.\nWe start by looping throut our list with the iterations.\nThe row = i // 3 and col = i % 3 section calculate the row and column positions for the subplot that corresponds to the current solution. This arrangement distributes the plots across the 3x3 grid based on their index. // is the floor division operator, ensuring an integer result, and % gives the remainder, effectively wrapping the column index after every third plot.\nlastly, we use pyplot to graph everything :)\n\nimport matplotlib.pyplot as plt\n\n# Create a 3x3 grid of subplots\nfig, axs = plt.subplots(3, 3, figsize=(10, 10))\n\n\n# Loop over the solutions and plot each one\nfor i, solution in enumerate(solutions1):\n    # Calculate the row and column indices for the subplot\n    row = i // 3\n    col = i % 3\n\n    # Plot the solution on the subplot\n    axs[row, col].imshow(solution, cmap='viridis', interpolation='nearest')\n    axs[row, col].set_title(f\"Iteration {(i+1)*300}\")\n\n\n# Display the plot\nplt.show()\n\n\n\n\n\nWow look how pretty they all look!! To be able to compare the diffrent methods we also made sure to capture how long it took to creat the heat diffusions as can be seen right below\n\nprint (f\"Total time it took:{total_time1:0.2f}sec\")\n\nTotal time it took:217.09sec"
  },
  {
    "objectID": "posts/Hw1/Untitled1.html",
    "href": "posts/Hw1/Untitled1.html",
    "title": "Myblog",
    "section": "",
    "text": "import sqlite3\nimport pandas as pd\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    conn=sqlite3.connect(db_file) \n    c = conn.cursor()\n\n    # Construct the SQL query using f-strings\n    sql_query = f\"\"\"\n        SELECT S.NAME AS Name, S.LATITUDE AS Latitude, S.LONGITUDE AS Longitude, C.Name AS Name, \n               T.Year, T.Month,T.Temp\n        FROM temperatures T\n        LEFT JOIN stations S ON T.ID = S.ID\n        LEFT JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.'FIPS 10-4'\n        WHERE C.Name = '{country}'\n          AND T.Year BETWEEN {year_begin} AND {year_end}\n          AND T.Month = {month}\n        GROUP BY S.NAME, T.Year, T.Month\n    \"\"\"\n\n    # Execute the SQL query and fetch the results into a DataFrame\n    df = pd.read_sql_query(sql_query, conn)\n\n    # Close the database connection\n    conn.close()\n\n    return df\n\n\nquery_climate_database(db_file = \"your_database_file.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\nDatabaseError: Execution failed on sql '\n        SELECT S.NAME AS Name, S.LATITUDE AS Latitude, S.LONGITUDE AS Longitude, C.Name AS Name, \n               T.Year, T.Month,T.Temp\n        FROM temperatures T\n        LEFT JOIN stations S ON T.ID = S.ID\n        LEFT JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.'FIPS 10-4'\n        WHERE C.Name = 'India'\n          AND T.Year BETWEEN 1980 AND 2020\n          AND T.Month = 1\n        GROUP BY S.NAME, T.Year, T.Month\n    ': no such table: temperatures"
  },
  {
    "objectID": "posts/HW 5/index.html",
    "href": "posts/HW 5/index.html",
    "title": "HW 5: Image Classification",
    "section": "",
    "text": "italicized text— title: “HW 5, Image Classifications” author: “Neomi Goodman” date: “2024-02-29” categories: [week 8, HW 5 ]"
  },
  {
    "objectID": "posts/HW 5/index.html#first-model",
    "href": "posts/HW 5/index.html#first-model",
    "title": "HW 5: Image Classification",
    "section": "First Model",
    "text": "First Model\nWe will creat a keras.Sequential model with Conv2D layers, MaxPooling2D layers, Flatten layer,Dense layer, and Dropout layer. To do this we will first start by importing layers, models from tensorflow.keras . The model will classify images as either 1 or 0- that is either cat or dog. We start with models.Sequential() which intializes a liner stack of layers so each layer will have exactly one inpute tensor and one output tensor which will be the input tensor to the other layer.\nBefore we move to the rest of the modle lets talk about tensors. Tensors are the fundamental data structures used in deep learning to generalize scalars, vectirs, and matrices to higher dimensions. When using it in the model the tensors are manipulated through operations duirng forward and backward asses of trianing. Essentially, tensors are container of data similar to an array.\nNow that we understnd tensors we can continue with our model. We creat our first Conv2D layer which adds the first 2D convolutional layer with 32 filters (or kernels), each of size 3x3. The activation=‘relu’ parameter applies the Rectified Linear Unit (ReLU) activation function to the output of each convolution operation. the input_shape refres to the shape of our images, which we made 150x150 earlier when resizing the images. The first MaxPooling 2D layer reduces the spatial dimensions (height and width) of the input feature maps by half, effectively downsampling the input and reducing the number of parameters the model needs to learn, which helps in controlling overfitting.\nNext we move to the second Conv2D layer which increases the model to allow it to learn more complex features from the input images.The second MaxPooling2D layer is also used to further reduce the spatial dimensions of the feature maps. We then us layers.Flatten() to transforms the 3D output of the preceding layer into a 1D array, which is necessary because the following dense layer expects a 1D input.\nLastly, our two dense layers and dropout layer are used to takes the flattened input and learns non-linear combinations of features. Then to randomly sets half of the input units to 0 at each update during training time, which helps prevent overfitting by ensuring that the model does not become too reliant on any one node.And lastly, the layers.Dense(1, activation=‘sigmoid’), outputs a single value between 0 and 1, representing the probability that the input image belongs to the target class (1).\n\nfrom tensorflow.keras import layers, models\n\nmodel1 = models.Sequential([\n    # First Conv2D layer\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    # First MaxPooling2D layer\n    layers.MaxPooling2D((2, 2)),\n\n    # Second Conv2D layer\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    # Second MaxPooling2D layer\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    # Flatten layer to convert 3D feature maps to 1D feature vectors\n    layers.Flatten(),\n\n    # Dense layer\n    layers.Dense(128, activation='relu'),\n\n    # Dropout layer to reduce overfitting\n    layers.Dropout(0.4),\n\n    # Output layer\n    layers.Dense(1, activation='sigmoid')\n])\n\n\nCompile the model\nOnce we created the model we need to compile it to configure the model for training in the TensorFlow/Keras framework. This oart is crucial for setting up the model for training, defining how it should learn (optimizer), what it should minimize (loss function), and how its performance should be measured (metrics).\n\n model1.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\n\n\nTrain the Model\n\nhistory = model1.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 410s 3s/step - loss: 6.0001 - accuracy: 0.5716 - val_loss: 0.6708 - val_accuracy: 0.5907\nEpoch 2/20\n146/146 [==============================] - 405s 3s/step - loss: 0.6556 - accuracy: 0.6126 - val_loss: 0.6578 - val_accuracy: 0.6066\nEpoch 3/20\n146/146 [==============================] - 406s 3s/step - loss: 0.6132 - accuracy: 0.6505 - val_loss: 0.6754 - val_accuracy: 0.6199\nEpoch 4/20\n146/146 [==============================] - 397s 3s/step - loss: 0.5779 - accuracy: 0.6935 - val_loss: 0.6973 - val_accuracy: 0.6118\nEpoch 5/20\n146/146 [==============================] - 403s 3s/step - loss: 0.5260 - accuracy: 0.7255 - val_loss: 0.7253 - val_accuracy: 0.6036\nEpoch 6/20\n146/146 [==============================] - 386s 3s/step - loss: 0.4838 - accuracy: 0.7516 - val_loss: 0.7233 - val_accuracy: 0.6268\nEpoch 7/20\n146/146 [==============================] - 406s 3s/step - loss: 0.4277 - accuracy: 0.7947 - val_loss: 0.7512 - val_accuracy: 0.6135\nEpoch 8/20\n146/146 [==============================] - 424s 3s/step - loss: 0.3935 - accuracy: 0.8159 - val_loss: 0.8102 - val_accuracy: 0.5911\nEpoch 9/20\n146/146 [==============================] - 406s 3s/step - loss: 0.3507 - accuracy: 0.8417 - val_loss: 0.8663 - val_accuracy: 0.6281\nEpoch 10/20\n146/146 [==============================] - 384s 3s/step - loss: 0.2897 - accuracy: 0.8733 - val_loss: 1.1332 - val_accuracy: 0.6328\nEpoch 11/20\n146/146 [==============================] - 407s 3s/step - loss: 0.2878 - accuracy: 0.8742 - val_loss: 0.9575 - val_accuracy: 0.6320\nEpoch 12/20\n146/146 [==============================] - 425s 3s/step - loss: 0.2689 - accuracy: 0.8900 - val_loss: 1.0345 - val_accuracy: 0.6238\nEpoch 13/20\n146/146 [==============================] - 405s 3s/step - loss: 0.2221 - accuracy: 0.9095 - val_loss: 1.2777 - val_accuracy: 0.6247\nEpoch 14/20\n146/146 [==============================] - 412s 3s/step - loss: 0.2065 - accuracy: 0.9191 - val_loss: 1.1663 - val_accuracy: 0.6255\nEpoch 15/20\n146/146 [==============================] - 408s 3s/step - loss: 0.1906 - accuracy: 0.9250 - val_loss: 1.2094 - val_accuracy: 0.6178\nEpoch 16/20\n146/146 [==============================] - 406s 3s/step - loss: 0.1646 - accuracy: 0.9356 - val_loss: 1.3935 - val_accuracy: 0.6346\nEpoch 17/20\n146/146 [==============================] - 418s 3s/step - loss: 0.1592 - accuracy: 0.9452 - val_loss: 1.4878 - val_accuracy: 0.6101\nEpoch 18/20\n146/146 [==============================] - 405s 3s/step - loss: 0.1603 - accuracy: 0.9400 - val_loss: 1.3832 - val_accuracy: 0.6101\nEpoch 19/20\n146/146 [==============================] - 393s 3s/step - loss: 0.1471 - accuracy: 0.9476 - val_loss: 1.4424 - val_accuracy: 0.6174\nEpoch 20/20\n146/146 [==============================] - 409s 3s/step - loss: 0.1405 - accuracy: 0.9484 - val_loss: 1.6150 - val_accuracy: 0.6294\n\n\nBefore analyzing the results we are also going to plot the training and validation accuracy\n\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n\n\n\nAfter running the code and training the model the accuracy of the code stablized at around 60%\nIn compare to the baseline, this model is 10% more accurate in predicting the correct images. There does seem to be a lot of overfitting as the training accuracy seems to go up to 90% while the validation accuracy stays at around 60%.\n\n\nModel with Data Augmentation\nFor this model we will be adding data augmentation layers to your model. We are going to include some modified copies of the same imagies into our training set to imporve the model ability of identifying images\nWe will first start by creating a keras.layers.RandomFlip() layer. In the code below we select a single image and apply the augmentation on it to test to see how it will show. The random_flip_layer creats a new layer that we will than apply to the image and then plot it.\n\n\nimport matplotlib.pyplot as plt\n\n\n# take a single image from the dataset to apply augmentation\nfor images, _ in train_ds.take(1):\n    # Take the first image in the batch for demonstration\n    original_image = images[0]\n    break\n\n# Create the RandomFlip layer\nrandom_flip_layer = keras.Sequential([\n    keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n])\n\n# Visualize original and flipped images\nplt.figure(figsize=(10, 10))\nfor images, _ in train_ds.take(1):\n    for i in range(9):\n        augmented_ig = random_flip_layer(images)\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(augmented_ig[0].numpy().astype(\"uint8\"))\n        plt.axis(\"off\")\nplt.show()\n\n\n\n\nWow look how cool that is! Now lets do the same thing but instead of flip we will rotate the image. To increase the rotation range we can make the factor be .45 if the factor was smaller, so would the rotataion range be.\n\nrotation_ig = keras.Sequential([\n    keras.layers.RandomRotation(factor=0.45),\n])\n\n# Visualize original and rotated images\nplt.figure(figsize=(10, 10))\nfor images, _ in train_ds.take(1):\n    for i in range(9):\n        augmented_ig2 = rotation_ig(images)\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(augmented_ig2[0].numpy().astype(\"uint8\"))\n        plt.axis(\"off\")\nplt.show()\n\n\n\n\nOnce we were able to visualize the two forms of agumantation we will incoporate them and create model2. Similar to model 1, we will train the datat and keep track of its accuracy. To make sure that the model is more accurate now that we added the agumentation factor we will change teh rotation range back down to .2. We also added layers.BatchNormalization() which will improve training stability and speed by normalizing the input to each activation layer and changed the dropout layer to .5 instead of .4 to help with the overfitting\n\nmodel2 = models.Sequential([\n    # Data Augmentation layers\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.2),\n\n\n    # First Conv2D layer\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    # First MaxPooling2D layer\n    layers.MaxPooling2D((2, 2)),\n\n    # Second Conv2D layer\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    # Second MaxPooling2D layer\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    # Flatten layer to convert 3D feature maps to 1D feature vectors\n    layers.Flatten(),\n\n    # Dense layer\n    layers.Dense(128, activation='relu'),\n\n    # Dropout layer to reduce overfitting\n    layers.Dropout(0.5),\n\n    # Output layer\n    layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel2.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\n# Train the model with data augmentation\nhistory = model2.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 10s 46ms/step - loss: 1.0536 - accuracy: 0.5786 - val_loss: 0.6513 - val_accuracy: 0.6445\nEpoch 2/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.6405 - accuracy: 0.6347 - val_loss: 0.6429 - val_accuracy: 0.6367\nEpoch 3/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.6184 - accuracy: 0.6596 - val_loss: 0.6523 - val_accuracy: 0.5821\nEpoch 4/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.6119 - accuracy: 0.6651 - val_loss: 0.5984 - val_accuracy: 0.6604\nEpoch 5/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.6045 - accuracy: 0.6768 - val_loss: 0.5956 - val_accuracy: 0.6844\nEpoch 6/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.5869 - accuracy: 0.6844 - val_loss: 0.5415 - val_accuracy: 0.7240\nEpoch 7/20\n146/146 [==============================] - 7s 47ms/step - loss: 0.5756 - accuracy: 0.7008 - val_loss: 0.5436 - val_accuracy: 0.7257\nEpoch 8/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.5662 - accuracy: 0.7114 - val_loss: 1.2838 - val_accuracy: 0.5052\nEpoch 9/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.5612 - accuracy: 0.7176 - val_loss: 0.7067 - val_accuracy: 0.6113\nEpoch 10/20\n146/146 [==============================] - 8s 53ms/step - loss: 0.5496 - accuracy: 0.7229 - val_loss: 0.5536 - val_accuracy: 0.6883\nEpoch 11/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.5404 - accuracy: 0.7385 - val_loss: 0.5986 - val_accuracy: 0.6604\nEpoch 12/20\n146/146 [==============================] - 7s 51ms/step - loss: 0.5304 - accuracy: 0.7468 - val_loss: 0.6166 - val_accuracy: 0.6896\nEpoch 13/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.5197 - accuracy: 0.7516 - val_loss: 0.4979 - val_accuracy: 0.7511\nEpoch 14/20\n146/146 [==============================] - 7s 47ms/step - loss: 0.5161 - accuracy: 0.7574 - val_loss: 0.5920 - val_accuracy: 0.6733\nEpoch 15/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.5079 - accuracy: 0.7588 - val_loss: 0.8534 - val_accuracy: 0.6277\nEpoch 16/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.4959 - accuracy: 0.7681 - val_loss: 0.4682 - val_accuracy: 0.7721\nEpoch 17/20\n146/146 [==============================] - 7s 50ms/step - loss: 0.4855 - accuracy: 0.7709 - val_loss: 0.4667 - val_accuracy: 0.7855\nEpoch 18/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.4950 - accuracy: 0.7673 - val_loss: 0.4966 - val_accuracy: 0.7709\nEpoch 19/20\n146/146 [==============================] - 7s 47ms/step - loss: 0.4841 - accuracy: 0.7772 - val_loss: 0.5106 - val_accuracy: 0.7459\nEpoch 20/20\n146/146 [==============================] - 7s 47ms/step - loss: 0.4868 - accuracy: 0.7724 - val_loss: 0.5122 - val_accuracy: 0.7571\n\n\nLet us again visualize the training accuracy and validation accuracy\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.show()\n\n\n\n\nWith the data agumentation, the validation accuracy of the model was between 60% and 70% There were a few drops in which the accuracy went down to 50 or low 60 but then it went back up to the 70s. In comparsion to model1 the validation accuracy has increased although the training accuracy seem to have been lower than it was in model 1. However, model2 does seem to be way better at acounting for and fixing overfitting.\nThe diffrenece between the models can be seen even better when looking at the graph. the graph for model2 shows how drastically the validation accuracy changed through the training while in model 1 the validation accuracy seemed to have stayed consistently low.\n\n\nData Preprocessing\nTo make the model train fatser we can also change the RGB values. Changing the RGB value can also help save our time and let us shift our attention to dealing with actual signal in the data. RBG is the color model used in digital imaging and displays. It stands for Red Green and Blue and is based on the idea that all colors are based on teh combination of those three colors. Often time each color is typically represented by a value ranging from 0 to 255. To make the model train faster we wnat the RGB to be normalized between 0 and 1 which is what we are goining to try and do.\nFor this section we will create model3 with a preprocessing layer. The preprocessing layer coded below will normalize image pixel values\nLet’s break down how to creat the preprocessor layer. First we will start by defining the input tensor for the preprocessing model. Next step is to creat the rescaling layer. The scale_layer will transform the input pixel values. Llastly, the last line bundles the input tensor and the output of the rescaling layer into a standalone Keras model named preprocessor. Essentially what this will do is take an input image/s and produces the normalized version of the image/s as output.\n\n\n# Define the input shape\ni = keras.Input(shape=(150, 150, 3))\n\n# Create the Rescaling layer to normalize pixel values\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n\n# Apply the scaling layer to the input\nx = scale_layer(i)\n\n# Create a preprocessing model\npreprocessor = keras.Model(inputs=[i], outputs=[x])\n\nNext up we will need to creat model 3. Similar to model 1 and 2 we will include all the same factors as well as the agumentation section in addition to the preprocessor. Because we made more changes to teh datat we will also need to make further adjusment to the model to make sure it is more accurate. We increased the dense layer to 256 and chnaged the drop layer to .2\n\nmodel3 = keras.Sequential([\n    preprocessor,  # Preprocessing layer for input normalization\n\n    # Data augmentation layers\n    keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n    keras.layers.RandomRotation(0.2),\n\n   # First Conv2D layer\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    # First MaxPooling2D layer\n    layers.MaxPooling2D((2, 2)),\n\n    # Second Conv2D layer\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    # Second MaxPooling2D layer\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    # Flatten layer to convert 3D feature maps to 1D feature vectors\n    layers.Flatten(),\n\n    # Dense layer\n    layers.Dense(256, activation='relu'),\n\n    # Dropout layer to reduce overfitting\n    layers.Dropout(0.2),\n\n    # Output layer\n    layers.Dense(1, activation='sigmoid')\n])\n\n\n\n# Compile the model\nmodel3.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\n\nhistory = model3.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 9s 48ms/step - loss: 1.3734 - accuracy: 0.5857 - val_loss: 0.6798 - val_accuracy: 0.6470\nEpoch 2/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.6146 - accuracy: 0.6570 - val_loss: 0.6377 - val_accuracy: 0.6930\nEpoch 3/20\n146/146 [==============================] - 7s 47ms/step - loss: 0.5927 - accuracy: 0.6820 - val_loss: 0.5921 - val_accuracy: 0.6862\nEpoch 4/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.5750 - accuracy: 0.6970 - val_loss: 0.5740 - val_accuracy: 0.6973\nEpoch 5/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.5563 - accuracy: 0.7150 - val_loss: 0.5271 - val_accuracy: 0.7356\nEpoch 6/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.5441 - accuracy: 0.7191 - val_loss: 0.5199 - val_accuracy: 0.7390\nEpoch 7/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.5304 - accuracy: 0.7415 - val_loss: 0.5779 - val_accuracy: 0.7223\nEpoch 8/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.5178 - accuracy: 0.7436 - val_loss: 0.5007 - val_accuracy: 0.7687\nEpoch 9/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.5067 - accuracy: 0.7531 - val_loss: 0.4866 - val_accuracy: 0.7687\nEpoch 10/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.4978 - accuracy: 0.7558 - val_loss: 0.5418 - val_accuracy: 0.7451\nEpoch 11/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.4887 - accuracy: 0.7677 - val_loss: 0.4750 - val_accuracy: 0.7618\nEpoch 12/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.4901 - accuracy: 0.7644 - val_loss: 0.4799 - val_accuracy: 0.7713\nEpoch 13/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.4733 - accuracy: 0.7751 - val_loss: 0.5148 - val_accuracy: 0.7455\nEpoch 14/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.4636 - accuracy: 0.7819 - val_loss: 0.4434 - val_accuracy: 0.7902\nEpoch 15/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.4472 - accuracy: 0.7893 - val_loss: 0.5685 - val_accuracy: 0.7528\nEpoch 16/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.4451 - accuracy: 0.7940 - val_loss: 0.4437 - val_accuracy: 0.8040\nEpoch 17/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.4374 - accuracy: 0.7956 - val_loss: 0.4520 - val_accuracy: 0.7954\nEpoch 18/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.4354 - accuracy: 0.7980 - val_loss: 0.4444 - val_accuracy: 0.7928\nEpoch 19/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.4190 - accuracy: 0.8096 - val_loss: 0.4100 - val_accuracy: 0.8052\nEpoch 20/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.4217 - accuracy: 0.8058 - val_loss: 0.4142 - val_accuracy: 0.8108\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model3 Training and Validation Accuracy')\nplt.legend()\nplt.show()\n\n\n\n\nWe were able to get the model Validation accuracy up to 81% this is again big imporvemnt from model 1 and 2. Additionally , were were able to minimize abnd bring down the overfiting by chnaging the drop layer to .2."
  },
  {
    "objectID": "posts/HW 5/index.html#transfer-learning",
    "href": "posts/HW 5/index.html#transfer-learning",
    "title": "HW 5: Image Classification",
    "section": "Transfer Learning",
    "text": "Transfer Learning\nWhile we were able to create a model that can distingiuse between pictures of dogs and cats, there are people who have already created similar models. In this next section we are going to try to use a pre existing model to do our task.\nWe will first need to start by getting a preexisting model and use it as our base model before incoportating it to our model and train our data on it.\nWe will first start by importing the MobileNetV3Large model and making it into a layer that we will be able to add to our model.\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n12683000/12683000 [==============================] - 0s 0us/step\n\n\nNow that we have the pre made model we will make model4 which will include this new added layer.\nWhile model 4 also included the data agumentation and a layer for classification, it is way more accurat thanks to the base_model_layer.\nLets break down the code for model 4. To make the model look cleaner we created the data_agumentation variable outside of the model construction. We then construct the model by incoportating the base_model_layer and the augmentation layer. We the use GlobalMaxPooling2D to reduces the spatial dimensions of the feature maps to a single maximum value per feature map. We set the drpout at .20 to help minimize overfitting.\nOnce the model is done with compile it and then train it\n\ndata_augmentation = keras.Sequential([\n    keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n    keras.layers.RandomRotation(0.2),\n])\n\n# Model construction\nmodel4 = keras.Sequential([\n    data_augmentation,\n    base_model_layer,\n    layers.GlobalMaxPooling2D(),\n    layers.Dropout(0.2),  # Dropout layer to reduce overfitting\n    layers.Dense(2, activation='softmax')  # Classification layer\n])\n\n# Compile the model\nmodel4.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\n\n\nhistory = model4.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 16s 63ms/step - loss: 1.7371 - accuracy: 0.8186 - val_loss: 0.3864 - val_accuracy: 0.9527\nEpoch 2/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.9484 - accuracy: 0.8830 - val_loss: 0.3616 - val_accuracy: 0.9549\nEpoch 3/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6807 - accuracy: 0.9038 - val_loss: 0.1966 - val_accuracy: 0.9626\nEpoch 4/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.5961 - accuracy: 0.9031 - val_loss: 0.1960 - val_accuracy: 0.9609\nEpoch 5/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.5247 - accuracy: 0.9025 - val_loss: 0.1887 - val_accuracy: 0.9609\nEpoch 6/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4708 - accuracy: 0.9059 - val_loss: 0.1530 - val_accuracy: 0.9622\nEpoch 7/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4174 - accuracy: 0.9087 - val_loss: 0.1354 - val_accuracy: 0.9647\nEpoch 8/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3734 - accuracy: 0.9085 - val_loss: 0.1250 - val_accuracy: 0.9613\nEpoch 9/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3980 - accuracy: 0.8998 - val_loss: 0.3346 - val_accuracy: 0.9248\nEpoch 10/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3886 - accuracy: 0.9020 - val_loss: 0.1424 - val_accuracy: 0.9574\nEpoch 11/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3555 - accuracy: 0.9055 - val_loss: 0.1492 - val_accuracy: 0.9566\nEpoch 12/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3352 - accuracy: 0.9103 - val_loss: 0.1395 - val_accuracy: 0.9617\nEpoch 13/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3212 - accuracy: 0.9045 - val_loss: 0.1145 - val_accuracy: 0.9635\nEpoch 14/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.3161 - accuracy: 0.9090 - val_loss: 0.1256 - val_accuracy: 0.9626\nEpoch 15/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3287 - accuracy: 0.9037 - val_loss: 0.1235 - val_accuracy: 0.9583\nEpoch 16/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3050 - accuracy: 0.9083 - val_loss: 0.2015 - val_accuracy: 0.9398\nEpoch 17/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3136 - accuracy: 0.9100 - val_loss: 0.1114 - val_accuracy: 0.9682\nEpoch 18/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3184 - accuracy: 0.9018 - val_loss: 0.1304 - val_accuracy: 0.9592\nEpoch 19/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.2954 - accuracy: 0.9103 - val_loss: 0.1378 - val_accuracy: 0.9553\nEpoch 20/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.2985 - accuracy: 0.9090 - val_loss: 0.1122 - val_accuracy: 0.9622\n\n\n\nmodel4.summary()\n\nModel: \"sequential_26\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n sequential_25 (Sequential)  (None, 150, 150, 3)       0         \n                                                                 \n model_1 (Functional)        (None, 5, 5, 960)         2996352   \n                                                                 \n global_max_pooling2d_1 (Gl  (None, 960)               0         \n obalMaxPooling2D)                                               \n                                                                 \n dropout_22 (Dropout)        (None, 960)               0         \n                                                                 \n dense_43 (Dense)            (None, 2)                 1922      \n                                                                 \n=================================================================\nTotal params: 2998274 (11.44 MB)\nTrainable params: 1922 (7.51 KB)\nNon-trainable params: 2996352 (11.43 MB)\n_________________________________________________________________\n\n\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs_range = range(20)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\n\n\nText(0.5, 1.0, 'Training and Validation Accuracy')\n\n\n\n\n\nWe started with around 50%-60% accuracy in our first model, and now with the help of the base model we were able to bring up the accuracy all the way up to around 95%. As seen in the summary we only have 2998274 parameter to train the model on. There also seem to be very little over fitting of around 2%-3%."
  },
  {
    "objectID": "posts/HW 5/index.html#score-on-test-data",
    "href": "posts/HW 5/index.html#score-on-test-data",
    "title": "HW 5: Image Classification",
    "section": "Score on test data",
    "text": "Score on test data\nAfter making all theses models it is time for us to run the accuracy test on our test dataset!!\nSince model4 had the highest accuracy rate I chose to run the test dataset on it.\n\ntest_loss, test_acc = model4.evaluate(test_ds)\nprint(f\"Test accuracy: {test_acc*100:.2f}%\")\n\n37/37 [==============================] - 4s 101ms/step - loss: 0.1498 - accuracy: 0.9553\nTest accuracy: 95.53%\n\n\nLook at that !! We were able to get an accuracy precent of 95.53% !\nThere are a lot of diffrent moving parts and many diffrent things that can be chnaged and corrected. But as we seen in this blog post, simply trying out diffrent things and chnaging the setting can help imporve the accuracy rate of the model greatly!"
  },
  {
    "objectID": "posts/bruin/index.html",
    "href": "posts/bruin/index.html",
    "title": "Creating posts",
    "section": "",
    "text": "### Import all the data and plotly libary\n\nimport plotly.express as px \nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\ndf= penguins \n# changed the name of data frame to make it easier to work with\n\n\n# in order to isnpect the data I printed out the first five rows so I could see the variables I need to work with \ndf.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\ndf=df[df.Sex.notnull()]\n\ncolumes=(['Species','Region', 'Island', 'Sex', 'Culmen Length (mm)','Culmen Depth (mm)','Flipper Length (mm)', 'Body Mass (g)'])\ndfnew=df[columes]\n# this creats a new data fram with the selected columes that I want to be analyzing in my visualization \n\n\ndfnew.head()\n\n\n\n\n\n\n\n\nSpecies\nRegion\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfig=px.scatter(df,x='Flipper Length (mm)',\n               y='Body Mass (g)',\n              color='Species',\n              title='Flipper Length vs Body Mass')\n# this creats a scatter plot that represents the X and Y varaibles as well as distingues speices by color \nfig.show()"
  },
  {
    "objectID": "posts/bruin/index.html#hw0",
    "href": "posts/bruin/index.html#hw0",
    "title": "Creating posts",
    "section": "",
    "text": "### Import all the data and plotly libary\n\nimport plotly.express as px \nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\ndf= penguins \n# changed the name of data frame to make it easier to work with\n\n\n# in order to isnpect the data I printed out the first five rows so I could see the variables I need to work with \ndf.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\ndf=df[df.Sex.notnull()]\n\ncolumes=(['Species','Region', 'Island', 'Sex', 'Culmen Length (mm)','Culmen Depth (mm)','Flipper Length (mm)', 'Body Mass (g)'])\ndfnew=df[columes]\n# this creats a new data fram with the selected columes that I want to be analyzing in my visualization \n\n\ndfnew.head()\n\n\n\n\n\n\n\n\nSpecies\nRegion\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfig=px.scatter(df,x='Flipper Length (mm)',\n               y='Body Mass (g)',\n              color='Species',\n              title='Flipper Length vs Body Mass')\n# this creats a scatter plot that represents the X and Y varaibles as well as distingues speices by color \nfig.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Myblog",
    "section": "",
    "text": "HW 5: Image Classification\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nHW 3, Webapp\n\n\n\n\n\n\n\nweek 6\n\n\nHW 3\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2024\n\n\nNeomi Goodman\n\n\n\n\n\n\n  \n\n\n\n\nHW 4, Heat Diffusion\n\n\n\n\n\n\n\nweek 7\n\n\nHW 4\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nNeomi Goodman\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nNeomi Goodman\n\n\n\n\n\n\n  \n\n\n\n\nHW 2, Web Scarpping\n\n\n\n\n\n\n\nweek 5\n\n\nHW 2\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nNeomi Goodman\n\n\n\n\n\n\n  \n\n\n\n\nHW 1, Database\n\n\n\n\n\n\n\nweek 4\n\n\nHW 1\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2023\n\n\nNeomi Goodman\n\n\n\n\n\n\n  \n\n\n\n\nCreating posts\n\n\n\n\n\n\n\nweek 0\n\n\nHW 0\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2023\n\n\nNeomi Goodman\n\n\n\n\n\n\n  \n\n\n\n\nCreating posts\n\n\n\n\n\n\n\nweek 0\n\n\nHW 0\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2023\n\n\nNeomi Goodman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/bruin/Index1.html",
    "href": "posts/bruin/Index1.html",
    "title": "Creating posts",
    "section": "",
    "text": "import plotly.express as px \nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\ndf= penguins \n# changed the name of data frame to make it easier to work with\n\n\n# in order to isnpect the data I printed out the first five rows so I could see the variables I need to work with \ndf.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf=df[df.Sex.notnull()]\n\ncolumes=(['Species','Region', 'Island', 'Sex', 'Culmen Length (mm)','Culmen Depth (mm)','Flipper Length (mm)', 'Body Mass (g)'])\ndfnew=df[columes]\n# this creats a new data fram with the selected columes that I want to be analyzing in my visualization \n\n\ndfnew.head()\n\n\n\n\n\n\n\n\nSpecies\nRegion\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfig=px.scatter(df,x='Flipper Length (mm)',\n               y='Body Mass (g)',\n              color='Species',\n              title='Flipper Length vs Body Mass')\n# this creats a scatter plot that represents the X and Y varaibles as well as distingues speices by color \nfig.show()"
  },
  {
    "objectID": "posts/bruin/Index1.html#hw0",
    "href": "posts/bruin/Index1.html#hw0",
    "title": "Creating posts",
    "section": "",
    "text": "import plotly.express as px \nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\ndf= penguins \n# changed the name of data frame to make it easier to work with\n\n\n# in order to isnpect the data I printed out the first five rows so I could see the variables I need to work with \ndf.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf=df[df.Sex.notnull()]\n\ncolumes=(['Species','Region', 'Island', 'Sex', 'Culmen Length (mm)','Culmen Depth (mm)','Flipper Length (mm)', 'Body Mass (g)'])\ndfnew=df[columes]\n# this creats a new data fram with the selected columes that I want to be analyzing in my visualization \n\n\ndfnew.head()\n\n\n\n\n\n\n\n\nSpecies\nRegion\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfig=px.scatter(df,x='Flipper Length (mm)',\n               y='Body Mass (g)',\n              color='Species',\n              title='Flipper Length vs Body Mass')\n# this creats a scatter plot that represents the X and Y varaibles as well as distingues speices by color \nfig.show()"
  },
  {
    "objectID": "posts/Hw001/index.html",
    "href": "posts/Hw001/index.html",
    "title": "HW 1, Database",
    "section": "",
    "text": "# import all packages I might need \nimport pandas as pd \nimport sqlite3\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom plotly.io import write_html\nimport plotly.express as px\n\n\n\n\n\n# create a database called db_file \nconn= sqlite3.connect(\"db_file\")\n\n\n\n\n\ndf = pd.read_csv(\"temps.csv\")\ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\n\n\n#clean and prepare the data \ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\n\ndf = prepare_df(df)\n\n\n#creat a table with the temperature data\nfor i, df in enumerate(df_iter):\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\n\n#creats a table with all the stations \nurl = \"station-metadata.csv\"\nstations = pd.read_csv(url)\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n\n27585\n\n\n\n#creats a table with all the countries \ncountries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\ncountries= pd.read_csv(countries_url)\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n279\n\n\n\n\n\n\n#to make sure we have created the database with the three tables we wanted we print a list with our tables\ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('stations',), ('countries',)]\n\n\n\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\n\n\n\n\ncmd = \\\n\"\"\"\nSELECT T.id, T.month, T.temp\nFROM temperatures T\nLEFT JOIN countries C ON SUBSTR(T.id, 1, 2) = SUBSTR(C.\"FIPS 10-4\", 1, 2)\nLEFT JOIN stations S ON SUBSTR(T.id, 1, 2) = SUBSTR(S.id, 1, 2)\nWHERE C.Name IS NOT NULL\n\n\"\"\"\ncursor = conn.cursor()\ncursor.execute(cmd)\nresult = [cursor.fetchone() for i in range(10)]  # get just the first 10 results\nresult\n\n[('AFM00040911', 1, 2.99),\n ('AFM00040911', 1, 2.99),\n ('AFM00040911', 1, 2.99),\n ('AFM00040911', 1, 2.99),\n ('AFM00040911', 1, 2.99),\n ('AFM00040911', 1, 2.99),\n ('AFM00040911', 2, 8.09),\n ('AFM00040911', 2, 8.09),\n ('AFM00040911', 2, 8.09),\n ('AFM00040911', 2, 8.09)]\n\n\n\nconn.close()\n\n\n\n\n\n\n\n\n\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \n   # this connects it to the database we created earlier \n    conn= sqlite3.connect(db_file)\n    \n    query = f\"\"\"\n    SELECT S.NAME AS NAME, S.LATITUDE AS LATITUDE, S.LONGITUDE AS LONGITUDE, C.Name AS Country,\n           T.Year, T.Month, AVG(T.Temp) AS Temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.ID = S.ID\n    LEFT JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.'FIPS 10-4'\n    WHERE C.Name = '{country}'\n          AND T.Year BETWEEN {year_begin} AND {year_end}\n          AND T.Month = {month}\n   \"\"\"\n    \n    df = pd.read_sql_query(query, conn)\n    # this closes the database connction \n    conn.close()\n   \n    return df\n\n\nquery_climate_database(db_file = \"db_file\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n19.765926\n\n\n\n\n\n\n\n\n\n\n\n\n\n# I first created a function to calculate the yearly temp coefficient so I will not have to do this calculation within my next function and the work would be presented more clearly \n\ndef calc_temp_coefficient(data_group):\n    x = data_group[['Year']].values\n    y = data_group['Temp'].values\n    if len(x) &lt; 2:\n        return np.nan  # this is in case there is not enough data to caluclate the regression \n    lr = LinearRegression()\n    lr.fit(x, y)\n    return round(lr.coef_[0], 5) \n\n# a function that shows how the average yearly change in temperature vary within a given country\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs=10 ,**kwargs): \n    conn=sqlite3.connect(\"db_file\")\n    \n    # SQL query to get the temperature data for a the specified country in a year range and month\n    query = f\"\"\"\n    SELECT S.NAME AS Station_Name, S.LATITUDE AS Latitude, S.LONGITUDE AS Longitude,\n           C.Name AS Country, T.Year, T.Month, AVG(T.Temp) AS Temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.ID = S.ID\n    LEFT JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.'FIPS 10-4'\n    WHERE C.Name = '{country}'\n          AND T.Year BETWEEN {year_begin} AND {year_end}\n          AND T.Month = {month}\n    GROUP BY S.NAME, T.Year, T.Month\n    \"\"\"\n  \n    # I used .read_sql_querey to read the dtatbase datat and store it as Pandas DataFrame\n    df = pd.read_sql_query(query, conn)\n    \n    # filter stations with at least min_obs years of data value and group them by the station ID \n    df = df.groupby('Station_Name').filter(lambda x: x['Year'].count() &gt;= min_obs)\n    \n   \n     # Calculate the estimated yearly temperature increase for each station\n    yearly_changes = df.groupby('Station_Name').apply(\n        lambda group: calc_temp_coefficient(group)\n    ).reset_index(name='Estimated Yearly Increase (°C)')\n    \n    \n     # Create a Plotly Express scatter mapbox plot\n    fig = px.scatter_mapbox(\n        df,\n        lat='Latitude',\n        lon='Longitude',\n        hover_name='Station_Name',\n        color='Temp',  # Use the calculated column as color\n        height=600,\n        **kwargs\n    )\n    \n    # Set layout options and styling for the plot\n    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n    \n    # Return the figure\n    return fig\n\n\n\n\n\n\n\nfrom climate_database import query_climate_database\nfrom climate_database import calc_temp_coefficient\nfrom climate_database import temperature_coefficient_plot\n\nimport inspect\n#print(inspect.getsource(query_climate_database))\n#print(inspect.getsource(calc_temp_coefficient))\n#print(inspect.getsource(temperature_coefficient_plot))\n\n\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file=\"db_file\",country=\"India\",year_begin=1980, year_end=2020, month=1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map,\n                                   title=\"Estimates of the Yearly Temperature increase in India \")\n\n\nfig.show()\n\nValueError: Value of 'color' is not the name of a column in 'data_frame'. Expected one of ['Station_Name', 'Latitude', 'Longitude', 'Country', 'Year', 'Month', 'Temp'] but received: EYI\n\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file=\"db_file\",country=\"France\",year_begin=1980, year_end=2020, month=9, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map,\n                                   title=\"Estimates of the Yearly Temperature increase in France\")\n\nfig.show()\n\n\n\n\n\n\n\n\n\nfrom climate_database import analyze_seasonal_temperature_change\n\n\nimport inspect\nprint(inspect.getsource(analyze_seasonal_temperature_change))\n\n\ndb_file = \"db_file\"\n\n# Define the year range\nyear_range = (2000, 2020)\n\n# Define the country (optional)\ncountry = \"USA\"\n\n# Call the function\nanalyze_seasonal_temperature_change(db_file, year_range)\n\n\n\n\n\nimport plotly.express as px\n\ndef visualize_avg_monthly_temperature_by_country(df):\n    fig = px.line(df, x='Month', y='Avg_Temperature', title='Average Monthly Temperature by Country')\n    return fig\n\ndef get_data_from_database(db_file):\n    conn = sqlite3.connect(db_file)\n    query = \"SELECT * FROM temperatures\"  # Adjust your SQL query accordingly\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n    return df\n\n\nmy_df = get_data_from_database(\"db_file\")\nfig = visualize_avg_monthly_temperature_by_country(my_df)\n\n\ndef visualize_monthly_temperature_trends_for_station(df):\n    fig = px.line(df, x='Month', y='Temp', color='Year', title='Monthly Temperature Trends for a Specific Station')\n    return fig"
  },
  {
    "objectID": "posts/Hw001/index.html#part-2-create-a-query-function",
    "href": "posts/Hw001/index.html#part-2-create-a-query-function",
    "title": "HW 1, Database",
    "section": "",
    "text": "def query_climate_database(db_file, country, year_begin, year_end, month):\n    \n   # this connects it to the database we created earlier \n    conn= sqlite3.connect(db_file)\n    \n    query = f\"\"\"\n    SELECT S.NAME AS NAME, S.LATITUDE AS LATITUDE, S.LONGITUDE AS LONGITUDE, C.Name AS Country,\n           T.Year, T.Month, AVG(T.Temp) AS Temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.ID = S.ID\n    LEFT JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.'FIPS 10-4'\n    WHERE C.Name = '{country}'\n          AND T.Year BETWEEN {year_begin} AND {year_end}\n          AND T.Month = {month}\n   \"\"\"\n    \n    df = pd.read_sql_query(query, conn)\n    # this closes the database connction \n    conn.close()\n   \n    return df\n\n\nquery_climate_database(db_file = \"db_file\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n19.765926"
  },
  {
    "objectID": "posts/Hw001/index.html#part-3-create-an-interactive-viusalization",
    "href": "posts/Hw001/index.html#part-3-create-an-interactive-viusalization",
    "title": "HW 1, Database",
    "section": "",
    "text": "# I first created a function to calculate the yearly temp coefficient so I will not have to do this calculation within my next function and the work would be presented more clearly \n\ndef calc_temp_coefficient(data_group):\n    x = data_group[['Year']].values\n    y = data_group['Temp'].values\n    if len(x) &lt; 2:\n        return np.nan  # this is in case there is not enough data to caluclate the regression \n    lr = LinearRegression()\n    lr.fit(x, y)\n    return round(lr.coef_[0], 5) \n\n# a function that shows how the average yearly change in temperature vary within a given country\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs=10 ,**kwargs): \n    conn=sqlite3.connect(\"db_file\")\n    \n    # SQL query to get the temperature data for a the specified country in a year range and month\n    query = f\"\"\"\n    SELECT S.NAME AS Station_Name, S.LATITUDE AS Latitude, S.LONGITUDE AS Longitude,\n           C.Name AS Country, T.Year, T.Month, AVG(T.Temp) AS Temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.ID = S.ID\n    LEFT JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.'FIPS 10-4'\n    WHERE C.Name = '{country}'\n          AND T.Year BETWEEN {year_begin} AND {year_end}\n          AND T.Month = {month}\n    GROUP BY S.NAME, T.Year, T.Month\n    \"\"\"\n  \n    # I used .read_sql_querey to read the dtatbase datat and store it as Pandas DataFrame\n    df = pd.read_sql_query(query, conn)\n    \n    # filter stations with at least min_obs years of data value and group them by the station ID \n    df = df.groupby('Station_Name').filter(lambda x: x['Year'].count() &gt;= min_obs)\n    \n   \n     # Calculate the estimated yearly temperature increase for each station\n    yearly_changes = df.groupby('Station_Name').apply(\n        lambda group: calc_temp_coefficient(group)\n    ).reset_index(name='Estimated Yearly Increase (°C)')\n    \n    \n     # Create a Plotly Express scatter mapbox plot\n    fig = px.scatter_mapbox(\n        df,\n        lat='Latitude',\n        lon='Longitude',\n        hover_name='Station_Name',\n        color='Temp',  # Use the calculated column as color\n        height=600,\n        **kwargs\n    )\n    \n    # Set layout options and styling for the plot\n    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n    \n    # Return the figure\n    return fig\n\n\n\n\n\n\n\nfrom climate_database import query_climate_database\nfrom climate_database import calc_temp_coefficient\nfrom climate_database import temperature_coefficient_plot\n\nimport inspect\n#print(inspect.getsource(query_climate_database))\n#print(inspect.getsource(calc_temp_coefficient))\n#print(inspect.getsource(temperature_coefficient_plot))\n\n\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file=\"db_file\",country=\"India\",year_begin=1980, year_end=2020, month=1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map,\n                                   title=\"Estimates of the Yearly Temperature increase in India \")\n\n\nfig.show()\n\nValueError: Value of 'color' is not the name of a column in 'data_frame'. Expected one of ['Station_Name', 'Latitude', 'Longitude', 'Country', 'Year', 'Month', 'Temp'] but received: EYI\n\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file=\"db_file\",country=\"France\",year_begin=1980, year_end=2020, month=9, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map,\n                                   title=\"Estimates of the Yearly Temperature increase in France\")\n\nfig.show()"
  },
  {
    "objectID": "posts/Hw001/index.html#part-4-creating-two-additional-visuals",
    "href": "posts/Hw001/index.html#part-4-creating-two-additional-visuals",
    "title": "HW 1, Database",
    "section": "",
    "text": "from climate_database import analyze_seasonal_temperature_change\n\n\nimport inspect\nprint(inspect.getsource(analyze_seasonal_temperature_change))\n\n\ndb_file = \"db_file\"\n\n# Define the year range\nyear_range = (2000, 2020)\n\n# Define the country (optional)\ncountry = \"USA\"\n\n# Call the function\nanalyze_seasonal_temperature_change(db_file, year_range)\n\n\n\n\n\nimport plotly.express as px\n\ndef visualize_avg_monthly_temperature_by_country(df):\n    fig = px.line(df, x='Month', y='Avg_Temperature', title='Average Monthly Temperature by Country')\n    return fig\n\ndef get_data_from_database(db_file):\n    conn = sqlite3.connect(db_file)\n    query = \"SELECT * FROM temperatures\"  # Adjust your SQL query accordingly\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n    return df\n\n\nmy_df = get_data_from_database(\"db_file\")\nfig = visualize_avg_monthly_temperature_by_country(my_df)\n\n\ndef visualize_monthly_temperature_trends_for_station(df):\n    fig = px.line(df, x='Month', y='Temp', color='Year', title='Monthly Temperature Trends for a Specific Station')\n    return fig"
  },
  {
    "objectID": "posts/HW2/index.html",
    "href": "posts/HW2/index.html",
    "title": "HW 2, Web Scarpping",
    "section": "",
    "text": "In this blog we will go through on how to scrap a website. In this example we will formulate a list of all the actors who acted in the first Hunger Game movie and create a data fram with all the other movies in which they acted in to help recomand new movies to others.\n\n\nLocate the Url that you would be scrapping information from. For this blog we will use this website https://www.themoviedb.org/ and will scarpe a list of actors from the Hunger Game to create recomandtion system that gives users a list of the other movies the actors played in. \nWe First want to make sure that we are in the right enviroment and create a TMDB_scraper folder with all the files we will need by running the following lines in your terminal.\n\nPIC16B-24W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\n\n\n\n\n#### Part 1 We create a new file inside the spider directory and named it tmdb_spider.py and run the following lines\nTo run the code in the terminal we need to run this line with the correct added subdir for the movie page we chose\n“scrapy crawl tmdb_spider -o movies.csv -a subdir=”\n\n\nNext we will create and implemnt three diffrent parsing methods to get the url for each actor page and then to scrape their page and collect a list of all the movies they have acted in\nOnce we have create the intial function to be able to explore the movie we chose for we need to add the correct subdir .\nWhen going to the hunger game movie page the url is as follow : https://www.themoviedb.org/movie/70160-the-hunger-games When running the code in our terminal we can also just write 70160 for the subdir instead of also inclduing the movie name\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n    '''\n     Initialize the TmdbSpider instance.\n     '''\n    \n\nIn this parse method we create a way to navigate to our cast and crew page by adding /cast to our orginal url. This can be more easily visualize by going to the movie webiste. Once you are there select the full cast and crew page and you will get redirected to a new page. If you look at the url you would notice that it now changed to :https://www.themoviedb.org/movie/70160-the-hunger-games/cast\n\ndef parse(self, response):\n    ''''\n    We start on a movie page, and then  navigate to the Full Cast & Crew page\n    ''''\n            # Navigate to the Full Cast & Crew page\n            full_cast_url = f\"{response.url + '/cast/'}\"\n            # Yield a scrapy.Request for the page of each actor listed on the Full Cast & Crew page\n            yield scrapy.Request(full_cast_url, callback=self.parse_full_credits)\n            \n            \n        \n  \n\nNow that we are at the full cast and crew page the nect step is to go to the actors main page where we can get a list of all the Movies and TV shows they acted in. In order to understand how to find all the information we need we need to inspect the webpage we are at. When we inspect it we are able to use css or xpath to explore and navigate to the right urls. To locate the cast members we will use css to search within the h3 aspect. We will then go through the list of actors by examining ol. To get the actors url we ill examine the href attribute under the div class info\n\n  def parse_full_credits(self, response):\n        '''\n        Following the previous parse we start off at the main page with all the list of the cast and crew \n        \n        We will navigate to get a list of all the actors in The Hunger Games movie \n        \n        This will yiled a request that will get us to each of the actors main page where we \n        could get a list of the movies and TV shows they have been in \n        '''\n        actors_url=response.css('h3:contains(\"Cast\") + ol')\n        #actor_name=actors_url.css(\":: text\")\n        list_a=actors_url.css(\"div.info a::attr(href)\").getall()\n        \n        for url in list_a:\n            full_url = response.urljoin(url)\n            \n            yield scrapy.Request(full_url,callback=self.parse_actor_page) \n           \n\nNow that we are at the actors webapge we will use css to get a list of all the movies they were in. We also used the h2 element to collect all the actors name that were in the title of each of their webpage. Once we collect all the information we created a for loop that yoield a dictionary with the name of the actor and the movies and TV shows that they have been in.\n\ndef parse_actor_page(self, response):\n        \n        '''\n        In this section we assume that we are starting in the actore webapge\n    \n        We will be grabbing and formulating a list of all the movies they were in \n        \n        '''\n        # get the actor name, in this case we located in the title of the page \n        actor_name=response.css('h2.title &gt; a::text').get()\n        # this lines grabs the list of all the movies they have been in \n    \n        card_credit=response.xpath('//h3[text()=\"Acting\"]/following-sibling::table[1]')\n        movie_id=card_credit.css('td.role.true.account_adult_false.item_adult_false &gt; a.tooltip &gt; bdi::text').getall()\n        \n         #This will go through the list of all the movies we have collected and create a dictionary with all the movies and actors \n       \n        for movie in movie_id:\n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie}\n       \n\n\n\n\nOnce we have finished writing all the parsing for the class TmdbSpider we will run the following line with the correct subdir to create a .csv file for all of our actors and moves/ TV they have been in.\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=70160-the-hunger-games"
  },
  {
    "objectID": "posts/HW2/index.html#first-step",
    "href": "posts/HW2/index.html#first-step",
    "title": "HW 2, Web Scarpping",
    "section": "",
    "text": "Locate the Url that you would be scrapping information from. For this blog we will use this website https://www.themoviedb.org/ and will scarpe a list of actors from the Hunger Game to create recomandtion system that gives users a list of the other movies the actors played in. \nWe First want to make sure that we are in the right enviroment and create a TMDB_scraper folder with all the files we will need by running the following lines in your terminal.\n\nPIC16B-24W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper"
  },
  {
    "objectID": "posts/HW2/index.html#second-step",
    "href": "posts/HW2/index.html#second-step",
    "title": "HW 2, Web Scarpping",
    "section": "",
    "text": "#### Part 1 We create a new file inside the spider directory and named it tmdb_spider.py and run the following lines\nTo run the code in the terminal we need to run this line with the correct added subdir for the movie page we chose\n“scrapy crawl tmdb_spider -o movies.csv -a subdir=”\n\n\nNext we will create and implemnt three diffrent parsing methods to get the url for each actor page and then to scrape their page and collect a list of all the movies they have acted in\nOnce we have create the intial function to be able to explore the movie we chose for we need to add the correct subdir .\nWhen going to the hunger game movie page the url is as follow : https://www.themoviedb.org/movie/70160-the-hunger-games When running the code in our terminal we can also just write 70160 for the subdir instead of also inclduing the movie name\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n    '''\n     Initialize the TmdbSpider instance.\n     '''\n    \n\nIn this parse method we create a way to navigate to our cast and crew page by adding /cast to our orginal url. This can be more easily visualize by going to the movie webiste. Once you are there select the full cast and crew page and you will get redirected to a new page. If you look at the url you would notice that it now changed to :https://www.themoviedb.org/movie/70160-the-hunger-games/cast\n\ndef parse(self, response):\n    ''''\n    We start on a movie page, and then  navigate to the Full Cast & Crew page\n    ''''\n            # Navigate to the Full Cast & Crew page\n            full_cast_url = f\"{response.url + '/cast/'}\"\n            # Yield a scrapy.Request for the page of each actor listed on the Full Cast & Crew page\n            yield scrapy.Request(full_cast_url, callback=self.parse_full_credits)\n            \n            \n        \n  \n\nNow that we are at the full cast and crew page the nect step is to go to the actors main page where we can get a list of all the Movies and TV shows they acted in. In order to understand how to find all the information we need we need to inspect the webpage we are at. When we inspect it we are able to use css or xpath to explore and navigate to the right urls. To locate the cast members we will use css to search within the h3 aspect. We will then go through the list of actors by examining ol. To get the actors url we ill examine the href attribute under the div class info\n\n  def parse_full_credits(self, response):\n        '''\n        Following the previous parse we start off at the main page with all the list of the cast and crew \n        \n        We will navigate to get a list of all the actors in The Hunger Games movie \n        \n        This will yiled a request that will get us to each of the actors main page where we \n        could get a list of the movies and TV shows they have been in \n        '''\n        actors_url=response.css('h3:contains(\"Cast\") + ol')\n        #actor_name=actors_url.css(\":: text\")\n        list_a=actors_url.css(\"div.info a::attr(href)\").getall()\n        \n        for url in list_a:\n            full_url = response.urljoin(url)\n            \n            yield scrapy.Request(full_url,callback=self.parse_actor_page) \n           \n\nNow that we are at the actors webapge we will use css to get a list of all the movies they were in. We also used the h2 element to collect all the actors name that were in the title of each of their webpage. Once we collect all the information we created a for loop that yoield a dictionary with the name of the actor and the movies and TV shows that they have been in.\n\ndef parse_actor_page(self, response):\n        \n        '''\n        In this section we assume that we are starting in the actore webapge\n    \n        We will be grabbing and formulating a list of all the movies they were in \n        \n        '''\n        # get the actor name, in this case we located in the title of the page \n        actor_name=response.css('h2.title &gt; a::text').get()\n        # this lines grabs the list of all the movies they have been in \n    \n        card_credit=response.xpath('//h3[text()=\"Acting\"]/following-sibling::table[1]')\n        movie_id=card_credit.css('td.role.true.account_adult_false.item_adult_false &gt; a.tooltip &gt; bdi::text').getall()\n        \n         #This will go through the list of all the movies we have collected and create a dictionary with all the movies and actors \n       \n        for movie in movie_id:\n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie}\n       \n\n\n\n\nOnce we have finished writing all the parsing for the class TmdbSpider we will run the following line with the correct subdir to create a .csv file for all of our actors and moves/ TV they have been in.\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=70160-the-hunger-games"
  },
  {
    "objectID": "posts/Index.html",
    "href": "posts/Index.html",
    "title": "HW 3, Webapp",
    "section": "",
    "text": "To first start I created three HTML files, base.html, submit.html, and view.html. each one of them fullfill a diffrent porpose that I will explain below. The app.py file is the application script file where is the main file that will handle the flask functions and any other function that we will use in the creation of the webapp.\n\n\n\nThis HTML code sets up the main page that opens up when you open the URL. Within the first page, you are displayed with the two links, to either see the message or to submit a message, as well as a list of random messages that have been submitted previously. The other HTML sites will essentially inherent from the base page.\n&lt;!doctype html&gt; \n\n{% block title %}{% endblock %} - PIC16B Website\n\n\n/* this section changes the diffrent colors of the title*/\n\n\n\nPlease follow the submit a message link to submit your message.\n\n\n\n/* this displays the two links on the main page, one to submit a message, and the second one to view the message*/\n\nSubmit a Message\n\n\nView Messages\n\n\n\n\n\n{% block header %}{% endblock %}\n\n{% block content %}{% endblock %}\n\n\n/* this is a secondary header that will show a list of the random messages */\n\n\n\n{% for message in messages %} \n\n{{ message[1] }}: {{ message[2] }}\n\n{% endfor %}\n\n\n\n\n\n\nThis html takes care of the page in which indvdiuals submit their message. They are asked to provide their name and submit a message. Once submitting, they can then also chose to see any previous message or be directed to main page.\n{% extends ‘base.html’ %}\n{% block header %}\n\n{% endblock %}\n{% block content %}  /* this sets up the two links for people to either view the message or go back to the first main page */\n\nReturn to main page\n\nView Messages\n\n /* this will change the header to blue and the submit botton to green*/\n\n/* this section prompts the users to write their name and leave a message, and creates the submit botton*/\n\n&lt;label for-“name”&gt; Your name.  Please write a message.  \n\n\n{% endblock %}\n\n\n\nThis last html presents the messages that have been submmited and orders them inside a table so it is easier to view past messages. At the bottom of the post I also included the code for the style.css file. For now, the pourpose of that file is to modify the webapp by changing the color of the page, the font, the location of the text, and any other stylistic changes you wish to make.\n&lt;!doctype html&gt;\n\n\n\n\n\n\n/* this for loop goes through all the messages and organizes them inside a table */ {% for message in messages %}\n\n{% endfor %}\n\n\n\n\nID\n\n\nName\n\n\nMessage\n\n\n\n{{ message[0] }}\n\n\n{{ message[1] }}\n\n\n{{ message[2] }}\n\n\n/* this creats a link that allows people to go back and submit another message*/\n\n\n\n\n\n\nWhen working on this project you want to make sure your folder with all the files is organized in a certain way. Inside your project directory you want to have three main folders called: app.py, templates, and static. Inside the app.py you will have your functions, inside the templates you will add your base.html,submit.html, and view.html files. And lastly, inside the static folder you will have your style.css.\n\n\nNow that we have everything organized and set up we are ready to start writing the functions !\n\n\n\nfrom flask import Flask, g, render_template, request\nimport sqlite3\n\n\nimport sklearn as sk\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pickle\nimport os\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\nfrom matplotlib.figure import Figure\n\nimport io\nimport base64\n\n\n\n\n\nFor this project we will need to use route functions. A route function is a URL patter that Flask is able to read and respond to. To ensure that a specific function is called and run when the URL is opned we use “@app.route” to indicate which function we want to use.\nTo create the flask web server instance run the following code:\n\napp = Flask(__name__)\n\nBelow are all the route functions we will need to use, after that I will go into the main functions that theses route functions are refering back to.\nFirst we start by creating a database and naming it.The base function retrives 3 random messages and renders the base.html templetes, which also passes the messages to be displayed on the main page. The submit function takes the POST commend to add the message into the database. The submit.html templet is then rendered to prompt users to submit a message. Lastly, the message function retrives all the messages that were added to the database amd renders the view.html template which displays the messages to the users.\n\n\n\"\"\"\nThis script contains routes and functions for a Flask web application.\n\nIt defines routes for the main page, message submission, and viewing messages.\n\"\"\"\nDATABASE = 'messages_db.sqlite'\n \"\"\"\n    Renders the main page of the web application.\n\n    Retrieves 3 random messages and renders the 'base.html' template with these messages.\n    \"\"\"\n@app.route('/')\ndef base():\n    messages = random_messages(3)  # Retrieve 3 random messages\n    return render_template('base.html', messages=messages)\n\n  \"\"\"\n    Handles message submission.\n\n    If the request method is POST, it inserts the message into the database.\n    Renders the 'submit.html' template.\n   \"\"\"\n@app.route('/submit', methods=['GET','POST'])\ndef submit():\n    if request.method == 'POST':\n        insert_message(request)\n    return render_template('submit.html')\n\n\"\"\"\n    Displays all messages stored in the database.\n\n    Retrieves all messages from the database and renders the 'view.html' template with these messages.\n\"\"\"\n@app.route('/message')\ndef message():\n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute(\"SELECT * FROM messages\")\n    messages = cursor.fetchall()\n    cursor.close()\n    return render_template('view.html', messages=messages)\n\n\nIndentationError: unindent does not match any outer indentation level (&lt;tokenize&gt;, line 17)\n\n\n\n\n\nThe pourpose of this function is to handle the messages that are being submitted by creating a database. The function first checks the SQL database ( which is called message_db) to see if the database exists. If it does not, it creates a new one and stores it in g.message_db. Once that is done it followes the SQL command to create the message table with the added message, and end the connection once done.\n\n# Function to get the message database\n\"\"\"\n    Retrieve the SQLite database connection for storing messages.\n\n    If the 'message_db' attribute is not present in the global 'g' object, \n    it creates a new connection to the SQLite database defined by the \n    'DATABASE' constant. It also creates a 'messages' table in the database \n    if it doesn't already exist.\n\n    Returns:\n    - sqlite3.Connection: The SQLite database connection object.\n    \"\"\"\ndef get_message_db():\n    if 'message_db' not in g:\n        g.message_db = sqlite3.connect(DATABASE)\n        cursor = g.message_db.cursor()\n        cursor.execute('''CREATE TABLE IF NOT EXISTS messages (\n                            id INTEGER PRIMARY KEY,\n                            handle TEXT,\n                            message TEXT\n                        )''')\n        g.message_db.commit()\n    return g.message_db\n    \n\n\nWe then need to create a function that inserts the message into the database.It retracts the name of the user and their message from the form data using the request.form[‘nm’] and request.form[‘message’].Then, it executes an SQL query to insert the name and message into the ‘messages’ table in the database.\n\n\n\nThis function than deals with storing the messages submitted by the users. it calles the get_message_db to connect back to the database, connect to the cursor and then followes the SQL command to add the message with the id ( the name inputted by the user) to the message table. Once that is done it commits the function and closes the database connection again\n\ndef insert_message(request):\n     \"\"\"\n    Insert a user message into the database.\n\n    Args:\n    - request (flask.request): The request object containing form data.\n\n    Returns:\n    - str: The message that was inserted into the database.\n    \"\"\"\n    db = get_message_db()\n    cursor = db.cursor()\n    name = request.form['nm']\n    message = request.form['message']\n    cursor.execute(\"INSERT INTO messages (handle, message) VALUES (?, ?)\", (name, message))\n    db.commit()\n    cursor.close()\n    return message\n \n\nIndentationError: unindent does not match any outer indentation level (&lt;tokenize&gt;, line 11)\n\n\n\n\n\nLastly, the last two functions display the page that presents the list of random messages and randomizes the message that are being presented.The functions are used to rabndomize five diffrent messages from the database. This will be seen in our main page. The function works by connecting to the cursor, executing SQL command by selecting rnaodm messages before closing the database connection again.\n\n@app.route('/view_messages')\ndef view_messages():\n     \"\"\"\n    Display a page showing a random selection of messages.\n\n    Returns:\n    - flask.render_template: HTML page displaying the random messages.\n    \"\"\"\n    messages = random_messages(5)  # You can change the number of messages to retrieve\n    return render_template('view.html', messages=messages)\n\n# Function to retrieve n random messages from the database\ndef random_messages(n):\n    \"\"\"\n    Retrieve n random messages from the database.\n\n    Args:\n    - n (int): Number of random messages to retrieve.\n\n    Returns:\n    - list: A list of tuples containing the retrieved messages.\n    \"\"\"\n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute(\"SELECT * FROM messages ORDER BY RANDOM() LIMIT ?\", (n,))\n    messages = cursor.fetchall()\n    cursor.close()\n    db.close()  # Close the database connection\n    return messages\n\n\n\n\n\n\n\nset FLASK_ENV=app.py\nflask run\n\n\n\n\n\nAs you can see there are two links: submit message and view message in the main page. This is were our rout functions come into play. We are essentially calling submit.html and view.html routes in order to be able to be directed to another page\n\n\n\nbetter front page.png\n\n\n\n\n\n\nOnce opening the submit a message url you are prompted to this page. Like I talked about before, this is where we need to utalize the Get and Post methods in order to be able to repsond to HTTP request and send the datat of the submitted messages.\n\n\n\nbetter submit page.png\n\n\n\n\n\nThe last part is to send the users to a page where they can view their messages. This is where the message route message function and view.html come into play. Since we are not transmitting any data we will only be using the GET method in this route function. Below I added the route message again so we can further examine it\nThe function goes into the database with the messages and uses .fetchall() to select all the messages submitted by the user.Once it is done it makes sure to close the sursor again and return our view.html templet. By doing so we are able to see the page depicted below , with some of the message I wrote.\n\n@app.route('/message')\ndef message():\n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute(\"SELECT * FROM messages\")\n    messages = cursor.fetchall()\n    cursor.close()\n    return render_template('view.html', messages=messages)\n\n\n\n\ncolor message listy.png\n\n\n\n\n\n\nhtml {\n    font-family: Times New Roman;\n    background: aliceblue;\n    padding: 1rem;\n}\n\nbody {\n    background:aliceblue;\n    font-family: Times New Roman;\n    max-width: 900px;\n    margin: 0 auto;\n}\n\nh1 {\n    color: rgb(0, 0, 0);\n    font-family: Times New Roman;\n    margin: 1rem 0;\n    text-align: center;\n}"
  },
  {
    "objectID": "posts/Index.html#important-note",
    "href": "posts/Index.html#important-note",
    "title": "HW 3, Webapp",
    "section": "",
    "text": "When working on this project you want to make sure your folder with all the files is organized in a certain way. Inside your project directory you want to have three main folders called: app.py, templates, and static. Inside the app.py you will have your functions, inside the templates you will add your base.html,submit.html, and view.html files. And lastly, inside the static folder you will have your style.css.\n\n\nNow that we have everything organized and set up we are ready to start writing the functions !\n\n\n\nfrom flask import Flask, g, render_template, request\nimport sqlite3\n\n\nimport sklearn as sk\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pickle\nimport os\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\nfrom matplotlib.figure import Figure\n\nimport io\nimport base64\n\n\n\n\n\nFor this project we will need to use route functions. A route function is a URL patter that Flask is able to read and respond to. To ensure that a specific function is called and run when the URL is opned we use “@app.route” to indicate which function we want to use.\nTo create the flask web server instance run the following code:\n\napp = Flask(__name__)\n\nBelow are all the route functions we will need to use, after that I will go into the main functions that theses route functions are refering back to.\nFirst we start by creating a database and naming it.The base function retrives 3 random messages and renders the base.html templetes, which also passes the messages to be displayed on the main page. The submit function takes the POST commend to add the message into the database. The submit.html templet is then rendered to prompt users to submit a message. Lastly, the message function retrives all the messages that were added to the database amd renders the view.html template which displays the messages to the users.\n\n\n\"\"\"\nThis script contains routes and functions for a Flask web application.\n\nIt defines routes for the main page, message submission, and viewing messages.\n\"\"\"\nDATABASE = 'messages_db.sqlite'\n \"\"\"\n    Renders the main page of the web application.\n\n    Retrieves 3 random messages and renders the 'base.html' template with these messages.\n    \"\"\"\n@app.route('/')\ndef base():\n    messages = random_messages(3)  # Retrieve 3 random messages\n    return render_template('base.html', messages=messages)\n\n  \"\"\"\n    Handles message submission.\n\n    If the request method is POST, it inserts the message into the database.\n    Renders the 'submit.html' template.\n   \"\"\"\n@app.route('/submit', methods=['GET','POST'])\ndef submit():\n    if request.method == 'POST':\n        insert_message(request)\n    return render_template('submit.html')\n\n\"\"\"\n    Displays all messages stored in the database.\n\n    Retrieves all messages from the database and renders the 'view.html' template with these messages.\n\"\"\"\n@app.route('/message')\ndef message():\n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute(\"SELECT * FROM messages\")\n    messages = cursor.fetchall()\n    cursor.close()\n    return render_template('view.html', messages=messages)\n\n\nIndentationError: unindent does not match any outer indentation level (&lt;tokenize&gt;, line 17)\n\n\n\n\n\nThe pourpose of this function is to handle the messages that are being submitted by creating a database. The function first checks the SQL database ( which is called message_db) to see if the database exists. If it does not, it creates a new one and stores it in g.message_db. Once that is done it followes the SQL command to create the message table with the added message, and end the connection once done.\n\n# Function to get the message database\n\"\"\"\n    Retrieve the SQLite database connection for storing messages.\n\n    If the 'message_db' attribute is not present in the global 'g' object, \n    it creates a new connection to the SQLite database defined by the \n    'DATABASE' constant. It also creates a 'messages' table in the database \n    if it doesn't already exist.\n\n    Returns:\n    - sqlite3.Connection: The SQLite database connection object.\n    \"\"\"\ndef get_message_db():\n    if 'message_db' not in g:\n        g.message_db = sqlite3.connect(DATABASE)\n        cursor = g.message_db.cursor()\n        cursor.execute('''CREATE TABLE IF NOT EXISTS messages (\n                            id INTEGER PRIMARY KEY,\n                            handle TEXT,\n                            message TEXT\n                        )''')\n        g.message_db.commit()\n    return g.message_db\n    \n\n\nWe then need to create a function that inserts the message into the database.It retracts the name of the user and their message from the form data using the request.form[‘nm’] and request.form[‘message’].Then, it executes an SQL query to insert the name and message into the ‘messages’ table in the database.\n\n\n\nThis function than deals with storing the messages submitted by the users. it calles the get_message_db to connect back to the database, connect to the cursor and then followes the SQL command to add the message with the id ( the name inputted by the user) to the message table. Once that is done it commits the function and closes the database connection again\n\ndef insert_message(request):\n     \"\"\"\n    Insert a user message into the database.\n\n    Args:\n    - request (flask.request): The request object containing form data.\n\n    Returns:\n    - str: The message that was inserted into the database.\n    \"\"\"\n    db = get_message_db()\n    cursor = db.cursor()\n    name = request.form['nm']\n    message = request.form['message']\n    cursor.execute(\"INSERT INTO messages (handle, message) VALUES (?, ?)\", (name, message))\n    db.commit()\n    cursor.close()\n    return message\n \n\nIndentationError: unindent does not match any outer indentation level (&lt;tokenize&gt;, line 11)\n\n\n\n\n\nLastly, the last two functions display the page that presents the list of random messages and randomizes the message that are being presented.The functions are used to rabndomize five diffrent messages from the database. This will be seen in our main page. The function works by connecting to the cursor, executing SQL command by selecting rnaodm messages before closing the database connection again.\n\n@app.route('/view_messages')\ndef view_messages():\n     \"\"\"\n    Display a page showing a random selection of messages.\n\n    Returns:\n    - flask.render_template: HTML page displaying the random messages.\n    \"\"\"\n    messages = random_messages(5)  # You can change the number of messages to retrieve\n    return render_template('view.html', messages=messages)\n\n# Function to retrieve n random messages from the database\ndef random_messages(n):\n    \"\"\"\n    Retrieve n random messages from the database.\n\n    Args:\n    - n (int): Number of random messages to retrieve.\n\n    Returns:\n    - list: A list of tuples containing the retrieved messages.\n    \"\"\"\n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute(\"SELECT * FROM messages ORDER BY RANDOM() LIMIT ?\", (n,))\n    messages = cursor.fetchall()\n    cursor.close()\n    db.close()  # Close the database connection\n    return messages"
  },
  {
    "objectID": "posts/Index.html#now-is-the-fun-part",
    "href": "posts/Index.html#now-is-the-fun-part",
    "title": "HW 3, Webapp",
    "section": "",
    "text": "set FLASK_ENV=app.py\nflask run\n\n\n\n\n\nAs you can see there are two links: submit message and view message in the main page. This is were our rout functions come into play. We are essentially calling submit.html and view.html routes in order to be able to be directed to another page\n\n\n\nbetter front page.png\n\n\n\n\n\n\nOnce opening the submit a message url you are prompted to this page. Like I talked about before, this is where we need to utalize the Get and Post methods in order to be able to repsond to HTTP request and send the datat of the submitted messages.\n\n\n\nbetter submit page.png\n\n\n\n\n\nThe last part is to send the users to a page where they can view their messages. This is where the message route message function and view.html come into play. Since we are not transmitting any data we will only be using the GET method in this route function. Below I added the route message again so we can further examine it\nThe function goes into the database with the messages and uses .fetchall() to select all the messages submitted by the user.Once it is done it makes sure to close the sursor again and return our view.html templet. By doing so we are able to see the page depicted below , with some of the message I wrote.\n\n@app.route('/message')\ndef message():\n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute(\"SELECT * FROM messages\")\n    messages = cursor.fetchall()\n    cursor.close()\n    return render_template('view.html', messages=messages)\n\n\n\n\ncolor message listy.png\n\n\n\n\n\n\nhtml {\n    font-family: Times New Roman;\n    background: aliceblue;\n    padding: 1rem;\n}\n\nbody {\n    background:aliceblue;\n    font-family: Times New Roman;\n    max-width: 900px;\n    margin: 0 auto;\n}\n\nh1 {\n    color: rgb(0, 0, 0);\n    font-family: Times New Roman;\n    margin: 1rem 0;\n    text-align: center;\n}"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]