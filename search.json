[
  {
    "objectID": "TMDB_scraper/hw2blog.html",
    "href": "TMDB_scraper/hw2blog.html",
    "title": "HW 2, Web Scarpping",
    "section": "",
    "text": "In this blog we will go through on how to scrap a website. In this example we will formulate a list of all the actors who acted in the first Hunger Game movie and create a data fram with all the other movies in which they acted in to help recomand new movies to others.\n\n\nLocate the Url that you would be scrapping information from. For this blog we will use this website https://www.themoviedb.org/ and will scarpe a list of actors from the Hunger Game to create recomandtion system that gives users a list of the other movies the actors played in. \nWe First want to make sure that we are in the right enviroment and create a TMDB_scraper folder with all the files we will need by running the following lines in your terminal.\n\nPIC16B-24W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\n\n\n\n\n#### Part 1 We create a new file inside the spider directory and named it tmdb_spider.py and run the following lines\nTo run the code in the terminal we need to run this line with the correct added subdir for the movie page we chose\n“scrapy crawl tmdb_spider -o movies.csv -a subdir=”\n\n\nNext we will create and implemnt three diffrent parsing methods to get the url for each actor page and then to scrape their page and collect a list of all the movies they have acted in\nOnce we have create the intial function to be able to explore the movie we chose for we need to add the correct subdir .\nWhen going to the hunger game movie page the url is as follow : https://www.themoviedb.org/movie/70160-the-hunger-games When running the code in our terminal we can also just write 70160 for the subdir instead of also inclduing the movie name\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n    '''\n     Initialize the TmdbSpider instance.\n     '''\n    \n\nIn this parse method we create a way to navigate to our cast and crew page by adding /cast to our orginal url. This can be more easily visualize by going to the movie webiste. Once you are there select the full cast and crew page and you will get redirected to a new page. If you look at the url you would notice that it now changed to :https://www.themoviedb.org/movie/70160-the-hunger-games/cast\n\ndef parse(self, response):\n    ''''\n    We start on a movie page, and then  navigate to the Full Cast & Crew page\n    ''''\n            # Navigate to the Full Cast & Crew page\n            full_cast_url = f\"{response.url + '/cast/'}\"\n            # Yield a scrapy.Request for the page of each actor listed on the Full Cast & Crew page\n            yield scrapy.Request(full_cast_url, callback=self.parse_full_credits)\n            \n            \n        \n  \n\nNow that we are at the full cast and crew page the nect step is to go to the actors main page where we can get a list of all the Movies and TV shows they acted in. In order to understand how to find all the information we need we need to inspect the webpage we are at. When we inspect it we are able to use css or xpath to explore and navigate to the right urls. To locate the cast members we will use css to search within the h3 aspect. We will then go through the list of actors by examining ol. To get the actors url we ill examine the href attribute under the div class info\n\n  def parse_full_credits(self, response):\n        '''\n        Following the previous parse we start off at the main page with all the list of the cast and crew \n        \n        We will navigate to get a list of all the actors in The Hunger Games movie \n        \n        This will yiled a request that will get us to each of the actors main page where we \n        could get a list of the movies and TV shows they have been in \n        '''\n        actors_url=response.css('h3:contains(\"Cast\") + ol')\n        #actor_name=actors_url.css(\":: text\")\n        list_a=actors_url.css(\"div.info a::attr(href)\").getall()\n        \n        for url in list_a:\n            full_url = response.urljoin(url)\n            \n            yield scrapy.Request(full_url,callback=self.parse_actor_page) \n           \n\nNow that we are at the actors webapge we will use css to get a list of all the movies they were in. We also used the h2 element to collect all the actors name that were in the title of each of their webpage. Once we collect all the information we created a for loop that yoield a dictionary with the name of the actor and the movies and TV shows that they have been in.\n\ndef parse_actor_page(self, response):\n        \n        '''\n        In this section we assume that we are starting in the actore webapge\n    \n        We will be grabbing and formulating a list of all the movies they were in \n        \n        '''\n        # get the actor name, in this case we located in the title of the page \n        actor_name=response.css('h2.title &gt; a::text').get()\n        # this lines grabs the list of all the movies they have been in \n    \n        card_credit=response.xpath('//h3[text()=\"Acting\"]/following-sibling::table[1]')\n        movie_id=card_credit.css('td.role.true.account_adult_false.item_adult_false &gt; a.tooltip &gt; bdi::text').getall()\n        \n         #This will go through the list of all the movies we have collected and create a dictionary with all the movies and actors \n       \n        for movie in movie_id:\n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie}\n       \n\n\n\n\nOnce we have finished writing all the parsing for the class TmdbSpider we will run the following line with the correct subdir to create a .csv file for all of our actors and moves/ TV they have been in.\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=70160-the-hunger-games"
  },
  {
    "objectID": "TMDB_scraper/hw2blog.html#first-step",
    "href": "TMDB_scraper/hw2blog.html#first-step",
    "title": "HW 2, Web Scarpping",
    "section": "",
    "text": "Locate the Url that you would be scrapping information from. For this blog we will use this website https://www.themoviedb.org/ and will scarpe a list of actors from the Hunger Game to create recomandtion system that gives users a list of the other movies the actors played in. \nWe First want to make sure that we are in the right enviroment and create a TMDB_scraper folder with all the files we will need by running the following lines in your terminal.\n\nPIC16B-24W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper"
  },
  {
    "objectID": "TMDB_scraper/hw2blog.html#second-step",
    "href": "TMDB_scraper/hw2blog.html#second-step",
    "title": "HW 2, Web Scarpping",
    "section": "",
    "text": "#### Part 1 We create a new file inside the spider directory and named it tmdb_spider.py and run the following lines\nTo run the code in the terminal we need to run this line with the correct added subdir for the movie page we chose\n“scrapy crawl tmdb_spider -o movies.csv -a subdir=”\n\n\nNext we will create and implemnt three diffrent parsing methods to get the url for each actor page and then to scrape their page and collect a list of all the movies they have acted in\nOnce we have create the intial function to be able to explore the movie we chose for we need to add the correct subdir .\nWhen going to the hunger game movie page the url is as follow : https://www.themoviedb.org/movie/70160-the-hunger-games When running the code in our terminal we can also just write 70160 for the subdir instead of also inclduing the movie name\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n    '''\n     Initialize the TmdbSpider instance.\n     '''\n    \n\nIn this parse method we create a way to navigate to our cast and crew page by adding /cast to our orginal url. This can be more easily visualize by going to the movie webiste. Once you are there select the full cast and crew page and you will get redirected to a new page. If you look at the url you would notice that it now changed to :https://www.themoviedb.org/movie/70160-the-hunger-games/cast\n\ndef parse(self, response):\n    ''''\n    We start on a movie page, and then  navigate to the Full Cast & Crew page\n    ''''\n            # Navigate to the Full Cast & Crew page\n            full_cast_url = f\"{response.url + '/cast/'}\"\n            # Yield a scrapy.Request for the page of each actor listed on the Full Cast & Crew page\n            yield scrapy.Request(full_cast_url, callback=self.parse_full_credits)\n            \n            \n        \n  \n\nNow that we are at the full cast and crew page the nect step is to go to the actors main page where we can get a list of all the Movies and TV shows they acted in. In order to understand how to find all the information we need we need to inspect the webpage we are at. When we inspect it we are able to use css or xpath to explore and navigate to the right urls. To locate the cast members we will use css to search within the h3 aspect. We will then go through the list of actors by examining ol. To get the actors url we ill examine the href attribute under the div class info\n\n  def parse_full_credits(self, response):\n        '''\n        Following the previous parse we start off at the main page with all the list of the cast and crew \n        \n        We will navigate to get a list of all the actors in The Hunger Games movie \n        \n        This will yiled a request that will get us to each of the actors main page where we \n        could get a list of the movies and TV shows they have been in \n        '''\n        actors_url=response.css('h3:contains(\"Cast\") + ol')\n        #actor_name=actors_url.css(\":: text\")\n        list_a=actors_url.css(\"div.info a::attr(href)\").getall()\n        \n        for url in list_a:\n            full_url = response.urljoin(url)\n            \n            yield scrapy.Request(full_url,callback=self.parse_actor_page) \n           \n\nNow that we are at the actors webapge we will use css to get a list of all the movies they were in. We also used the h2 element to collect all the actors name that were in the title of each of their webpage. Once we collect all the information we created a for loop that yoield a dictionary with the name of the actor and the movies and TV shows that they have been in.\n\ndef parse_actor_page(self, response):\n        \n        '''\n        In this section we assume that we are starting in the actore webapge\n    \n        We will be grabbing and formulating a list of all the movies they were in \n        \n        '''\n        # get the actor name, in this case we located in the title of the page \n        actor_name=response.css('h2.title &gt; a::text').get()\n        # this lines grabs the list of all the movies they have been in \n    \n        card_credit=response.xpath('//h3[text()=\"Acting\"]/following-sibling::table[1]')\n        movie_id=card_credit.css('td.role.true.account_adult_false.item_adult_false &gt; a.tooltip &gt; bdi::text').getall()\n        \n         #This will go through the list of all the movies we have collected and create a dictionary with all the movies and actors \n       \n        for movie in movie_id:\n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie}\n       \n\n\n\n\nOnce we have finished writing all the parsing for the class TmdbSpider we will run the following line with the correct subdir to create a .csv file for all of our actors and moves/ TV they have been in.\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=70160-the-hunger-games"
  },
  {
    "objectID": "posts/Untitled.html",
    "href": "posts/Untitled.html",
    "title": "HW 6, Fake News Classification",
    "section": "",
    "text": "HW 6"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/HW6/index.html",
    "href": "posts/HW6/index.html",
    "title": "HW 6: Fake News Classification",
    "section": "",
    "text": "In this blog we work with Keras to create a model that can classify fake news. We will aquire data, create a database, construct the models, train them, and evalute their accuracy before we also include some visualization. There is so much work to do so lets get started!"
  },
  {
    "objectID": "posts/HW6/index.html#step-1-acquire-training-data",
    "href": "posts/HW6/index.html#step-1-acquire-training-data",
    "title": "HW 6: Fake News Classification",
    "section": "Step 1 : Acquire Training Data",
    "text": "Step 1 : Acquire Training Data\nTo start off we will need to get data that contianes both fake and real news so we can train our model with it. For this blog we will use the article by Ahmed H. and Saad s 2017\n(Ahmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).)\nFeel free to using the same database to follow along with the blog post more easily.\nFirst step is to import any package we might need for the creation of the model\n\n!pip install keras --upgrade\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.1.1)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.7)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.10.0)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\nRequirement already satisfied: typing-extensions&gt;=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree-&gt;keras) (4.10.0)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\n\n\n\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\n\nimport re\nimport string\nfrom sklearn.model_selection import train_test_split\nfrom keras import layers, losses\nfrom keras.layers import TextVectorization\n\nimport keras\n\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\n\nOnce we imported the pandas package we can creat the train_url variable with the url for our database csv and read the database into python by calling pd.read_csv()\n\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\ntrain_df=pd.read_csv(train_url)\ntrain_df\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n...\n...\n...\n...\n...\n\n\n22444\n10709\nALARMING: NSA Refuses to Release Clinton-Lynch...\nIf Clinton and Lynch just talked about grandki...\n1\n\n\n22445\n8731\nCan Pence's vow not to sling mud survive a Tru...\n() - In 1990, during a close and bitter congre...\n0\n\n\n22446\n4733\nWatch Trump Campaign Try To Spin Their Way Ou...\nA new ad by the Hillary Clinton SuperPac Prior...\n1\n\n\n22447\n3993\nTrump celebrates first 100 days as president, ...\nHARRISBURG, Pa.U.S. President Donald Trump hit...\n0\n\n\n22448\n12896\nTRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...\nMELBOURNE, FL is a town with a population of 7...\n1\n\n\n\n\n\n22449 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nAs you can see we are working with a table that three main columes that we will be working with. The title column includes the title of the article, the text column provides us with the entier artcile text, while the last column, fake, tells us if the artcile has fake news or is real. 0 for true and 1 for fake news.\nNow we can start creating a dataset, lets get started"
  },
  {
    "objectID": "posts/HW6/index.html#step-2-make-a-dataset",
    "href": "posts/HW6/index.html#step-2-make-a-dataset",
    "title": "HW 6: Fake News Classification",
    "section": "Step 2: Make a Dataset",
    "text": "Step 2: Make a Dataset\nWe will creat a function called make_dataset which will do three things\nThe function will change all the text to lowercase,remove any stopwords( unimportant or essential words), and lastly construct and retrun a tf.data.Dataset which will have to two inputs and one output. This might seem like a lot but don’t worry we will break it down and go step by step.\nTo first start we need to import the nltk package and get the stopwords from the nltk.corpus libary to make the function easier to create. nltk. download, downloads the list of English stopwords (common words that are usually removed in the preprocessing steps of text analysis because they carry less meaningful information for analysis, e.g., “the”, “is”, “in”). Now we are ready to start working on the function.\nThe first step is to convert all the article text and title text to lowercase as it would impact the way the words are seen. Mor specifcially, the model views “Hello” and “hello” to be diffrent things which will make us have to work double as much.\nThe stopword removal step is integrated directly into the apply method calls for both the ‘text’ and ‘title’ columns. A lambda function is used to iterate over each word in the input text, filter out stopwords, and then join the remaining words back into a single string.\nWe then creat a TensorFlow dataset from the preprocessed ‘text’ and ‘title’ columns along with the target ‘fake’ column. This dataset is suitable for feeding into a machine learning model built with TensorFlow. Let’s break down how to do it.\ntf.data.Dataset.from_tensor_slices(…) if a TensorFlow function which creates a Dataset object from tensor slices. It is meant to handel large amounts of data in an efficent manner and allows you to batch and shuffle the data in a very simple manner. “from_tensor_slices” is a tuple with two elements, thje first is the text and title keys- each corresponding to a numpy array from the training_df and conatine all the values from the colum. The second tuple has the keys of wether the text is false or not. Lastly, dataset.batch(100) batchs consective elemnts of the datasets. It is important we batch as it is as it allows for parallel computation over the batch, reducing training time. It also helps with memory management, as it limits the amount of data loaded into memory at once.\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\ndef make_dataset(df):\n    \"\"\"\n    Preprocesses the input DataFrame by converting text to lowercase, removing stopwords,\n    and constructing a TensorFlow dataset.\n\n    Parameters:\n    - train_df: A pandas DataFrame with columns 'text', 'title', and 'fake'.\n\n    Returns:\n    - A batched TensorFlow dataset containing preprocessed text and titles as features\n      and 'fake' column values as labels.\n    \"\"\"\n    # Convert text to lowercase\n    df['text'] = df['text'].apply(lambda x: x.lower())\n    df['title'] = df['title'].apply(lambda x: x.lower())\n    #train_df['title'] = train_df['title'].apply(lambda x: x.lower())\n\n    # Directly remove stopwords from text and title using a lambda function\n    df['text'] = df['text'].apply(lambda input_text: \" \".join([word for word in input_text.split() if word not in stop_words]))\n    df['title'] = df['title'].apply(lambda input_text: \" \".join([word for word in input_text.split() if word not in stop_words]))\n\n    title_tensor = tf.constant(df['title'].values, dtype=tf.string)\n    text_tensor = tf.constant(df['text'].values, dtype=tf.string)\n    fake_tensor = tf.constant(df['fake'].values, dtype=tf.int32)\n\n\n\n    # Construct the dataset\n    dataset = tf.data.Dataset.from_tensor_slices((\n      {\"title\": df['title'].values, \"text\": df['text'].values},\n        df['fake'].values\n    ))\n\n    # Batch the dataset\n    dataset = dataset.batch(100)\n\n    return dataset\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\nSpliting data: get validation data\nNow that we have created our main dataset we can go ahead and split off 20% of our data to use for validation. When creating a model it is important to divided you data, so you cna use some of it to train and some to test. The pourpose of this is so when you finish training you model you can test your model on data that it has not yet been exposed to. Initially, the dataset is shuffled using train_df.shuffle(buffer_size=len(train_df), reshuffle_each_iteration=False), ensuring a random distribution of data each time the operation is executed, with buffer_size equal to the dataset’s length to shuffle all data at once. The reshuffling for every iteration is disabled to maintain the order once established. Following this, a validation set size is determined as 20% of the total dataset (val_size = int(0.2 * len(train_df))).\nThe dataset is then divided, with the validation subset being extracted first using val_dataset = train_df.take(val_size) to take the first 20% of the data after shuffling. The remaining 80% constitutes the training dataset, obtained through train_dataset = train_df.skip(val_size), skipping the records already allocated to the validation set.\nTo further refine these subsets for model ingestion, the make_dataset function is invoked on both train_dataset and val_dataset. This function is tasked with cleaning and structuring the data into a format amenable to TensorFlow, performing necessary preprocessing such as text data transformation. The resultant TensorFlow datasets are batched and prepped, ready for model training and validation phases, respectively. This comprehensive preparation ensures the datasets are optimally configured for efficient model training and validation, fostering effective learning and model assessment.\n\ntrain_df= make_dataset(train_df)\n\n\n# Shuffle the data and determine the validation set size\ntrain_df= train_df.shuffle(buffer_size = len(train_df), reshuffle_each_iteration=False)\nval_size = int(0.2 * len(train_df))\n\n# Split the dataset\nval_dataset  = train_df.take(val_size)\ntrain_dataset = train_df.skip(val_size)\n\n\n\nBase Rate\nNext to be able to measure our progress and test the accuracy of the model we need to establish a base accuracy rate. We will determine the vase rate for this data set by examining the labels on the training set. When we run the code we see that our start rate is 52% which is a good start. Now lets\n\nlabels = []\n\n# Iterate through the dataset to collect labels\nfor _, label in train_dataset.unbatch().as_numpy_iterator():\n    labels.append(label)\n\n# Convert labels list to numpy array for easy frequency calculation\nlabels = np.array(labels)\n\n# Calculate frequencies of each label\nvalues, counts = np.unique(labels, return_counts=True)\n# Calculate the base rate (max frequency)\nbase_rate = counts.max() / counts.sum()\n\n#base_rate = train_df['fake'].value_counts(normalize=True).max()\nprint(f\"Base rate: {base_rate:.2f}\")\n\nBase rate: 0.52\n\n\n\n\nTextVectorization\nWhen we creat machine learning models they work with numerical data rather, howvere since our database examines artciles we currently have raw text. Therefore, we need to creat a textvectorization layer to transform the text into a format that our model can use to train on. Let’s breakdown how to creat such a layer.\n1st step: we need to set a maximum number of words to keep in the vocabulary. For this model we will consider the top 2,000 most frequent words in the dataset which should cover in our instances all if not most words.\n2nd step: We creat a function that defines a custom standarization to preprocess text data. It converst all the text to lowercase and removes any punctation. It is important we due thois as it helps limit unnecessary variability within our data and makes it easier for our models to learn better.\n3rd step: We then creat an instance of the TextVectorization layer using the previously defined standardization function. This converts text into numerical data that can be fed into a TensorFlow model. The max_tokens Sets the size of the vocabulary to the value of size_vocabulary and the output_mode=‘int’ has the layer output integer indices representing the words in the vocabulary. Lastly, output_sequence_length=500 ensures that all output sequences have a fixed length of 500 tokens.In the case that the text is shorter zeros will be added to it.\n4th step:The last step is to fit the layer to the training data using adapt. It learns the vocabulary from the ‘title’ field of the training dataset.\n\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation\n\ntitle_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\ntitle_vectorize_layer.adapt(train_dataset.map(lambda x, y: x[\"title\"]))\n\nWe can now do the exact same things for the the text\n\ntext_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\ntext_vectorize_layer.adapt(train_dataset.map(lambda x, y: x[\"text\"]))\n\nWe also need to creat an embedding layer which is a transforme which will convert words into dense vectors that we can work with. Theses vector formulat the connection between the words and their conext\n\nembedding_layer1 = layers.Embedding(size_vocabulary, 10, name=\"embedding\")"
  },
  {
    "objectID": "posts/HW6/index.html#step-3-create-models",
    "href": "posts/HW6/index.html#step-3-create-models",
    "title": "HW 6: Fake News Classification",
    "section": "Step 3: Create models",
    "text": "Step 3: Create models\nNext step is to creat a model that will detect if the news are fake or not. We will actually creat three models. One will use only use the artcile title as input, one will only use artical text as input, and the last one will use both article title and text as input. We are creating the three models so we could determine wether it is most effective to look at the title, full text or both when trying to detect fake news.\nWe have a lot of wrok ahead of us lets get started!\n\nModel 1: only article title as input\nLike before we are going to need to import some more packages that will be used in creating the model. From tensorflow.keras we are importing Model which is used to instantiate a new model, and Input which is used to specify the input layer of the model.Dense and GlobalAveragePooling1D layers are also imported from tensorflow.keras.layers. Dense is a fully connected neural network layer, and GlobalAveragePooling1D is used for pooling operations.\nThe initial step in constructing the model involves establishing an embedding layer, which plays a crucial role in processing text within neural networks. This layer converts positive integers (indexes) into dense vectors of a fixed size (output_dim). The first parameter, input_dim, represents the size of the vocabulary (size_vocabulary) plus one, to accommodate zero padding (as previously mentioned, zeros will be added to shorter sequences). This layer is subsequently applied to the vectorized titles.\ntitle_input is designed to receive a single string at a time, representing the article’s title. It’s essential to define this input to specify the shape and data type the model will receive. The title_input is then processed through a TextVectorization layer to convert the string into sequences of integers that the model can interpret. Following this, the sequence of integers is passed through the embedding layer, transforming it into a sequence of dense vectors.\nNext, a GlobalAveragePooling1D layer is created, which pools features by averaging across the sequence dimension of the embeddings, thus reducing the output to a fixed-length vector. This process condenses the information from the entire title into a format suitable for the prediction layer, simplifying the network’s processing task.\nA dense layer is then established, ensuring the output is between 0 and 1, making it suitable for binary classification tasks.\nThe model is defined with specific inputs and outputs and is then ready to be compiled. It is compiled using the Adam optimizer and the binary_crossentropy loss function, appropriate for binary classification tasks.\n\nfrom tensorflow.keras import Model, Input\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling1D\n\n\n# Assuming you have already defined the title_input\ntitle_input = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"title\")\n\n# Create the model for title input\ntitle_features = title_vectorize_layer(title_input)\ntitle_features = embedding_layer1(title_features)\ntitle_features = layers.Dropout(0.5)(title_features)\ntitle_features = layers.Conv1D(128, 5, activation='relu')(title_features)\ntitle_features = layers.MaxPooling1D(5)(title_features)\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dense(64, activation='relu')(title_features)\ntitle_features = layers.Dropout(0.3)(title_features)\ntitle_features = layers.Dense(32, activation='relu')(title_features)\n\n# Define the output layer for title input\ntitle_output = layers.Dense(1, activation='sigmoid', name=\"title_output\")(title_features)\n\nmodel_title = tf.keras.Model(inputs=title_input, outputs=title_output)\n\nmodel_title.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel_title.summary()\n\nModel: \"functional_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)                   │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization                   │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding (Embedding)                │ (None, 500, 10)             │          20,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (Dropout)                    │ (None, 500, 10)             │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv1d (Conv1D)                      │ (None, 496, 128)            │           6,528 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling1d (MaxPooling1D)         │ (None, 99, 128)             │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d             │ (None, 128)                 │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (Dense)                        │ (None, 64)                  │           8,256 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (Dropout)                  │ (None, 64)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (Dense)                      │ (None, 32)                  │           2,080 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ title_output (Dense)                 │ (None, 1)                   │              33 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 36,897 (144.13 KB)\n\n\n\n Trainable params: 36,897 (144.13 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# Fit the model\nfrom keras import utils\n\nutils.plot_model(model_title, \"model_title.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\nrun_1=model_title.fit(train_dataset, validation_data=val_dataset, epochs=20)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 13ms/step - accuracy: 0.5214 - loss: 0.6924 - val_accuracy: 0.7229 - val_loss: 0.6669\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 10ms/step - accuracy: 0.7161 - loss: 0.5483 - val_accuracy: 0.9191 - val_loss: 0.2328\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.8965 - loss: 0.2548 - val_accuracy: 0.9349 - val_loss: 0.1897\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9251 - loss: 0.1950 - val_accuracy: 0.9342 - val_loss: 0.1819\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9334 - loss: 0.1784 - val_accuracy: 0.9336 - val_loss: 0.1777\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9377 - loss: 0.1708 - val_accuracy: 0.9371 - val_loss: 0.1722\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9438 - loss: 0.1554 - val_accuracy: 0.9373 - val_loss: 0.1668\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9417 - loss: 0.1550 - val_accuracy: 0.9398 - val_loss: 0.1655\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 9ms/step - accuracy: 0.9492 - loss: 0.1432 - val_accuracy: 0.9382 - val_loss: 0.1648\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9469 - loss: 0.1430 - val_accuracy: 0.9382 - val_loss: 0.1646\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.9488 - loss: 0.1389 - val_accuracy: 0.9373 - val_loss: 0.1638\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9546 - loss: 0.1339 - val_accuracy: 0.9382 - val_loss: 0.1630\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9539 - loss: 0.1319 - val_accuracy: 0.9393 - val_loss: 0.1627\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 9ms/step - accuracy: 0.9541 - loss: 0.1277 - val_accuracy: 0.9373 - val_loss: 0.1640\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 10ms/step - accuracy: 0.9566 - loss: 0.1230 - val_accuracy: 0.9389 - val_loss: 0.1621\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.9546 - loss: 0.1256 - val_accuracy: 0.9391 - val_loss: 0.1636\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9588 - loss: 0.1185 - val_accuracy: 0.9411 - val_loss: 0.1624\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9588 - loss: 0.1167 - val_accuracy: 0.9411 - val_loss: 0.1626\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9596 - loss: 0.1159 - val_accuracy: 0.9411 - val_loss: 0.1659\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 10ms/step - accuracy: 0.9607 - loss: 0.1125 - val_accuracy: 0.9404 - val_loss: 0.1645\n\n\nThe validation accuracy predictions look pretty good !! The went from 50% all the way up to 90% With the majority of the rounds being in the mid to high 80% !! So far using the title for accuracy seems like a very good option and helps us creat a strong model\n\nutils.plot_model(model_title, \"model_title.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n# To visualize the results of the training using matplotlib\nimport matplotlib.pyplot as plt\nplt.plot(run_1.history['accuracy'], label='Training Accuracy')\nplt.plot(run_1.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model_Titles')\nplt.legend()\nplt.show()\n\n\n\n\n\nModel 2: only article text an input\nFor the second model we will follow the same tasks as the first model but simply shift from the title to text.\n\n\ntext_input = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"text\")\n\n\ntext_features = text_vectorize_layer(text_input)\ntext_features = embedding_layer1(text_features)\ntext_features = layers.Dropout(0.5)(text_features)\ntext_features = layers.Conv1D(128, 5, activation='relu')(text_features)\ntext_features = layers.MaxPooling1D(5)(text_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dense(64, activation='relu')(text_features)\ntext_features = layers.Dropout(0.3)(text_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\n\n\ntext_output = layers.Dense(1, activation='sigmoid', name=\"text_output\")(text_features)\n\nmodel_text = tf.keras.Model(inputs=text_input, outputs=text_output)\nmodel_text.summary()\n\nModel: \"functional_3\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ text (InputLayer)                    │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization_1                 │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding (Embedding)                │ (None, 500, 10)             │          20,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (Dropout)                  │ (None, 500, 10)             │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv1d_1 (Conv1D)                    │ (None, 496, 128)            │           6,528 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling1d_1 (MaxPooling1D)       │ (None, 99, 128)             │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_1           │ (None, 128)                 │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (Dense)                      │ (None, 64)                  │           8,256 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (Dropout)                  │ (None, 64)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (Dense)                      │ (None, 32)                  │           2,080 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_output (Dense)                  │ (None, 1)                   │              33 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 36,897 (144.13 KB)\n\n\n\n Trainable params: 36,897 (144.13 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# Fit the model\nmodel_text.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nrun_2=model_text.fit(train_dataset, validation_data=val_dataset, epochs=20)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 18ms/step - accuracy: 0.6171 - loss: 0.6224 - val_accuracy: 0.9484 - val_loss: 0.1890\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9389 - loss: 0.1813 - val_accuracy: 0.9633 - val_loss: 0.1190\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9611 - loss: 0.1155 - val_accuracy: 0.9724 - val_loss: 0.0936\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9682 - loss: 0.0907 - val_accuracy: 0.9764 - val_loss: 0.0827\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.9743 - loss: 0.0758 - val_accuracy: 0.9744 - val_loss: 0.0770\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9785 - loss: 0.0621 - val_accuracy: 0.9762 - val_loss: 0.0746\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.9800 - loss: 0.0563 - val_accuracy: 0.9793 - val_loss: 0.0706\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 17ms/step - accuracy: 0.9825 - loss: 0.0495 - val_accuracy: 0.9818 - val_loss: 0.0660\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 17ms/step - accuracy: 0.9855 - loss: 0.0415 - val_accuracy: 0.9773 - val_loss: 0.0787\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9861 - loss: 0.0394 - val_accuracy: 0.9802 - val_loss: 0.0677\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 17ms/step - accuracy: 0.9865 - loss: 0.0351 - val_accuracy: 0.9724 - val_loss: 0.0921\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9883 - loss: 0.0331 - val_accuracy: 0.9769 - val_loss: 0.0767\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 19ms/step - accuracy: 0.9890 - loss: 0.0317 - val_accuracy: 0.9816 - val_loss: 0.0657\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.9899 - loss: 0.0308 - val_accuracy: 0.9791 - val_loss: 0.0689\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9905 - loss: 0.0287 - val_accuracy: 0.9796 - val_loss: 0.0746\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9908 - loss: 0.0283 - val_accuracy: 0.9804 - val_loss: 0.0702\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 18ms/step - accuracy: 0.9898 - loss: 0.0276 - val_accuracy: 0.9789 - val_loss: 0.0750\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.9918 - loss: 0.0243 - val_accuracy: 0.9722 - val_loss: 0.1015\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.9906 - loss: 0.0278 - val_accuracy: 0.9787 - val_loss: 0.0791\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 23ms/step - accuracy: 0.9929 - loss: 0.0225 - val_accuracy: 0.9744 - val_loss: 0.0951\n\n\nLook at that validation accuracy predictions !! I thought the ones before were good, but these atre even high with almost all the validation accuracy being in the high 90%. This makes it look like using the text to train and creat the model is far more efficent and accurat than using the title\n\nutils.plot_model(model_title, \"model_title.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\nplt.plot(run_2.history['accuracy'], label='Training Accuracy')\nplt.plot(run_2.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model_Texts')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nModel 3: both the article title and article text as inputs\nFor the last model we combine and account for both trhe text and the title when trying to determine if the information is true or false. We follow similar direction as model 1 and 2 yet this time we use concatenate to get both the title_feature and the text_feature. To make it simpliar we are able to simple combine all the text and title features that we created in the models above.\nombine = layers.Dense(32, activation=‘relu’)(combine) introduces a dense (fully connected) layer with 32 units or neurons, with ReLU (Rectified Linear Unit) as the activation function.The ReLU activation function introduces non-linearity to the model, allowing it to learn more complex patterns. After the dense layer a dropout layer is added with a dropout rate of 0.5 (combine = layers.Dropout(0.5)(combine) ),. This means that during training, 50% of the neurons in the preceding layer will be randomly dropped (i.e., their output is set to zero). This prevents the network from relying too much on any single neuron and helps in reducing overfitting by forcing the network to learn more robust features that generalize better to unseen data.\nresult = layers.Dense(1, name = “fake”)(combine) adds another dense layer, this time with just one unit, which is typical for binary classification tasks (e.g., fake or not fake)\nLastly, model_combine = keras.Model(inputs = [title_input, text_input], outputs = output) constructs the model by specifying its inputs and outputs. The inputs parameter is given as a list containing title_input and text_input, indicating that this model expects two separate inputs. model_combine.summary() calling .summary() on the model object which prints a summary of the model’s architecture, including the number of parameters (weights) in each layer and the shape of the output at each stage.\n\ncombine= layers.concatenate([title_features, text_features], axis = 1)\n\n#make sure there will be no overfitting\ncombine = layers.Dense(32, activation='relu')(combine)\ncombine = layers.Dropout(0.5)(combine)\nresult = layers.Dense(1, name = \"fake\")(combine)\n\n\n#creats model\nmodel_combine = keras.Model(\n    inputs = [title_input, text_input],\n    outputs = result\n)\n\nmodel_combine.summary()\n\nmodel_combine.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nrun_3 = model_combine.fit(train_dataset, epochs=20, validation_data=val_dataset)\n\nModel: \"functional_5\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)              ┃ Output Shape           ┃        Param # ┃ Connected to           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)        │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text (InputLayer)         │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization        │ (None, 500)            │              0 │ title[0][0]            │\n│ (TextVectorization)       │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization_1      │ (None, 500)            │              0 │ text[0][0]             │\n│ (TextVectorization)       │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding (Embedding)     │ (None, 500, 10)        │         20,000 │ text_vectorization[0]… │\n│                           │                        │                │ text_vectorization_1[… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout (Dropout)         │ (None, 500, 10)        │              0 │ embedding[0][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_2 (Dropout)       │ (None, 500, 10)        │              0 │ embedding[1][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv1d (Conv1D)           │ (None, 496, 128)       │          6,528 │ dropout[0][0]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv1d_1 (Conv1D)         │ (None, 496, 128)       │          6,528 │ dropout_2[0][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling1d             │ (None, 99, 128)        │              0 │ conv1d[0][0]           │\n│ (MaxPooling1D)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling1d_1           │ (None, 99, 128)        │              0 │ conv1d_1[0][0]         │\n│ (MaxPooling1D)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d  │ (None, 128)            │              0 │ max_pooling1d[0][0]    │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d… │ (None, 128)            │              0 │ max_pooling1d_1[0][0]  │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (Dense)             │ (None, 64)             │          8,256 │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (Dense)           │ (None, 64)             │          8,256 │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_1 (Dropout)       │ (None, 64)             │              0 │ dense[0][0]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_3 (Dropout)       │ (None, 64)             │              0 │ dense_2[0][0]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (Dense)           │ (None, 32)             │          2,080 │ dropout_1[0][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (Dense)           │ (None, 32)             │          2,080 │ dropout_3[0][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate (Concatenate) │ (None, 64)             │              0 │ dense_1[0][0],         │\n│                           │                        │                │ dense_3[0][0]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_4 (Dense)           │ (None, 32)             │          2,080 │ concatenate[0][0]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_4 (Dropout)       │ (None, 32)             │              0 │ dense_4[0][0]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ fake (Dense)              │ (None, 1)              │             33 │ dropout_4[0][0]        │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n\n\n\n Total params: 55,841 (218.13 KB)\n\n\n\n Trainable params: 55,841 (218.13 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 28ms/step - accuracy: 0.8903 - loss: 1.1074 - val_accuracy: 0.9767 - val_loss: 0.2219\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 23ms/step - accuracy: 0.9680 - loss: 0.2682 - val_accuracy: 0.9878 - val_loss: 0.1026\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 24ms/step - accuracy: 0.9905 - loss: 0.0613 - val_accuracy: 0.9851 - val_loss: 0.1527\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9928 - loss: 0.0437 - val_accuracy: 0.9876 - val_loss: 0.1287\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 26ms/step - accuracy: 0.9935 - loss: 0.0426 - val_accuracy: 0.9871 - val_loss: 0.1446\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 22ms/step - accuracy: 0.9915 - loss: 0.0621 - val_accuracy: 0.9882 - val_loss: 0.1179\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9929 - loss: 0.0322 - val_accuracy: 0.9887 - val_loss: 0.1140\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 24ms/step - accuracy: 0.9937 - loss: 0.0363 - val_accuracy: 0.9884 - val_loss: 0.1129\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9948 - loss: 0.0340 - val_accuracy: 0.9882 - val_loss: 0.1179\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 25ms/step - accuracy: 0.9940 - loss: 0.0270 - val_accuracy: 0.9884 - val_loss: 0.1033\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9951 - loss: 0.0277 - val_accuracy: 0.9882 - val_loss: 0.1187\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9930 - loss: 0.0418 - val_accuracy: 0.9871 - val_loss: 0.1131\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 25ms/step - accuracy: 0.9900 - loss: 0.0476 - val_accuracy: 0.9858 - val_loss: 0.1193\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9948 - loss: 0.0208 - val_accuracy: 0.9856 - val_loss: 0.1246\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 25ms/step - accuracy: 0.9949 - loss: 0.0248 - val_accuracy: 0.9869 - val_loss: 0.1224\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9902 - loss: 0.0538 - val_accuracy: 0.9889 - val_loss: 0.0945\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9937 - loss: 0.0363 - val_accuracy: 0.9902 - val_loss: 0.0956\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 25ms/step - accuracy: 0.9928 - loss: 0.0407 - val_accuracy: 0.9884 - val_loss: 0.1040\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9954 - loss: 0.0222 - val_accuracy: 0.9889 - val_loss: 0.0897\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 25ms/step - accuracy: 0.9956 - loss: 0.0250 - val_accuracy: 0.9896 - val_loss: 0.0737\n\n\nThe validation accuracy is alsmot 2% higher than the validation accuracy of the modle that only used text to train. This proves that using both text and title of artcle helps creat the most accurate model. Just to comap\n\nutils.plot_model(model_combine, \"model_combine.png\",\n                 show_shapes=True,\n                 show_layer_names=True,\n                 rankdir='LR')\n\n\n\n\n\nplt.plot(run_3.history['accuracy'], label='Training Accuracy')\nplt.plot(run_3.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model_combine')\nplt.legend()\nplt.show()\n\n\n\n\n\n\nModel Evaluation\nNow that we have tested the three modles we are ready to use the most efficent model, model three, and run it with our testing data. We will do this by importing the test data and follow the same step as we did with teh training datat. We will use read_csv to read and download the data and creat a teast_df database. We would then apply the make_dataset function to the new dataset and textvectornized it to make sure that it can go through our model. Omce that is done we send it through oiur third model as it proved to be the most effictive with the highest sucesses rate.\n\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ntest_df=pd.read_csv(train_url)\ntest_df.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ntest=make_dataset(test_df)\n\n\n#test_loss, test_accuracy = model_combine.evaluate(test_df)\ntest_loss, test_accuracy = model_combine.evaluate(test)\nprint(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n\n225/225 ━━━━━━━━━━━━━━━━━━━━ 5s 21ms/step - accuracy: 0.9971 - loss: 0.0187\nTest Accuracy: 99.72%\n\n\n\ntestdf = make_dataset(test_df)\n\nAs we can see we got a really high accuracy of up to 99%. It seems like if we used this model to detect if a new artcile contained fake or true information we would probably be able to get right up to 99% of the time which means our model is really good.\n\n\nEmbedding Visualization\nNow that we have run the model with our test data we can also make a visualization to get a better undertsnding of our model and the data that we use.\nWe need to perefeom dimensionality reduction on an embeddings matrix using Principal Component Analysis (PCA), which is a technique from linear algebra used for the simplification of data while retaining most of the original variance.\nTo do this we need toi import teh needed packages and then creat an instances of PCA class with n_components 2 to specify that the embedded metrix should be reduced to 2. We then first fits the PCA model to the data in embedding_matrix, then transforms the data into the reduced space defined by the first two principal components. The embedding_matrix is expected to be a 2D numpy array where each row represents an embedding vector (for example, a word vector in the context of NLP tasks).\nThe code below extracts the weights from an embedding layer named ‘embedding’ in the model_combine model, representing the vectorized representations of words. It retrieves the model’s vocabulary from a TextVectorization layer for correlation with these embeddings. Utilizing Principal Component Analysis (PCA) from the sklearn.decomposition package, the code reduces the dimensionality of these embedding vectors to two dimensions, facilitating visualization. This process is aimed at understanding the relationships between words by plotting their vectors in a 2D space. A pandas DataFrame is then created, mapping each word to its respective coordinates in this reduced space, enabling an intuitive visual analysis of how words are grouped or related based on their learned embeddings.\nWe need to do all of this inroder to be able to visualise our model. To visualuzale theses relationship we will use a plotly scatter plot\n\nweights = model_combine.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer\nvocab = text_vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nweights = pca.fit_transform(weights)\n\nembedding_df = pd.DataFrame({\n    'word' : vocab,\n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\n\n\nimport plotly.express as px\nimport numpy as np\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 5,\n                 hover_name = \"word\",\n                 title='Word Embeddings Visualization')\n\nfig.update_layout(\n    xaxis=dict(title='Principal Component 1'),\n    yaxis=dict(title='Principal Component 2'),\n)\n\nfig.show()\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n\nnewplot.png\n\n\n\n\n\nnewplot.png\n\n\n\n\nConclusion:\nThe graph above the distbution of words. It is intresting to see how tweet is very in the middle were as words like yesterday and daylie are in the outskirst. It is also intreatsing to see how close progfressive and income are. And this is it. It was long but overall not to bad you are now ready to go ahead and do this on your own with your dataset, good luck !\nWhen trying to dtermine if an artcoile containes false information or not it is advisoable to use natural language processors and text classification and utalzies both the text and the title of each artcile as we saw model_combien proved to be the most accurate."
  },
  {
    "objectID": "posts/HW2/index.html",
    "href": "posts/HW2/index.html",
    "title": "HW 2, Web Scarpping",
    "section": "",
    "text": "In this blog we will go through on how to scrap a website. In this example we will formulate a list of all the actors who acted in the first Hunger Game movie and create a data fram with all the other movies in which they acted in to help recomand new movies to others.\n\n\nLocate the Url that you would be scrapping information from. For this blog we will use this website https://www.themoviedb.org/ and will scarpe a list of actors from the Hunger Game to create recomandtion system that gives users a list of the other movies the actors played in. \nWe First want to make sure that we are in the right enviroment and create a TMDB_scraper folder with all the files we will need by running the following lines in your terminal.\n\nPIC16B-24W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\n\n\n\n\n#### Part 1 We create a new file inside the spider directory and named it tmdb_spider.py and run the following lines\nTo run the code in the terminal we need to run this line with the correct added subdir for the movie page we chose\n“scrapy crawl tmdb_spider -o movies.csv -a subdir=”\n\n\nNext we will create and implemnt three diffrent parsing methods to get the url for each actor page and then to scrape their page and collect a list of all the movies they have acted in\nOnce we have create the intial function to be able to explore the movie we chose for we need to add the correct subdir .\nWhen going to the hunger game movie page the url is as follow : https://www.themoviedb.org/movie/70160-the-hunger-games When running the code in our terminal we can also just write 70160 for the subdir instead of also inclduing the movie name\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n    '''\n     Initialize the TmdbSpider instance.\n     '''\n    \n\nIn this parse method we create a way to navigate to our cast and crew page by adding /cast to our orginal url. This can be more easily visualize by going to the movie webiste. Once you are there select the full cast and crew page and you will get redirected to a new page. If you look at the url you would notice that it now changed to :https://www.themoviedb.org/movie/70160-the-hunger-games/cast\n\ndef parse(self, response):\n    ''''\n    We start on a movie page, and then  navigate to the Full Cast & Crew page\n    ''''\n            # Navigate to the Full Cast & Crew page\n            full_cast_url = f\"{response.url + '/cast/'}\"\n            # Yield a scrapy.Request for the page of each actor listed on the Full Cast & Crew page\n            yield scrapy.Request(full_cast_url, callback=self.parse_full_credits)\n            \n            \n        \n  \n\nNow that we are at the full cast and crew page the nect step is to go to the actors main page where we can get a list of all the Movies and TV shows they acted in. In order to understand how to find all the information we need we need to inspect the webpage we are at. When we inspect it we are able to use css or xpath to explore and navigate to the right urls. To locate the cast members we will use css to search within the h3 aspect. We will then go through the list of actors by examining ol. To get the actors url we ill examine the href attribute under the div class info\n\n  def parse_full_credits(self, response):\n        '''\n        Following the previous parse we start off at the main page with all the list of the cast and crew \n        \n        We will navigate to get a list of all the actors in The Hunger Games movie \n        \n        This will yiled a request that will get us to each of the actors main page where we \n        could get a list of the movies and TV shows they have been in \n        '''\n        actors_url=response.css('h3:contains(\"Cast\") + ol')\n        #actor_name=actors_url.css(\":: text\")\n        list_a=actors_url.css(\"div.info a::attr(href)\").getall()\n        \n        for url in list_a:\n            full_url = response.urljoin(url)\n            \n            yield scrapy.Request(full_url,callback=self.parse_actor_page) \n           \n\nNow that we are at the actors webapge we will use css to get a list of all the movies they were in. We also used the h2 element to collect all the actors name that were in the title of each of their webpage. Once we collect all the information we created a for loop that yoield a dictionary with the name of the actor and the movies and TV shows that they have been in.\n\ndef parse_actor_page(self, response):\n        \n        '''\n        In this section we assume that we are starting in the actore webapge\n    \n        We will be grabbing and formulating a list of all the movies they were in \n        \n        '''\n        # get the actor name, in this case we located in the title of the page \n        actor_name=response.css('h2.title &gt; a::text').get()\n        # this lines grabs the list of all the movies they have been in \n    \n        card_credit=response.xpath('//h3[text()=\"Acting\"]/following-sibling::table[1]')\n        movie_id=card_credit.css('td.role.true.account_adult_false.item_adult_false &gt; a.tooltip &gt; bdi::text').getall()\n        \n         #This will go through the list of all the movies we have collected and create a dictionary with all the movies and actors \n       \n        for movie in movie_id:\n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie}\n       \n\n\n\n\nOnce we have finished writing all the parsing for the class TmdbSpider we will run the following line with the correct subdir to create a .csv file for all of our actors and moves/ TV they have been in.\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=70160-the-hunger-games"
  },
  {
    "objectID": "posts/HW2/index.html#first-step",
    "href": "posts/HW2/index.html#first-step",
    "title": "HW 2, Web Scarpping",
    "section": "",
    "text": "Locate the Url that you would be scrapping information from. For this blog we will use this website https://www.themoviedb.org/ and will scarpe a list of actors from the Hunger Game to create recomandtion system that gives users a list of the other movies the actors played in. \nWe First want to make sure that we are in the right enviroment and create a TMDB_scraper folder with all the files we will need by running the following lines in your terminal.\n\nPIC16B-24W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper"
  },
  {
    "objectID": "posts/HW2/index.html#second-step",
    "href": "posts/HW2/index.html#second-step",
    "title": "HW 2, Web Scarpping",
    "section": "",
    "text": "#### Part 1 We create a new file inside the spider directory and named it tmdb_spider.py and run the following lines\nTo run the code in the terminal we need to run this line with the correct added subdir for the movie page we chose\n“scrapy crawl tmdb_spider -o movies.csv -a subdir=”\n\n\nNext we will create and implemnt three diffrent parsing methods to get the url for each actor page and then to scrape their page and collect a list of all the movies they have acted in\nOnce we have create the intial function to be able to explore the movie we chose for we need to add the correct subdir .\nWhen going to the hunger game movie page the url is as follow : https://www.themoviedb.org/movie/70160-the-hunger-games When running the code in our terminal we can also just write 70160 for the subdir instead of also inclduing the movie name\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n    '''\n     Initialize the TmdbSpider instance.\n     '''\n    \n\nIn this parse method we create a way to navigate to our cast and crew page by adding /cast to our orginal url. This can be more easily visualize by going to the movie webiste. Once you are there select the full cast and crew page and you will get redirected to a new page. If you look at the url you would notice that it now changed to :https://www.themoviedb.org/movie/70160-the-hunger-games/cast\n\ndef parse(self, response):\n    ''''\n    We start on a movie page, and then  navigate to the Full Cast & Crew page\n    ''''\n            # Navigate to the Full Cast & Crew page\n            full_cast_url = f\"{response.url + '/cast/'}\"\n            # Yield a scrapy.Request for the page of each actor listed on the Full Cast & Crew page\n            yield scrapy.Request(full_cast_url, callback=self.parse_full_credits)\n            \n            \n        \n  \n\nNow that we are at the full cast and crew page the nect step is to go to the actors main page where we can get a list of all the Movies and TV shows they acted in. In order to understand how to find all the information we need we need to inspect the webpage we are at. When we inspect it we are able to use css or xpath to explore and navigate to the right urls. To locate the cast members we will use css to search within the h3 aspect. We will then go through the list of actors by examining ol. To get the actors url we ill examine the href attribute under the div class info\n\n  def parse_full_credits(self, response):\n        '''\n        Following the previous parse we start off at the main page with all the list of the cast and crew \n        \n        We will navigate to get a list of all the actors in The Hunger Games movie \n        \n        This will yiled a request that will get us to each of the actors main page where we \n        could get a list of the movies and TV shows they have been in \n        '''\n        actors_url=response.css('h3:contains(\"Cast\") + ol')\n        #actor_name=actors_url.css(\":: text\")\n        list_a=actors_url.css(\"div.info a::attr(href)\").getall()\n        \n        for url in list_a:\n            full_url = response.urljoin(url)\n            \n            yield scrapy.Request(full_url,callback=self.parse_actor_page) \n           \n\nNow that we are at the actors webapge we will use css to get a list of all the movies they were in. We also used the h2 element to collect all the actors name that were in the title of each of their webpage. Once we collect all the information we created a for loop that yoield a dictionary with the name of the actor and the movies and TV shows that they have been in.\n\ndef parse_actor_page(self, response):\n        \n        '''\n        In this section we assume that we are starting in the actore webapge\n    \n        We will be grabbing and formulating a list of all the movies they were in \n        \n        '''\n        # get the actor name, in this case we located in the title of the page \n        actor_name=response.css('h2.title &gt; a::text').get()\n        # this lines grabs the list of all the movies they have been in \n    \n        card_credit=response.xpath('//h3[text()=\"Acting\"]/following-sibling::table[1]')\n        movie_id=card_credit.css('td.role.true.account_adult_false.item_adult_false &gt; a.tooltip &gt; bdi::text').getall()\n        \n         #This will go through the list of all the movies we have collected and create a dictionary with all the movies and actors \n       \n        for movie in movie_id:\n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie}\n       \n\n\n\n\nOnce we have finished writing all the parsing for the class TmdbSpider we will run the following line with the correct subdir to create a .csv file for all of our actors and moves/ TV they have been in.\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=70160-the-hunger-games"
  },
  {
    "objectID": "posts/HW 5/index.html",
    "href": "posts/HW 5/index.html",
    "title": "HW 5, Image Classifications",
    "section": "",
    "text": "We will start by importing the required packages for this project. Once we import the packages we can then acsseses the sample datat set froim Kaggle with the labeld images of cats and dogs. We need this dataset to be able to train, validate, and test our model toi make sure it is working as we want it to.\n\nimport keras\nimport os\nfrom keras import utils\nimport tensorflow_datasets as tfds\n\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nNow that we run this code we have create a dataset that we will use to feed into our machine learining model. The next step is to go through all of our pictures and resize them to make sure they all fit the same format. We need to do this because the architecture of the model (number of layers, filter sizes, strides, etc.) is designed to process images of a specific dimension. Resizing ensures all input images have the same dimensions expected by the model. The code below resizes the images to a fixed size of 150x150\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nNext we will optimize the data pipeline for machine learning models, ensuring that data is efficiently prepared and supplied to the model during training, validation, and testing phases. This optimization helps in reducing Input/Output overhead, improving memory usage, and potentially speeding up the training process.\nThe code below does this by using batch_size to group multiple elements of the dataset into batches, so instead of going over each picture individualy, the model receive batch_size samples at each training step.The .prefetch(buffer_size) method allows the dataset to preload the next batch of data while the model is training on the current batch. Next the tf_data.AUTOTUNE allows TensorFlow to automatically tune the prefetch buffer size at runtime, finding an optimal value based on the system and environment conditions. Lastly the .cache() method caches the dataset, either in memory or on disk, after the first epoch of training. Overall the code goes over all the data points, trains the model, validate it and tests it.\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\n\n\nNext we will explore our current dataset by creating a function that will create a two row visualization. One row will show 3 random picture of cats, while the second will show 3 random rows of dogs.\nTo visualize three pictures of dogs and three pictures of cats we first start by creating a plotting area that is 10x10. The function then creats two list, one for dogs and one for cats. We then iterate through the database going over only one batch. As it loops through the batch it takes the images labeld 0 and put them in the cats list and 1 for dogs. Once our lists are done we then creat subplot and visualize the images for the cats and dogs.\n\nimport matplotlib.pyplot as plt\n\ndef visualize_images(dataset):\n    plt.figure(figsize=(10, 10))\n    cat_images = []\n    dog_images = []\n\n    for images, labels in dataset.take(1):\n        for i in range(len(labels)):\n            if len(cat_images) &lt; 3 and labels[i] == 0:\n                cat_images.append(images[i].numpy().astype(\"uint8\"))\n            elif len(dog_images) &lt; 3 and labels[i] == 1:\n                dog_images.append(images[i].numpy().astype(\"uint8\"))\n\n    for i, image in enumerate(cat_images):\n        plt.subplot(2, 3, i + 1)\n        plt.imshow(image)\n        plt.title(\"Cat\")\n        plt.axis(\"off\") # so it does not show the number grid\n\n    for i, image in enumerate(dog_images):\n        plt.subplot(2, 3, i + 4)\n        plt.imshow(image)\n        plt.title(\"Dog\")\n        plt.axis(\"off\") # so it does not show the number grid\n\n    plt.show()\n\n# Assuming train_dataset is your dataset variable\nvisualize_images(train_ds)\n\n\n\n\nSuch cute pictures of cats and dogs ! for the next step we will be to create a baseline for the model prefromance\n\n\n\n\nThe line belows creats an iterator (labels_iterator) which will iterate over all the labels in our training data. We will then create a baseline model which in machine learning is a simple model or heuristic used as a reference point for comparing how well a more complex model is performing.\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\nNext we are going to compute the total number of images in our database, seperating it by cats(0) and dogs(1). We will iterate over the labels_iterator once and add one for the dog list and one for cat list everytime there is a 0 or a 1.\n\n# Initialize counters for each label\ncat_count = 0\ndog_count = 0\n\n# Iterate over the labels_iterator to count labels\nfor label in labels_iterator:\n    if label == 0:\n        cat_count += 1\n    elif label == 1:\n        dog_count += 1\n\n\n# Print out the counts\nprint(f\"Number of cat images: {cat_count}\")\nprint(f\"Number of dog images: {dog_count}\")\n\nNumber of cat images: 4637\nNumber of dog images: 4668\n\n\nTo calculate the frequencies that the dog and cat images show up and check the baseline accuracy we add the total number of cats and dog images and divided the amount of cat images by total, and dog images by total images to get the frequencies in which they show up in the database.\n\n# Calculate frequencies\ntotal_img = cat_count + dog_count\ncat_frequency = cat_count / total_img\ndog_frequency = dog_count / total_img\n\n\n\nprint(f\"Frequency of Cats: {cat_frequency:.2f}\")\nprint(f\"Frequency of Dogs: {dog_frequency:.2f}\")\n\n\nFrequency of Cats: 0.50\nFrequency of Dogs: 0.50\n\n\n\n\nmost_frequent_label_count = max(cat_count, dog_count)\nbaseline_accuracy = most_frequent_label_count / total_img\nprint(f\"Baseline model accuracy: {baseline_accuracy:.2f}\")\n\nBaseline model accuracy: 0.50\n\n\n\n\nWhile there are more images of dogs than cats in the database, they both show up in the database at the same frequency of 50%. When we calaculated the baseline model accuracy we only got 50% . This mean the model will be correct simply by guessing the most frequent label every time 0.50 proprotions of the time. When we continue working on our models we should aime to get a high frequency of accuracy than 50%!\nNow that we now our baseline accuracy, we are ready to start working on our first model.\n\n\n\n\nWe will creat a keras.Sequential model with Conv2D layers, MaxPooling2D layers, Flatten layer,Dense layer, and Dropout layer. To do this we will first start by importing layers, models from tensorflow.keras . The model will classify images as either 1 or 0- that is either cat or dog. We start with models.Sequential() which intializes a liner stack of layers so each layer will have exactly one inpute tensor and one output tensor which will be the input tensor to the other layer.\nBefore we move to the rest of the modle lets talk about tensors. Tensors are the fundamental data structures used in deep learning to generalize scalars, vectirs, and matrices to higher dimensions. When using it in the model the tensors are manipulated through operations duirng forward and backward asses of trianing. Essentially, tensors are container of data similar to an array.\nNow that we understnd tensors we can continue with our model. We creat our first Conv2D layer which adds the first 2D convolutional layer with 32 filters (or kernels), each of size 3x3. The activation=‘relu’ parameter applies the Rectified Linear Unit (ReLU) activation function to the output of each convolution operation. the input_shape refres to the shape of our images, which we made 150x150 earlier when resizing the images. The first MaxPooling 2D layer reduces the spatial dimensions (height and width) of the input feature maps by half, effectively downsampling the input and reducing the number of parameters the model needs to learn, which helps in controlling overfitting.\nNext we move to the second Conv2D layer which increases the model to allow it to learn more complex features from the input images.The second MaxPooling2D layer is also used to further reduce the spatial dimensions of the feature maps. We then us layers.Flatten() to transforms the 3D output of the preceding layer into a 1D array, which is necessary because the following dense layer expects a 1D input.\nLastly, our two dense layers and dropout layer are used to takes the flattened input and learns non-linear combinations of features. Then to randomly sets half of the input units to 0 at each update during training time, which helps prevent overfitting by ensuring that the model does not become too reliant on any one node.And lastly, the layers.Dense(1, activation=‘sigmoid’), outputs a single value between 0 and 1, representing the probability that the input image belongs to the target class (1).\n\nfrom tensorflow.keras import layers, models\n\nmodel1 = models.Sequential([\n    # First Conv2D layer\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    # First MaxPooling2D layer\n    layers.MaxPooling2D((2, 2)),\n\n    # Second Conv2D layer\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    # Second MaxPooling2D layer\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    # Flatten layer to convert 3D feature maps to 1D feature vectors\n    layers.Flatten(),\n\n    # Dense layer\n    layers.Dense(128, activation='relu'),\n\n    # Dropout layer to reduce overfitting\n    layers.Dropout(0.4),\n\n    # Output layer\n    layers.Dense(1, activation='sigmoid')\n])\n\n\n\nOnce we created the model we need to compile it to configure the model for training in the TensorFlow/Keras framework. This oart is crucial for setting up the model for training, defining how it should learn (optimizer), what it should minimize (loss function), and how its performance should be measured (metrics).\n\n model1.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\n\n\n\n\nhistory = model1.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 410s 3s/step - loss: 6.0001 - accuracy: 0.5716 - val_loss: 0.6708 - val_accuracy: 0.5907\nEpoch 2/20\n146/146 [==============================] - 405s 3s/step - loss: 0.6556 - accuracy: 0.6126 - val_loss: 0.6578 - val_accuracy: 0.6066\nEpoch 3/20\n146/146 [==============================] - 406s 3s/step - loss: 0.6132 - accuracy: 0.6505 - val_loss: 0.6754 - val_accuracy: 0.6199\nEpoch 4/20\n146/146 [==============================] - 397s 3s/step - loss: 0.5779 - accuracy: 0.6935 - val_loss: 0.6973 - val_accuracy: 0.6118\nEpoch 5/20\n146/146 [==============================] - 403s 3s/step - loss: 0.5260 - accuracy: 0.7255 - val_loss: 0.7253 - val_accuracy: 0.6036\nEpoch 6/20\n146/146 [==============================] - 386s 3s/step - loss: 0.4838 - accuracy: 0.7516 - val_loss: 0.7233 - val_accuracy: 0.6268\nEpoch 7/20\n146/146 [==============================] - 406s 3s/step - loss: 0.4277 - accuracy: 0.7947 - val_loss: 0.7512 - val_accuracy: 0.6135\nEpoch 8/20\n146/146 [==============================] - 424s 3s/step - loss: 0.3935 - accuracy: 0.8159 - val_loss: 0.8102 - val_accuracy: 0.5911\nEpoch 9/20\n146/146 [==============================] - 406s 3s/step - loss: 0.3507 - accuracy: 0.8417 - val_loss: 0.8663 - val_accuracy: 0.6281\nEpoch 10/20\n146/146 [==============================] - 384s 3s/step - loss: 0.2897 - accuracy: 0.8733 - val_loss: 1.1332 - val_accuracy: 0.6328\nEpoch 11/20\n146/146 [==============================] - 407s 3s/step - loss: 0.2878 - accuracy: 0.8742 - val_loss: 0.9575 - val_accuracy: 0.6320\nEpoch 12/20\n146/146 [==============================] - 425s 3s/step - loss: 0.2689 - accuracy: 0.8900 - val_loss: 1.0345 - val_accuracy: 0.6238\nEpoch 13/20\n146/146 [==============================] - 405s 3s/step - loss: 0.2221 - accuracy: 0.9095 - val_loss: 1.2777 - val_accuracy: 0.6247\nEpoch 14/20\n146/146 [==============================] - 412s 3s/step - loss: 0.2065 - accuracy: 0.9191 - val_loss: 1.1663 - val_accuracy: 0.6255\nEpoch 15/20\n146/146 [==============================] - 408s 3s/step - loss: 0.1906 - accuracy: 0.9250 - val_loss: 1.2094 - val_accuracy: 0.6178\nEpoch 16/20\n146/146 [==============================] - 406s 3s/step - loss: 0.1646 - accuracy: 0.9356 - val_loss: 1.3935 - val_accuracy: 0.6346\nEpoch 17/20\n146/146 [==============================] - 418s 3s/step - loss: 0.1592 - accuracy: 0.9452 - val_loss: 1.4878 - val_accuracy: 0.6101\nEpoch 18/20\n146/146 [==============================] - 405s 3s/step - loss: 0.1603 - accuracy: 0.9400 - val_loss: 1.3832 - val_accuracy: 0.6101\nEpoch 19/20\n146/146 [==============================] - 393s 3s/step - loss: 0.1471 - accuracy: 0.9476 - val_loss: 1.4424 - val_accuracy: 0.6174\nEpoch 20/20\n146/146 [==============================] - 409s 3s/step - loss: 0.1405 - accuracy: 0.9484 - val_loss: 1.6150 - val_accuracy: 0.6294\n\n\nBefore analyzing the results we are also going to plot the training and validation accuracy\n\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n\n\n\nAfter running the code and training the model the accuracy of the code stablized at around 60%\nIn compare to the baseline, this model is 10% more accurate in predicting the correct images. There does seem to be a lot of overfitting as the training accuracy seems to go up to 90% while the validation accuracy stays at around 60%.\n\n\n\nFor this model we will be adding data augmentation layers to your model. We are going to include some modified copies of the same imagies into our training set to imporve the model ability of identifying images\nWe will first start by creating a keras.layers.RandomFlip() layer. In the code below we select a single image and apply the augmentation on it to test to see how it will show. The random_flip_layer creats a new layer that we will than apply to the image and then plot it.\n\n\nimport matplotlib.pyplot as plt\n\n\n# take a single image from the dataset to apply augmentation\nfor images, _ in train_ds.take(1):\n    # Take the first image in the batch for demonstration\n    original_image = images[0]\n    break\n\n# Create the RandomFlip layer\nrandom_flip_layer = keras.Sequential([\n    keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n])\n\n# Visualize original and flipped images\nplt.figure(figsize=(10, 10))\nfor images, _ in train_ds.take(1):\n    for i in range(9):\n        augmented_ig = random_flip_layer(images)\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(augmented_ig[0].numpy().astype(\"uint8\"))\n        plt.axis(\"off\")\nplt.show()\n\n\n\n\nWow look how cool that is! Now lets do the same thing but instead of flip we will rotate the image. To increase the rotation range we can make the factor be .45 if the factor was smaller, so would the rotataion range be.\n\nrotation_ig = keras.Sequential([\n    keras.layers.RandomRotation(factor=0.45),\n])\n\n# Visualize original and rotated images\nplt.figure(figsize=(10, 10))\nfor images, _ in train_ds.take(1):\n    for i in range(9):\n        augmented_ig2 = rotation_ig(images)\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(augmented_ig2[0].numpy().astype(\"uint8\"))\n        plt.axis(\"off\")\nplt.show()\n\n\n\n\nOnce we were able to visualize the two forms of agumantation we will incoporate them and create model2. Similar to model 1, we will train the datat and keep track of its accuracy. To make sure that the model is more accurate now that we added the agumentation factor we will change teh rotation range back down to .2. We also added layers.BatchNormalization() which will improve training stability and speed by normalizing the input to each activation layer and changed the dropout layer to .5 instead of .4 to help with the overfitting\n\nmodel2 = models.Sequential([\n    # Data Augmentation layers\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.2),\n\n\n    # First Conv2D layer\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    # First MaxPooling2D layer\n    layers.MaxPooling2D((2, 2)),\n\n    # Second Conv2D layer\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    # Second MaxPooling2D layer\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    # Flatten layer to convert 3D feature maps to 1D feature vectors\n    layers.Flatten(),\n\n    # Dense layer\n    layers.Dense(128, activation='relu'),\n\n    # Dropout layer to reduce overfitting\n    layers.Dropout(0.5),\n\n    # Output layer\n    layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel2.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\n# Train the model with data augmentation\nhistory = model2.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 10s 46ms/step - loss: 1.0536 - accuracy: 0.5786 - val_loss: 0.6513 - val_accuracy: 0.6445\nEpoch 2/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.6405 - accuracy: 0.6347 - val_loss: 0.6429 - val_accuracy: 0.6367\nEpoch 3/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.6184 - accuracy: 0.6596 - val_loss: 0.6523 - val_accuracy: 0.5821\nEpoch 4/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.6119 - accuracy: 0.6651 - val_loss: 0.5984 - val_accuracy: 0.6604\nEpoch 5/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.6045 - accuracy: 0.6768 - val_loss: 0.5956 - val_accuracy: 0.6844\nEpoch 6/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.5869 - accuracy: 0.6844 - val_loss: 0.5415 - val_accuracy: 0.7240\nEpoch 7/20\n146/146 [==============================] - 7s 47ms/step - loss: 0.5756 - accuracy: 0.7008 - val_loss: 0.5436 - val_accuracy: 0.7257\nEpoch 8/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.5662 - accuracy: 0.7114 - val_loss: 1.2838 - val_accuracy: 0.5052\nEpoch 9/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.5612 - accuracy: 0.7176 - val_loss: 0.7067 - val_accuracy: 0.6113\nEpoch 10/20\n146/146 [==============================] - 8s 53ms/step - loss: 0.5496 - accuracy: 0.7229 - val_loss: 0.5536 - val_accuracy: 0.6883\nEpoch 11/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.5404 - accuracy: 0.7385 - val_loss: 0.5986 - val_accuracy: 0.6604\nEpoch 12/20\n146/146 [==============================] - 7s 51ms/step - loss: 0.5304 - accuracy: 0.7468 - val_loss: 0.6166 - val_accuracy: 0.6896\nEpoch 13/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.5197 - accuracy: 0.7516 - val_loss: 0.4979 - val_accuracy: 0.7511\nEpoch 14/20\n146/146 [==============================] - 7s 47ms/step - loss: 0.5161 - accuracy: 0.7574 - val_loss: 0.5920 - val_accuracy: 0.6733\nEpoch 15/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.5079 - accuracy: 0.7588 - val_loss: 0.8534 - val_accuracy: 0.6277\nEpoch 16/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.4959 - accuracy: 0.7681 - val_loss: 0.4682 - val_accuracy: 0.7721\nEpoch 17/20\n146/146 [==============================] - 7s 50ms/step - loss: 0.4855 - accuracy: 0.7709 - val_loss: 0.4667 - val_accuracy: 0.7855\nEpoch 18/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.4950 - accuracy: 0.7673 - val_loss: 0.4966 - val_accuracy: 0.7709\nEpoch 19/20\n146/146 [==============================] - 7s 47ms/step - loss: 0.4841 - accuracy: 0.7772 - val_loss: 0.5106 - val_accuracy: 0.7459\nEpoch 20/20\n146/146 [==============================] - 7s 47ms/step - loss: 0.4868 - accuracy: 0.7724 - val_loss: 0.5122 - val_accuracy: 0.7571\n\n\nLet us again visualize the training accuracy and validation accuracy\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.show()\n\n\n\n\nWith the data agumentation, the validation accuracy of the model was between 60% and 70% There were a few drops in which the accuracy went down to 50 or low 60 but then it went back up to the 70s. In comparsion to model1 the validation accuracy has increased although the training accuracy seem to have been lower than it was in model 1. However, model2 does seem to be way better at acounting for and fixing overfitting.\nThe diffrenece between the models can be seen even better when looking at the graph. the graph for model2 shows how drastically the validation accuracy changed through the training while in model 1 the validation accuracy seemed to have stayed consistently low.\n\n\n\nTo make the model train fatser we can also change the RGB values. Changing the RGB value can also help save our time and let us shift our attention to dealing with actual signal in the data. RBG is the color model used in digital imaging and displays. It stands for Red Green and Blue and is based on the idea that all colors are based on teh combination of those three colors. Often time each color is typically represented by a value ranging from 0 to 255. To make the model train faster we wnat the RGB to be normalized between 0 and 1 which is what we are goining to try and do.\nFor this section we will create model3 with a preprocessing layer. The preprocessing layer coded below will normalize image pixel values\nLet’s break down how to creat the preprocessor layer. First we will start by defining the input tensor for the preprocessing model. Next step is to creat the rescaling layer. The scale_layer will transform the input pixel values. Llastly, the last line bundles the input tensor and the output of the rescaling layer into a standalone Keras model named preprocessor. Essentially what this will do is take an input image/s and produces the normalized version of the image/s as output.\n\n\n# Define the input shape\ni = keras.Input(shape=(150, 150, 3))\n\n# Create the Rescaling layer to normalize pixel values\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n\n# Apply the scaling layer to the input\nx = scale_layer(i)\n\n# Create a preprocessing model\npreprocessor = keras.Model(inputs=[i], outputs=[x])\n\nNext up we will need to creat model 3. Similar to model 1 and 2 we will include all the same factors as well as the agumentation section in addition to the preprocessor. Because we made more changes to teh datat we will also need to make further adjusment to the model to make sure it is more accurate. We increased the dense layer to 256 and chnaged the drop layer to .2\n\nmodel3 = keras.Sequential([\n    preprocessor,  # Preprocessing layer for input normalization\n\n    # Data augmentation layers\n    keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n    keras.layers.RandomRotation(0.2),\n\n   # First Conv2D layer\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    # First MaxPooling2D layer\n    layers.MaxPooling2D((2, 2)),\n\n    # Second Conv2D layer\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    # Second MaxPooling2D layer\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    # Flatten layer to convert 3D feature maps to 1D feature vectors\n    layers.Flatten(),\n\n    # Dense layer\n    layers.Dense(256, activation='relu'),\n\n    # Dropout layer to reduce overfitting\n    layers.Dropout(0.2),\n\n    # Output layer\n    layers.Dense(1, activation='sigmoid')\n])\n\n\n\n# Compile the model\nmodel3.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\n\nhistory = model3.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 9s 48ms/step - loss: 1.3734 - accuracy: 0.5857 - val_loss: 0.6798 - val_accuracy: 0.6470\nEpoch 2/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.6146 - accuracy: 0.6570 - val_loss: 0.6377 - val_accuracy: 0.6930\nEpoch 3/20\n146/146 [==============================] - 7s 47ms/step - loss: 0.5927 - accuracy: 0.6820 - val_loss: 0.5921 - val_accuracy: 0.6862\nEpoch 4/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.5750 - accuracy: 0.6970 - val_loss: 0.5740 - val_accuracy: 0.6973\nEpoch 5/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.5563 - accuracy: 0.7150 - val_loss: 0.5271 - val_accuracy: 0.7356\nEpoch 6/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.5441 - accuracy: 0.7191 - val_loss: 0.5199 - val_accuracy: 0.7390\nEpoch 7/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.5304 - accuracy: 0.7415 - val_loss: 0.5779 - val_accuracy: 0.7223\nEpoch 8/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.5178 - accuracy: 0.7436 - val_loss: 0.5007 - val_accuracy: 0.7687\nEpoch 9/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.5067 - accuracy: 0.7531 - val_loss: 0.4866 - val_accuracy: 0.7687\nEpoch 10/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.4978 - accuracy: 0.7558 - val_loss: 0.5418 - val_accuracy: 0.7451\nEpoch 11/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.4887 - accuracy: 0.7677 - val_loss: 0.4750 - val_accuracy: 0.7618\nEpoch 12/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.4901 - accuracy: 0.7644 - val_loss: 0.4799 - val_accuracy: 0.7713\nEpoch 13/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.4733 - accuracy: 0.7751 - val_loss: 0.5148 - val_accuracy: 0.7455\nEpoch 14/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.4636 - accuracy: 0.7819 - val_loss: 0.4434 - val_accuracy: 0.7902\nEpoch 15/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.4472 - accuracy: 0.7893 - val_loss: 0.5685 - val_accuracy: 0.7528\nEpoch 16/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.4451 - accuracy: 0.7940 - val_loss: 0.4437 - val_accuracy: 0.8040\nEpoch 17/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.4374 - accuracy: 0.7956 - val_loss: 0.4520 - val_accuracy: 0.7954\nEpoch 18/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.4354 - accuracy: 0.7980 - val_loss: 0.4444 - val_accuracy: 0.7928\nEpoch 19/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.4190 - accuracy: 0.8096 - val_loss: 0.4100 - val_accuracy: 0.8052\nEpoch 20/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.4217 - accuracy: 0.8058 - val_loss: 0.4142 - val_accuracy: 0.8108\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model3 Training and Validation Accuracy')\nplt.legend()\nplt.show()\n\n\n\n\nWe were able to get the model Validation accuracy up to 81% this is again big imporvemnt from model 1 and 2. Additionally , were were able to minimize abnd bring down the overfiting by chnaging the drop layer to .2.\n\n\n\n\nWhile we were able to create a model that can distingiuse between pictures of dogs and cats, there are people who have already created similar models. In this next section we are going to try to use a pre existing model to do our task.\nWe will first need to start by getting a preexisting model and use it as our base model before incoportating it to our model and train our data on it.\nWe will first start by importing the MobileNetV3Large model and making it into a layer that we will be able to add to our model.\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n12683000/12683000 [==============================] - 0s 0us/step\n\n\nNow that we have the pre made model we will make model4 which will include this new added layer.\nWhile model 4 also included the data agumentation and a layer for classification, it is way more accurat thanks to the base_model_layer.\nLets break down the code for model 4. To make the model look cleaner we created the data_agumentation variable outside of the model construction. We then construct the model by incoportating the base_model_layer and the augmentation layer. We the use GlobalMaxPooling2D to reduces the spatial dimensions of the feature maps to a single maximum value per feature map. We set the drpout at .20 to help minimize overfitting.\nOnce the model is done with compile it and then train it\n\ndata_augmentation = keras.Sequential([\n    keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n    keras.layers.RandomRotation(0.2),\n])\n\n# Model construction\nmodel4 = keras.Sequential([\n    data_augmentation,\n    base_model_layer,\n    layers.GlobalMaxPooling2D(),\n    layers.Dropout(0.2),  # Dropout layer to reduce overfitting\n    layers.Dense(2, activation='softmax')  # Classification layer\n])\n\n# Compile the model\nmodel4.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\n\n\nhistory = model4.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 16s 63ms/step - loss: 1.7371 - accuracy: 0.8186 - val_loss: 0.3864 - val_accuracy: 0.9527\nEpoch 2/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.9484 - accuracy: 0.8830 - val_loss: 0.3616 - val_accuracy: 0.9549\nEpoch 3/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6807 - accuracy: 0.9038 - val_loss: 0.1966 - val_accuracy: 0.9626\nEpoch 4/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.5961 - accuracy: 0.9031 - val_loss: 0.1960 - val_accuracy: 0.9609\nEpoch 5/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.5247 - accuracy: 0.9025 - val_loss: 0.1887 - val_accuracy: 0.9609\nEpoch 6/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4708 - accuracy: 0.9059 - val_loss: 0.1530 - val_accuracy: 0.9622\nEpoch 7/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4174 - accuracy: 0.9087 - val_loss: 0.1354 - val_accuracy: 0.9647\nEpoch 8/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3734 - accuracy: 0.9085 - val_loss: 0.1250 - val_accuracy: 0.9613\nEpoch 9/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3980 - accuracy: 0.8998 - val_loss: 0.3346 - val_accuracy: 0.9248\nEpoch 10/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3886 - accuracy: 0.9020 - val_loss: 0.1424 - val_accuracy: 0.9574\nEpoch 11/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3555 - accuracy: 0.9055 - val_loss: 0.1492 - val_accuracy: 0.9566\nEpoch 12/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3352 - accuracy: 0.9103 - val_loss: 0.1395 - val_accuracy: 0.9617\nEpoch 13/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3212 - accuracy: 0.9045 - val_loss: 0.1145 - val_accuracy: 0.9635\nEpoch 14/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.3161 - accuracy: 0.9090 - val_loss: 0.1256 - val_accuracy: 0.9626\nEpoch 15/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3287 - accuracy: 0.9037 - val_loss: 0.1235 - val_accuracy: 0.9583\nEpoch 16/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3050 - accuracy: 0.9083 - val_loss: 0.2015 - val_accuracy: 0.9398\nEpoch 17/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3136 - accuracy: 0.9100 - val_loss: 0.1114 - val_accuracy: 0.9682\nEpoch 18/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3184 - accuracy: 0.9018 - val_loss: 0.1304 - val_accuracy: 0.9592\nEpoch 19/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.2954 - accuracy: 0.9103 - val_loss: 0.1378 - val_accuracy: 0.9553\nEpoch 20/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.2985 - accuracy: 0.9090 - val_loss: 0.1122 - val_accuracy: 0.9622\n\n\n\nmodel4.summary()\n\nModel: \"sequential_26\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n sequential_25 (Sequential)  (None, 150, 150, 3)       0         \n                                                                 \n model_1 (Functional)        (None, 5, 5, 960)         2996352   \n                                                                 \n global_max_pooling2d_1 (Gl  (None, 960)               0         \n obalMaxPooling2D)                                               \n                                                                 \n dropout_22 (Dropout)        (None, 960)               0         \n                                                                 \n dense_43 (Dense)            (None, 2)                 1922      \n                                                                 \n=================================================================\nTotal params: 2998274 (11.44 MB)\nTrainable params: 1922 (7.51 KB)\nNon-trainable params: 2996352 (11.43 MB)\n_________________________________________________________________\n\n\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs_range = range(20)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\n\n\nText(0.5, 1.0, 'Training and Validation Accuracy')\n\n\n\n\n\nWe started with around 50%-60% accuracy in our first model, and now with the help of the base model we were able to bring up the accuracy all the way up to around 95%. As seen in the summary we only have 2998274 parameter to train the model on. There also seem to be very little over fitting of around 2%-3%.\n\n\n\nAfter making all theses models it is time for us to run the accuracy test on our test dataset!!\nSince model4 had the highest accuracy rate I chose to run the test dataset on it.\n\ntest_loss, test_acc = model4.evaluate(test_ds)\nprint(f\"Test accuracy: {test_acc*100:.2f}%\")\n\n37/37 [==============================] - 4s 101ms/step - loss: 0.1498 - accuracy: 0.9553\nTest accuracy: 95.53%\n\n\nLook at that !! We were able to get an accuracy precent of 95.53% !\nThere are a lot of diffrent moving parts and many diffrent things that can be chnaged and corrected. But as we seen in this blog post, simply trying out diffrent things and chnaging the setting can help imporve the accuracy rate of the model greatly!"
  },
  {
    "objectID": "posts/HW 5/index.html#first-model",
    "href": "posts/HW 5/index.html#first-model",
    "title": "HW 5, Image Classifications",
    "section": "",
    "text": "We will creat a keras.Sequential model with Conv2D layers, MaxPooling2D layers, Flatten layer,Dense layer, and Dropout layer. To do this we will first start by importing layers, models from tensorflow.keras . The model will classify images as either 1 or 0- that is either cat or dog. We start with models.Sequential() which intializes a liner stack of layers so each layer will have exactly one inpute tensor and one output tensor which will be the input tensor to the other layer.\nBefore we move to the rest of the modle lets talk about tensors. Tensors are the fundamental data structures used in deep learning to generalize scalars, vectirs, and matrices to higher dimensions. When using it in the model the tensors are manipulated through operations duirng forward and backward asses of trianing. Essentially, tensors are container of data similar to an array.\nNow that we understnd tensors we can continue with our model. We creat our first Conv2D layer which adds the first 2D convolutional layer with 32 filters (or kernels), each of size 3x3. The activation=‘relu’ parameter applies the Rectified Linear Unit (ReLU) activation function to the output of each convolution operation. the input_shape refres to the shape of our images, which we made 150x150 earlier when resizing the images. The first MaxPooling 2D layer reduces the spatial dimensions (height and width) of the input feature maps by half, effectively downsampling the input and reducing the number of parameters the model needs to learn, which helps in controlling overfitting.\nNext we move to the second Conv2D layer which increases the model to allow it to learn more complex features from the input images.The second MaxPooling2D layer is also used to further reduce the spatial dimensions of the feature maps. We then us layers.Flatten() to transforms the 3D output of the preceding layer into a 1D array, which is necessary because the following dense layer expects a 1D input.\nLastly, our two dense layers and dropout layer are used to takes the flattened input and learns non-linear combinations of features. Then to randomly sets half of the input units to 0 at each update during training time, which helps prevent overfitting by ensuring that the model does not become too reliant on any one node.And lastly, the layers.Dense(1, activation=‘sigmoid’), outputs a single value between 0 and 1, representing the probability that the input image belongs to the target class (1).\n\nfrom tensorflow.keras import layers, models\n\nmodel1 = models.Sequential([\n    # First Conv2D layer\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    # First MaxPooling2D layer\n    layers.MaxPooling2D((2, 2)),\n\n    # Second Conv2D layer\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    # Second MaxPooling2D layer\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    # Flatten layer to convert 3D feature maps to 1D feature vectors\n    layers.Flatten(),\n\n    # Dense layer\n    layers.Dense(128, activation='relu'),\n\n    # Dropout layer to reduce overfitting\n    layers.Dropout(0.4),\n\n    # Output layer\n    layers.Dense(1, activation='sigmoid')\n])\n\n\n\nOnce we created the model we need to compile it to configure the model for training in the TensorFlow/Keras framework. This oart is crucial for setting up the model for training, defining how it should learn (optimizer), what it should minimize (loss function), and how its performance should be measured (metrics).\n\n model1.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\n\n\n\n\nhistory = model1.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 410s 3s/step - loss: 6.0001 - accuracy: 0.5716 - val_loss: 0.6708 - val_accuracy: 0.5907\nEpoch 2/20\n146/146 [==============================] - 405s 3s/step - loss: 0.6556 - accuracy: 0.6126 - val_loss: 0.6578 - val_accuracy: 0.6066\nEpoch 3/20\n146/146 [==============================] - 406s 3s/step - loss: 0.6132 - accuracy: 0.6505 - val_loss: 0.6754 - val_accuracy: 0.6199\nEpoch 4/20\n146/146 [==============================] - 397s 3s/step - loss: 0.5779 - accuracy: 0.6935 - val_loss: 0.6973 - val_accuracy: 0.6118\nEpoch 5/20\n146/146 [==============================] - 403s 3s/step - loss: 0.5260 - accuracy: 0.7255 - val_loss: 0.7253 - val_accuracy: 0.6036\nEpoch 6/20\n146/146 [==============================] - 386s 3s/step - loss: 0.4838 - accuracy: 0.7516 - val_loss: 0.7233 - val_accuracy: 0.6268\nEpoch 7/20\n146/146 [==============================] - 406s 3s/step - loss: 0.4277 - accuracy: 0.7947 - val_loss: 0.7512 - val_accuracy: 0.6135\nEpoch 8/20\n146/146 [==============================] - 424s 3s/step - loss: 0.3935 - accuracy: 0.8159 - val_loss: 0.8102 - val_accuracy: 0.5911\nEpoch 9/20\n146/146 [==============================] - 406s 3s/step - loss: 0.3507 - accuracy: 0.8417 - val_loss: 0.8663 - val_accuracy: 0.6281\nEpoch 10/20\n146/146 [==============================] - 384s 3s/step - loss: 0.2897 - accuracy: 0.8733 - val_loss: 1.1332 - val_accuracy: 0.6328\nEpoch 11/20\n146/146 [==============================] - 407s 3s/step - loss: 0.2878 - accuracy: 0.8742 - val_loss: 0.9575 - val_accuracy: 0.6320\nEpoch 12/20\n146/146 [==============================] - 425s 3s/step - loss: 0.2689 - accuracy: 0.8900 - val_loss: 1.0345 - val_accuracy: 0.6238\nEpoch 13/20\n146/146 [==============================] - 405s 3s/step - loss: 0.2221 - accuracy: 0.9095 - val_loss: 1.2777 - val_accuracy: 0.6247\nEpoch 14/20\n146/146 [==============================] - 412s 3s/step - loss: 0.2065 - accuracy: 0.9191 - val_loss: 1.1663 - val_accuracy: 0.6255\nEpoch 15/20\n146/146 [==============================] - 408s 3s/step - loss: 0.1906 - accuracy: 0.9250 - val_loss: 1.2094 - val_accuracy: 0.6178\nEpoch 16/20\n146/146 [==============================] - 406s 3s/step - loss: 0.1646 - accuracy: 0.9356 - val_loss: 1.3935 - val_accuracy: 0.6346\nEpoch 17/20\n146/146 [==============================] - 418s 3s/step - loss: 0.1592 - accuracy: 0.9452 - val_loss: 1.4878 - val_accuracy: 0.6101\nEpoch 18/20\n146/146 [==============================] - 405s 3s/step - loss: 0.1603 - accuracy: 0.9400 - val_loss: 1.3832 - val_accuracy: 0.6101\nEpoch 19/20\n146/146 [==============================] - 393s 3s/step - loss: 0.1471 - accuracy: 0.9476 - val_loss: 1.4424 - val_accuracy: 0.6174\nEpoch 20/20\n146/146 [==============================] - 409s 3s/step - loss: 0.1405 - accuracy: 0.9484 - val_loss: 1.6150 - val_accuracy: 0.6294\n\n\nBefore analyzing the results we are also going to plot the training and validation accuracy\n\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n\n\n\nAfter running the code and training the model the accuracy of the code stablized at around 60%\nIn compare to the baseline, this model is 10% more accurate in predicting the correct images. There does seem to be a lot of overfitting as the training accuracy seems to go up to 90% while the validation accuracy stays at around 60%.\n\n\n\nFor this model we will be adding data augmentation layers to your model. We are going to include some modified copies of the same imagies into our training set to imporve the model ability of identifying images\nWe will first start by creating a keras.layers.RandomFlip() layer. In the code below we select a single image and apply the augmentation on it to test to see how it will show. The random_flip_layer creats a new layer that we will than apply to the image and then plot it.\n\n\nimport matplotlib.pyplot as plt\n\n\n# take a single image from the dataset to apply augmentation\nfor images, _ in train_ds.take(1):\n    # Take the first image in the batch for demonstration\n    original_image = images[0]\n    break\n\n# Create the RandomFlip layer\nrandom_flip_layer = keras.Sequential([\n    keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n])\n\n# Visualize original and flipped images\nplt.figure(figsize=(10, 10))\nfor images, _ in train_ds.take(1):\n    for i in range(9):\n        augmented_ig = random_flip_layer(images)\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(augmented_ig[0].numpy().astype(\"uint8\"))\n        plt.axis(\"off\")\nplt.show()\n\n\n\n\nWow look how cool that is! Now lets do the same thing but instead of flip we will rotate the image. To increase the rotation range we can make the factor be .45 if the factor was smaller, so would the rotataion range be.\n\nrotation_ig = keras.Sequential([\n    keras.layers.RandomRotation(factor=0.45),\n])\n\n# Visualize original and rotated images\nplt.figure(figsize=(10, 10))\nfor images, _ in train_ds.take(1):\n    for i in range(9):\n        augmented_ig2 = rotation_ig(images)\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(augmented_ig2[0].numpy().astype(\"uint8\"))\n        plt.axis(\"off\")\nplt.show()\n\n\n\n\nOnce we were able to visualize the two forms of agumantation we will incoporate them and create model2. Similar to model 1, we will train the datat and keep track of its accuracy. To make sure that the model is more accurate now that we added the agumentation factor we will change teh rotation range back down to .2. We also added layers.BatchNormalization() which will improve training stability and speed by normalizing the input to each activation layer and changed the dropout layer to .5 instead of .4 to help with the overfitting\n\nmodel2 = models.Sequential([\n    # Data Augmentation layers\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.2),\n\n\n    # First Conv2D layer\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    # First MaxPooling2D layer\n    layers.MaxPooling2D((2, 2)),\n\n    # Second Conv2D layer\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    # Second MaxPooling2D layer\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    # Flatten layer to convert 3D feature maps to 1D feature vectors\n    layers.Flatten(),\n\n    # Dense layer\n    layers.Dense(128, activation='relu'),\n\n    # Dropout layer to reduce overfitting\n    layers.Dropout(0.5),\n\n    # Output layer\n    layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel2.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\n# Train the model with data augmentation\nhistory = model2.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 10s 46ms/step - loss: 1.0536 - accuracy: 0.5786 - val_loss: 0.6513 - val_accuracy: 0.6445\nEpoch 2/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.6405 - accuracy: 0.6347 - val_loss: 0.6429 - val_accuracy: 0.6367\nEpoch 3/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.6184 - accuracy: 0.6596 - val_loss: 0.6523 - val_accuracy: 0.5821\nEpoch 4/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.6119 - accuracy: 0.6651 - val_loss: 0.5984 - val_accuracy: 0.6604\nEpoch 5/20\n146/146 [==============================] - 7s 45ms/step - loss: 0.6045 - accuracy: 0.6768 - val_loss: 0.5956 - val_accuracy: 0.6844\nEpoch 6/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.5869 - accuracy: 0.6844 - val_loss: 0.5415 - val_accuracy: 0.7240\nEpoch 7/20\n146/146 [==============================] - 7s 47ms/step - loss: 0.5756 - accuracy: 0.7008 - val_loss: 0.5436 - val_accuracy: 0.7257\nEpoch 8/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.5662 - accuracy: 0.7114 - val_loss: 1.2838 - val_accuracy: 0.5052\nEpoch 9/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.5612 - accuracy: 0.7176 - val_loss: 0.7067 - val_accuracy: 0.6113\nEpoch 10/20\n146/146 [==============================] - 8s 53ms/step - loss: 0.5496 - accuracy: 0.7229 - val_loss: 0.5536 - val_accuracy: 0.6883\nEpoch 11/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.5404 - accuracy: 0.7385 - val_loss: 0.5986 - val_accuracy: 0.6604\nEpoch 12/20\n146/146 [==============================] - 7s 51ms/step - loss: 0.5304 - accuracy: 0.7468 - val_loss: 0.6166 - val_accuracy: 0.6896\nEpoch 13/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.5197 - accuracy: 0.7516 - val_loss: 0.4979 - val_accuracy: 0.7511\nEpoch 14/20\n146/146 [==============================] - 7s 47ms/step - loss: 0.5161 - accuracy: 0.7574 - val_loss: 0.5920 - val_accuracy: 0.6733\nEpoch 15/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.5079 - accuracy: 0.7588 - val_loss: 0.8534 - val_accuracy: 0.6277\nEpoch 16/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.4959 - accuracy: 0.7681 - val_loss: 0.4682 - val_accuracy: 0.7721\nEpoch 17/20\n146/146 [==============================] - 7s 50ms/step - loss: 0.4855 - accuracy: 0.7709 - val_loss: 0.4667 - val_accuracy: 0.7855\nEpoch 18/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.4950 - accuracy: 0.7673 - val_loss: 0.4966 - val_accuracy: 0.7709\nEpoch 19/20\n146/146 [==============================] - 7s 47ms/step - loss: 0.4841 - accuracy: 0.7772 - val_loss: 0.5106 - val_accuracy: 0.7459\nEpoch 20/20\n146/146 [==============================] - 7s 47ms/step - loss: 0.4868 - accuracy: 0.7724 - val_loss: 0.5122 - val_accuracy: 0.7571\n\n\nLet us again visualize the training accuracy and validation accuracy\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.show()\n\n\n\n\nWith the data agumentation, the validation accuracy of the model was between 60% and 70% There were a few drops in which the accuracy went down to 50 or low 60 but then it went back up to the 70s. In comparsion to model1 the validation accuracy has increased although the training accuracy seem to have been lower than it was in model 1. However, model2 does seem to be way better at acounting for and fixing overfitting.\nThe diffrenece between the models can be seen even better when looking at the graph. the graph for model2 shows how drastically the validation accuracy changed through the training while in model 1 the validation accuracy seemed to have stayed consistently low.\n\n\n\nTo make the model train fatser we can also change the RGB values. Changing the RGB value can also help save our time and let us shift our attention to dealing with actual signal in the data. RBG is the color model used in digital imaging and displays. It stands for Red Green and Blue and is based on the idea that all colors are based on teh combination of those three colors. Often time each color is typically represented by a value ranging from 0 to 255. To make the model train faster we wnat the RGB to be normalized between 0 and 1 which is what we are goining to try and do.\nFor this section we will create model3 with a preprocessing layer. The preprocessing layer coded below will normalize image pixel values\nLet’s break down how to creat the preprocessor layer. First we will start by defining the input tensor for the preprocessing model. Next step is to creat the rescaling layer. The scale_layer will transform the input pixel values. Llastly, the last line bundles the input tensor and the output of the rescaling layer into a standalone Keras model named preprocessor. Essentially what this will do is take an input image/s and produces the normalized version of the image/s as output.\n\n\n# Define the input shape\ni = keras.Input(shape=(150, 150, 3))\n\n# Create the Rescaling layer to normalize pixel values\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n\n# Apply the scaling layer to the input\nx = scale_layer(i)\n\n# Create a preprocessing model\npreprocessor = keras.Model(inputs=[i], outputs=[x])\n\nNext up we will need to creat model 3. Similar to model 1 and 2 we will include all the same factors as well as the agumentation section in addition to the preprocessor. Because we made more changes to teh datat we will also need to make further adjusment to the model to make sure it is more accurate. We increased the dense layer to 256 and chnaged the drop layer to .2\n\nmodel3 = keras.Sequential([\n    preprocessor,  # Preprocessing layer for input normalization\n\n    # Data augmentation layers\n    keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n    keras.layers.RandomRotation(0.2),\n\n   # First Conv2D layer\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    # First MaxPooling2D layer\n    layers.MaxPooling2D((2, 2)),\n\n    # Second Conv2D layer\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    # Second MaxPooling2D layer\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    # Flatten layer to convert 3D feature maps to 1D feature vectors\n    layers.Flatten(),\n\n    # Dense layer\n    layers.Dense(256, activation='relu'),\n\n    # Dropout layer to reduce overfitting\n    layers.Dropout(0.2),\n\n    # Output layer\n    layers.Dense(1, activation='sigmoid')\n])\n\n\n\n# Compile the model\nmodel3.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\n\nhistory = model3.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 9s 48ms/step - loss: 1.3734 - accuracy: 0.5857 - val_loss: 0.6798 - val_accuracy: 0.6470\nEpoch 2/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.6146 - accuracy: 0.6570 - val_loss: 0.6377 - val_accuracy: 0.6930\nEpoch 3/20\n146/146 [==============================] - 7s 47ms/step - loss: 0.5927 - accuracy: 0.6820 - val_loss: 0.5921 - val_accuracy: 0.6862\nEpoch 4/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.5750 - accuracy: 0.6970 - val_loss: 0.5740 - val_accuracy: 0.6973\nEpoch 5/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.5563 - accuracy: 0.7150 - val_loss: 0.5271 - val_accuracy: 0.7356\nEpoch 6/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.5441 - accuracy: 0.7191 - val_loss: 0.5199 - val_accuracy: 0.7390\nEpoch 7/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.5304 - accuracy: 0.7415 - val_loss: 0.5779 - val_accuracy: 0.7223\nEpoch 8/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.5178 - accuracy: 0.7436 - val_loss: 0.5007 - val_accuracy: 0.7687\nEpoch 9/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.5067 - accuracy: 0.7531 - val_loss: 0.4866 - val_accuracy: 0.7687\nEpoch 10/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.4978 - accuracy: 0.7558 - val_loss: 0.5418 - val_accuracy: 0.7451\nEpoch 11/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.4887 - accuracy: 0.7677 - val_loss: 0.4750 - val_accuracy: 0.7618\nEpoch 12/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.4901 - accuracy: 0.7644 - val_loss: 0.4799 - val_accuracy: 0.7713\nEpoch 13/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.4733 - accuracy: 0.7751 - val_loss: 0.5148 - val_accuracy: 0.7455\nEpoch 14/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.4636 - accuracy: 0.7819 - val_loss: 0.4434 - val_accuracy: 0.7902\nEpoch 15/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.4472 - accuracy: 0.7893 - val_loss: 0.5685 - val_accuracy: 0.7528\nEpoch 16/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.4451 - accuracy: 0.7940 - val_loss: 0.4437 - val_accuracy: 0.8040\nEpoch 17/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.4374 - accuracy: 0.7956 - val_loss: 0.4520 - val_accuracy: 0.7954\nEpoch 18/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.4354 - accuracy: 0.7980 - val_loss: 0.4444 - val_accuracy: 0.7928\nEpoch 19/20\n146/146 [==============================] - 7s 48ms/step - loss: 0.4190 - accuracy: 0.8096 - val_loss: 0.4100 - val_accuracy: 0.8052\nEpoch 20/20\n146/146 [==============================] - 7s 49ms/step - loss: 0.4217 - accuracy: 0.8058 - val_loss: 0.4142 - val_accuracy: 0.8108\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model3 Training and Validation Accuracy')\nplt.legend()\nplt.show()\n\n\n\n\nWe were able to get the model Validation accuracy up to 81% this is again big imporvemnt from model 1 and 2. Additionally , were were able to minimize abnd bring down the overfiting by chnaging the drop layer to .2."
  },
  {
    "objectID": "posts/HW 5/index.html#transfer-learning",
    "href": "posts/HW 5/index.html#transfer-learning",
    "title": "HW 5, Image Classifications",
    "section": "",
    "text": "While we were able to create a model that can distingiuse between pictures of dogs and cats, there are people who have already created similar models. In this next section we are going to try to use a pre existing model to do our task.\nWe will first need to start by getting a preexisting model and use it as our base model before incoportating it to our model and train our data on it.\nWe will first start by importing the MobileNetV3Large model and making it into a layer that we will be able to add to our model.\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n12683000/12683000 [==============================] - 0s 0us/step\n\n\nNow that we have the pre made model we will make model4 which will include this new added layer.\nWhile model 4 also included the data agumentation and a layer for classification, it is way more accurat thanks to the base_model_layer.\nLets break down the code for model 4. To make the model look cleaner we created the data_agumentation variable outside of the model construction. We then construct the model by incoportating the base_model_layer and the augmentation layer. We the use GlobalMaxPooling2D to reduces the spatial dimensions of the feature maps to a single maximum value per feature map. We set the drpout at .20 to help minimize overfitting.\nOnce the model is done with compile it and then train it\n\ndata_augmentation = keras.Sequential([\n    keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n    keras.layers.RandomRotation(0.2),\n])\n\n# Model construction\nmodel4 = keras.Sequential([\n    data_augmentation,\n    base_model_layer,\n    layers.GlobalMaxPooling2D(),\n    layers.Dropout(0.2),  # Dropout layer to reduce overfitting\n    layers.Dense(2, activation='softmax')  # Classification layer\n])\n\n# Compile the model\nmodel4.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\n\n\nhistory = model4.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 16s 63ms/step - loss: 1.7371 - accuracy: 0.8186 - val_loss: 0.3864 - val_accuracy: 0.9527\nEpoch 2/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.9484 - accuracy: 0.8830 - val_loss: 0.3616 - val_accuracy: 0.9549\nEpoch 3/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.6807 - accuracy: 0.9038 - val_loss: 0.1966 - val_accuracy: 0.9626\nEpoch 4/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.5961 - accuracy: 0.9031 - val_loss: 0.1960 - val_accuracy: 0.9609\nEpoch 5/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.5247 - accuracy: 0.9025 - val_loss: 0.1887 - val_accuracy: 0.9609\nEpoch 6/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.4708 - accuracy: 0.9059 - val_loss: 0.1530 - val_accuracy: 0.9622\nEpoch 7/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.4174 - accuracy: 0.9087 - val_loss: 0.1354 - val_accuracy: 0.9647\nEpoch 8/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3734 - accuracy: 0.9085 - val_loss: 0.1250 - val_accuracy: 0.9613\nEpoch 9/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3980 - accuracy: 0.8998 - val_loss: 0.3346 - val_accuracy: 0.9248\nEpoch 10/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3886 - accuracy: 0.9020 - val_loss: 0.1424 - val_accuracy: 0.9574\nEpoch 11/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3555 - accuracy: 0.9055 - val_loss: 0.1492 - val_accuracy: 0.9566\nEpoch 12/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3352 - accuracy: 0.9103 - val_loss: 0.1395 - val_accuracy: 0.9617\nEpoch 13/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3212 - accuracy: 0.9045 - val_loss: 0.1145 - val_accuracy: 0.9635\nEpoch 14/20\n146/146 [==============================] - 7s 46ms/step - loss: 0.3161 - accuracy: 0.9090 - val_loss: 0.1256 - val_accuracy: 0.9626\nEpoch 15/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3287 - accuracy: 0.9037 - val_loss: 0.1235 - val_accuracy: 0.9583\nEpoch 16/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3050 - accuracy: 0.9083 - val_loss: 0.2015 - val_accuracy: 0.9398\nEpoch 17/20\n146/146 [==============================] - 6s 43ms/step - loss: 0.3136 - accuracy: 0.9100 - val_loss: 0.1114 - val_accuracy: 0.9682\nEpoch 18/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.3184 - accuracy: 0.9018 - val_loss: 0.1304 - val_accuracy: 0.9592\nEpoch 19/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.2954 - accuracy: 0.9103 - val_loss: 0.1378 - val_accuracy: 0.9553\nEpoch 20/20\n146/146 [==============================] - 6s 44ms/step - loss: 0.2985 - accuracy: 0.9090 - val_loss: 0.1122 - val_accuracy: 0.9622\n\n\n\nmodel4.summary()\n\nModel: \"sequential_26\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n sequential_25 (Sequential)  (None, 150, 150, 3)       0         \n                                                                 \n model_1 (Functional)        (None, 5, 5, 960)         2996352   \n                                                                 \n global_max_pooling2d_1 (Gl  (None, 960)               0         \n obalMaxPooling2D)                                               \n                                                                 \n dropout_22 (Dropout)        (None, 960)               0         \n                                                                 \n dense_43 (Dense)            (None, 2)                 1922      \n                                                                 \n=================================================================\nTotal params: 2998274 (11.44 MB)\nTrainable params: 1922 (7.51 KB)\nNon-trainable params: 2996352 (11.43 MB)\n_________________________________________________________________\n\n\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs_range = range(20)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\n\n\nText(0.5, 1.0, 'Training and Validation Accuracy')\n\n\n\n\n\nWe started with around 50%-60% accuracy in our first model, and now with the help of the base model we were able to bring up the accuracy all the way up to around 95%. As seen in the summary we only have 2998274 parameter to train the model on. There also seem to be very little over fitting of around 2%-3%."
  },
  {
    "objectID": "posts/HW 5/index.html#score-on-test-data",
    "href": "posts/HW 5/index.html#score-on-test-data",
    "title": "HW 5, Image Classifications",
    "section": "",
    "text": "After making all theses models it is time for us to run the accuracy test on our test dataset!!\nSince model4 had the highest accuracy rate I chose to run the test dataset on it.\n\ntest_loss, test_acc = model4.evaluate(test_ds)\nprint(f\"Test accuracy: {test_acc*100:.2f}%\")\n\n37/37 [==============================] - 4s 101ms/step - loss: 0.1498 - accuracy: 0.9553\nTest accuracy: 95.53%\n\n\nLook at that !! We were able to get an accuracy precent of 95.53% !\nThere are a lot of diffrent moving parts and many diffrent things that can be chnaged and corrected. But as we seen in this blog post, simply trying out diffrent things and chnaging the setting can help imporve the accuracy rate of the model greatly!"
  },
  {
    "objectID": "posts/bruin/index.html",
    "href": "posts/bruin/index.html",
    "title": "Creating posts",
    "section": "",
    "text": "### Import all the data and plotly libary\n\nimport plotly.express as px \nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\ndf= penguins \n# changed the name of data frame to make it easier to work with\n\n\n# in order to isnpect the data I printed out the first five rows so I could see the variables I need to work with \ndf.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\ndf=df[df.Sex.notnull()]\n\ncolumes=(['Species','Region', 'Island', 'Sex', 'Culmen Length (mm)','Culmen Depth (mm)','Flipper Length (mm)', 'Body Mass (g)'])\ndfnew=df[columes]\n# this creats a new data fram with the selected columes that I want to be analyzing in my visualization \n\n\ndfnew.head()\n\n\n\n\n\n\n\n\nSpecies\nRegion\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfig=px.scatter(df,x='Flipper Length (mm)',\n               y='Body Mass (g)',\n              color='Species',\n              title='Flipper Length vs Body Mass')\n# this creats a scatter plot that represents the X and Y varaibles as well as distingues speices by color \nfig.show()"
  },
  {
    "objectID": "posts/bruin/index.html#hw0",
    "href": "posts/bruin/index.html#hw0",
    "title": "Creating posts",
    "section": "",
    "text": "### Import all the data and plotly libary\n\nimport plotly.express as px \nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\ndf= penguins \n# changed the name of data frame to make it easier to work with\n\n\n# in order to isnpect the data I printed out the first five rows so I could see the variables I need to work with \ndf.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\ndf=df[df.Sex.notnull()]\n\ncolumes=(['Species','Region', 'Island', 'Sex', 'Culmen Length (mm)','Culmen Depth (mm)','Flipper Length (mm)', 'Body Mass (g)'])\ndfnew=df[columes]\n# this creats a new data fram with the selected columes that I want to be analyzing in my visualization \n\n\ndfnew.head()\n\n\n\n\n\n\n\n\nSpecies\nRegion\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfig=px.scatter(df,x='Flipper Length (mm)',\n               y='Body Mass (g)',\n              color='Species',\n              title='Flipper Length vs Body Mass')\n# this creats a scatter plot that represents the X and Y varaibles as well as distingues speices by color \nfig.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Myblog",
    "section": "",
    "text": "HW 6: Fake News Classification\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\nrecipe-homepage.png\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nHW 3, Webapp\n\n\n\n\n\n\n\nweek 6\n\n\nHW 3\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2024\n\n\nNeomi Goodman\n\n\n\n\n\n\n  \n\n\n\n\nHW 5, Image Classifications\n\n\n\n\n\n\n\nweek 8\n\n\nHW 5\n\n\n\n\n\n\n\n\n\n\n\nFeb 29, 2024\n\n\nNeomi Goodman\n\n\n\n\n\n\n  \n\n\n\n\nHW 6, Fake News Classification\n\n\n\n\n\n\n\nweek 9\n\n\nHW 6\n\n\n\n\n\n\n\n\n\n\n\nFeb 29, 2024\n\n\nNeomi Goodman\n\n\n\n\n\n\n  \n\n\n\n\nHW 4, Heat Diffusion\n\n\n\n\n\n\n\nweek 7\n\n\nHW 4\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nNeomi Goodman\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nNeomi Goodman\n\n\n\n\n\n\n  \n\n\n\n\nHW 2, Web Scarpping\n\n\n\n\n\n\n\nweek 5\n\n\nHW 2\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nNeomi Goodman\n\n\n\n\n\n\n  \n\n\n\n\nHW 1, Database\n\n\n\n\n\n\n\nweek 4\n\n\nHW 1\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2023\n\n\nNeomi Goodman\n\n\n\n\n\n\n  \n\n\n\n\nCreating posts\n\n\n\n\n\n\n\nweek 0\n\n\nHW 0\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2023\n\n\nNeomi Goodman\n\n\n\n\n\n\n  \n\n\n\n\nCreating posts\n\n\n\n\n\n\n\nweek 0\n\n\nHW 0\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2023\n\n\nNeomi Goodman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/bruin/Index1.html",
    "href": "posts/bruin/Index1.html",
    "title": "Creating posts",
    "section": "",
    "text": "For this blog we will cover how to analyze the penguins data sets and go over how we can visualize the data using plotly and pandas! Let’s get started.\n\n\nTo first start up we need to import the plotly and pandas packeges so we can utalize their featurs in our code. Next we need to bring in our dataset so we could analyze it. We bring in the dataset by creating the variable url which containes the url to our code. To read the url we will use pd.read_csv which is a function from the pandas libary which is designed to read a comma-separated values (CSV) file into a pandas DataFrame. The DataFrame is a two-dimensional, size-mutable, and potentially heterogeneous tabular data structure with labeled axes (rows and columns), making it a very powerful tool for data scientists and analysts to work with tabular data.\n\nimport plotly.express as px \nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nPerfect! Now that we have created the dataframe let us change the name of the dataframe so it will be easier to work with! which we might accidently spell penguins wrong, df is a very easy variable to work with.\n\ndf= penguins \n# changed the name of data frame to make it easier to work with\n\nTo see what varaibles exist in our dataframe and what we can work with when analyzing the data frame we can use the function .head(). This will print the 5 top columes of our dataframe.\n\n# in order to isnpect the data I printed out the first five rows so I could see the variables I need to work with \ndf.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\n\n\nNow that we have our dataframe we can starft working with it! the first step to analyze the data is to remove any unncessary columes to avoide any unnecessary noises that might cloud the data. Scientist often do this to make sure they can focuse only on the important datapoints. For our dataframe we are goining to remove all of the none female or male values with the sex columes as well as the comment section. When creating a visualization of our data we have no need for those values.\nWe clean the data by first calling the sex column by using df.Sex syntax. The first line filters the rows of df based on the condition provided by df.Sex.notnull(). Only rows where the ‘Sex’ column has non-null values are retained. We then use columes and call only the columes that we want to work with. Lastly we creat a new dataframe called dfnew that contains only the columns that we chose.\n\ndf=df[df.Sex.notnull()]\n\ncolumes=(['Species','Region', 'Island', 'Sex', 'Culmen Length (mm)','Culmen Depth (mm)','Flipper Length (mm)', 'Body Mass (g)'])\ndfnew=df[columes]\n# this creats a new data fram with the selected columes that I want to be analyzing in my visualization \n\nLet’s get another quick look into our new datafram! Wow so cool, we only have the columns that we asked for now !\n\ndfnew.head()\n\n\n\n\n\n\n\n\nSpecies\nRegion\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\n\n\n\nFrom the new data set I created a visualization comparing Body Mass and Flipper Length (mm) using a scatter plot. I decided to add more complexity to the visualization by having each specie in the data frame be represented in a different color.\nTo create the visualization I used plotly express to make a scatter plot. px.scatter(…): This function is called to create a scatter plot. I then indicated the dataframe I want to use, the x-axis value and the y-axis value. To assign diffrent colors to the point based on their species I included color=‘Species’ and add a title at the end. Lastly, to run the code and present the visualization all we need to do is include fig.show() and then we are done!\n\nfig=px.scatter(dfnew,x='Flipper Length (mm)',\n               y='Body Mass (g)',\n              color='Species',\n              title='Flipper Length vs Body Mass')\n# this creats a scatter plot that represents the X and Y varaibles as well as distingues speices by color \nfig.show()\n\n\n                                                \n\n\nSo cool! By looking at the visualization we can defintly see there is a pattern between the fliper lenagth and body mass based on the type of penguin. We can make so many more type of visualizations such as a bar graph, and analyzes so many more variables! Now that we are done I am sure that you will be able to go on and creat your own visualization using your own dataframe. If you want to start slow feel free to also try analyzing teh penguines datafram. Good luck !"
  },
  {
    "objectID": "posts/bruin/Index1.html#hw0",
    "href": "posts/bruin/Index1.html#hw0",
    "title": "Creating posts",
    "section": "",
    "text": "For this blog we will cover how to analyze the penguins data sets and go over how we can visualize the data using plotly and pandas! Let’s get started.\n\n\nTo first start up we need to import the plotly and pandas packeges so we can utalize their featurs in our code. Next we need to bring in our dataset so we could analyze it. We bring in the dataset by creating the variable url which containes the url to our code. To read the url we will use pd.read_csv which is a function from the pandas libary which is designed to read a comma-separated values (CSV) file into a pandas DataFrame. The DataFrame is a two-dimensional, size-mutable, and potentially heterogeneous tabular data structure with labeled axes (rows and columns), making it a very powerful tool for data scientists and analysts to work with tabular data.\n\nimport plotly.express as px \nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nPerfect! Now that we have created the dataframe let us change the name of the dataframe so it will be easier to work with! which we might accidently spell penguins wrong, df is a very easy variable to work with.\n\ndf= penguins \n# changed the name of data frame to make it easier to work with\n\nTo see what varaibles exist in our dataframe and what we can work with when analyzing the data frame we can use the function .head(). This will print the 5 top columes of our dataframe.\n\n# in order to isnpect the data I printed out the first five rows so I could see the variables I need to work with \ndf.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\n\n\nNow that we have our dataframe we can starft working with it! the first step to analyze the data is to remove any unncessary columes to avoide any unnecessary noises that might cloud the data. Scientist often do this to make sure they can focuse only on the important datapoints. For our dataframe we are goining to remove all of the none female or male values with the sex columes as well as the comment section. When creating a visualization of our data we have no need for those values.\nWe clean the data by first calling the sex column by using df.Sex syntax. The first line filters the rows of df based on the condition provided by df.Sex.notnull(). Only rows where the ‘Sex’ column has non-null values are retained. We then use columes and call only the columes that we want to work with. Lastly we creat a new dataframe called dfnew that contains only the columns that we chose.\n\ndf=df[df.Sex.notnull()]\n\ncolumes=(['Species','Region', 'Island', 'Sex', 'Culmen Length (mm)','Culmen Depth (mm)','Flipper Length (mm)', 'Body Mass (g)'])\ndfnew=df[columes]\n# this creats a new data fram with the selected columes that I want to be analyzing in my visualization \n\nLet’s get another quick look into our new datafram! Wow so cool, we only have the columns that we asked for now !\n\ndfnew.head()\n\n\n\n\n\n\n\n\nSpecies\nRegion\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\n\n\n\nFrom the new data set I created a visualization comparing Body Mass and Flipper Length (mm) using a scatter plot. I decided to add more complexity to the visualization by having each specie in the data frame be represented in a different color.\nTo create the visualization I used plotly express to make a scatter plot. px.scatter(…): This function is called to create a scatter plot. I then indicated the dataframe I want to use, the x-axis value and the y-axis value. To assign diffrent colors to the point based on their species I included color=‘Species’ and add a title at the end. Lastly, to run the code and present the visualization all we need to do is include fig.show() and then we are done!\n\nfig=px.scatter(dfnew,x='Flipper Length (mm)',\n               y='Body Mass (g)',\n              color='Species',\n              title='Flipper Length vs Body Mass')\n# this creats a scatter plot that represents the X and Y varaibles as well as distingues speices by color \nfig.show()\n\n\n                                                \n\n\nSo cool! By looking at the visualization we can defintly see there is a pattern between the fliper lenagth and body mass based on the type of penguin. We can make so many more type of visualizations such as a bar graph, and analyzes so many more variables! Now that we are done I am sure that you will be able to go on and creat your own visualization using your own dataframe. If you want to start slow feel free to also try analyzing teh penguines datafram. Good luck !"
  },
  {
    "objectID": "posts/Hw001/index.html",
    "href": "posts/Hw001/index.html",
    "title": "HW 1, Database",
    "section": "",
    "text": "# import all packages I might need \nimport pandas as pd \nimport sqlite3\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom plotly.io import write_html\nimport plotly.express as px\n\n\n\n\n\n# create a database called db_file \nconn= sqlite3.connect(\"db_file\")\n\n\n\n\n\ndf = pd.read_csv(\"temps.csv\")\ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\n\n\n#clean and prepare the data \ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\n\ndf = prepare_df(df)\n\n\n#creat a table with the temperature data\nfor i, df in enumerate(df_iter):\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\n\n#creats a table with all the stations \nurl = \"station-metadata.csv\"\nstations = pd.read_csv(url)\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n\n27585\n\n\n\n#creats a table with all the countries \ncountries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\ncountries= pd.read_csv(countries_url)\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n279\n\n\n\n\n\n\n#to make sure we have created the database with the three tables we wanted we print a list with our tables\ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('stations',), ('countries',)]\n\n\n\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\n\n\n\n\ncmd = \\\n\"\"\"\nSELECT T.id, T.month, T.temp\nFROM temperatures T\nLEFT JOIN countries C ON SUBSTR(T.id, 1, 2) = SUBSTR(C.\"FIPS 10-4\", 1, 2)\nLEFT JOIN stations S ON SUBSTR(T.id, 1, 2) = SUBSTR(S.id, 1, 2)\nWHERE C.Name IS NOT NULL\n\n\"\"\"\ncursor = conn.cursor()\ncursor.execute(cmd)\nresult = [cursor.fetchone() for i in range(10)]  # get just the first 10 results\nresult\n\n[('AFM00040911', 1, 2.99),\n ('AFM00040911', 1, 2.99),\n ('AFM00040911', 1, 2.99),\n ('AFM00040911', 1, 2.99),\n ('AFM00040911', 1, 2.99),\n ('AFM00040911', 1, 2.99),\n ('AFM00040911', 2, 8.09),\n ('AFM00040911', 2, 8.09),\n ('AFM00040911', 2, 8.09),\n ('AFM00040911', 2, 8.09)]\n\n\n\nconn.close()\n\n\n\n\n\n\n\n\n\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \n   # this connects it to the database we created earlier \n    conn= sqlite3.connect(db_file)\n    \n    query = f\"\"\"\n    SELECT S.NAME AS NAME, S.LATITUDE AS LATITUDE, S.LONGITUDE AS LONGITUDE, C.Name AS Country,\n           T.Year, T.Month, AVG(T.Temp) AS Temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.ID = S.ID\n    LEFT JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.'FIPS 10-4'\n    WHERE C.Name = '{country}'\n          AND T.Year BETWEEN {year_begin} AND {year_end}\n          AND T.Month = {month}\n   \"\"\"\n    \n    df = pd.read_sql_query(query, conn)\n    # this closes the database connction \n    conn.close()\n   \n    return df\n\n\nquery_climate_database(db_file = \"db_file\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n19.765926\n\n\n\n\n\n\n\n\n\n\n\n\n\n# I first created a function to calculate the yearly temp coefficient so I will not have to do this calculation within my next function and the work would be presented more clearly \n\ndef calc_temp_coefficient(data_group):\n    x = data_group[['Year']].values\n    y = data_group['Temp'].values\n    if len(x) &lt; 2:\n        return np.nan  # this is in case there is not enough data to caluclate the regression \n    lr = LinearRegression()\n    lr.fit(x, y)\n    return round(lr.coef_[0], 5) \n\n# a function that shows how the average yearly change in temperature vary within a given country\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs=10 ,**kwargs): \n    conn=sqlite3.connect(\"db_file\")\n    \n    # SQL query to get the temperature data for a the specified country in a year range and month\n    query = f\"\"\"\n    SELECT S.NAME AS Station_Name, S.LATITUDE AS Latitude, S.LONGITUDE AS Longitude,\n           C.Name AS Country, T.Year, T.Month, AVG(T.Temp) AS Temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.ID = S.ID\n    LEFT JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.'FIPS 10-4'\n    WHERE C.Name = '{country}'\n          AND T.Year BETWEEN {year_begin} AND {year_end}\n          AND T.Month = {month}\n    GROUP BY S.NAME, T.Year, T.Month\n    \"\"\"\n  \n    # I used .read_sql_querey to read the dtatbase datat and store it as Pandas DataFrame\n    df = pd.read_sql_query(query, conn)\n    \n    # filter stations with at least min_obs years of data value and group them by the station ID \n    df = df.groupby('Station_Name').filter(lambda x: x['Year'].count() &gt;= min_obs)\n    \n   \n     # Calculate the estimated yearly temperature increase for each station\n    yearly_changes = df.groupby('Station_Name').apply(\n        lambda group: calc_temp_coefficient(group)\n    ).reset_index(name='Estimated Yearly Increase (°C)')\n    \n    \n     # Create a Plotly Express scatter mapbox plot\n    fig = px.scatter_mapbox(\n        df,\n        lat='Latitude',\n        lon='Longitude',\n        hover_name='Station_Name',\n        color='Temp',  # Use the calculated column as color\n        height=600,\n        **kwargs\n    )\n    \n    # Set layout options and styling for the plot\n    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n    \n    # Return the figure\n    return fig\n\n\n\n\n\n\n\nfrom climate_database import query_climate_database\nfrom climate_database import calc_temp_coefficient\nfrom climate_database import temperature_coefficient_plot\n\nimport inspect\n#print(inspect.getsource(query_climate_database))\n#print(inspect.getsource(calc_temp_coefficient))\n#print(inspect.getsource(temperature_coefficient_plot))\n\n\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file=\"db_file\",country=\"India\",year_begin=1980, year_end=2020, month=1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map,\n                                   title=\"Estimates of the Yearly Temperature increase in India \")\n\n\nfig.show()\n\nValueError: Value of 'color' is not the name of a column in 'data_frame'. Expected one of ['Station_Name', 'Latitude', 'Longitude', 'Country', 'Year', 'Month', 'Temp'] but received: EYI\n\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file=\"db_file\",country=\"France\",year_begin=1980, year_end=2020, month=9, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map,\n                                   title=\"Estimates of the Yearly Temperature increase in France\")\n\nfig.show()\n\n\n\n\n\n\n\n\n\nfrom climate_database import analyze_seasonal_temperature_change\n\n\nimport inspect\nprint(inspect.getsource(analyze_seasonal_temperature_change))\n\n\ndb_file = \"db_file\"\n\n# Define the year range\nyear_range = (2000, 2020)\n\n# Define the country (optional)\ncountry = \"USA\"\n\n# Call the function\nanalyze_seasonal_temperature_change(db_file, year_range)\n\n\n\n\n\nimport plotly.express as px\n\ndef visualize_avg_monthly_temperature_by_country(df):\n    fig = px.line(df, x='Month', y='Avg_Temperature', title='Average Monthly Temperature by Country')\n    return fig\n\ndef get_data_from_database(db_file):\n    conn = sqlite3.connect(db_file)\n    query = \"SELECT * FROM temperatures\"  # Adjust your SQL query accordingly\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n    return df\n\n\nmy_df = get_data_from_database(\"db_file\")\nfig = visualize_avg_monthly_temperature_by_country(my_df)\n\n\ndef visualize_monthly_temperature_trends_for_station(df):\n    fig = px.line(df, x='Month', y='Temp', color='Year', title='Monthly Temperature Trends for a Specific Station')\n    return fig"
  },
  {
    "objectID": "posts/Hw001/index.html#part-2-create-a-query-function",
    "href": "posts/Hw001/index.html#part-2-create-a-query-function",
    "title": "HW 1, Database",
    "section": "",
    "text": "def query_climate_database(db_file, country, year_begin, year_end, month):\n    \n   # this connects it to the database we created earlier \n    conn= sqlite3.connect(db_file)\n    \n    query = f\"\"\"\n    SELECT S.NAME AS NAME, S.LATITUDE AS LATITUDE, S.LONGITUDE AS LONGITUDE, C.Name AS Country,\n           T.Year, T.Month, AVG(T.Temp) AS Temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.ID = S.ID\n    LEFT JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.'FIPS 10-4'\n    WHERE C.Name = '{country}'\n          AND T.Year BETWEEN {year_begin} AND {year_end}\n          AND T.Month = {month}\n   \"\"\"\n    \n    df = pd.read_sql_query(query, conn)\n    # this closes the database connction \n    conn.close()\n   \n    return df\n\n\nquery_climate_database(db_file = \"db_file\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n19.765926"
  },
  {
    "objectID": "posts/Hw001/index.html#part-3-create-an-interactive-viusalization",
    "href": "posts/Hw001/index.html#part-3-create-an-interactive-viusalization",
    "title": "HW 1, Database",
    "section": "",
    "text": "# I first created a function to calculate the yearly temp coefficient so I will not have to do this calculation within my next function and the work would be presented more clearly \n\ndef calc_temp_coefficient(data_group):\n    x = data_group[['Year']].values\n    y = data_group['Temp'].values\n    if len(x) &lt; 2:\n        return np.nan  # this is in case there is not enough data to caluclate the regression \n    lr = LinearRegression()\n    lr.fit(x, y)\n    return round(lr.coef_[0], 5) \n\n# a function that shows how the average yearly change in temperature vary within a given country\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs=10 ,**kwargs): \n    conn=sqlite3.connect(\"db_file\")\n    \n    # SQL query to get the temperature data for a the specified country in a year range and month\n    query = f\"\"\"\n    SELECT S.NAME AS Station_Name, S.LATITUDE AS Latitude, S.LONGITUDE AS Longitude,\n           C.Name AS Country, T.Year, T.Month, AVG(T.Temp) AS Temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.ID = S.ID\n    LEFT JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.'FIPS 10-4'\n    WHERE C.Name = '{country}'\n          AND T.Year BETWEEN {year_begin} AND {year_end}\n          AND T.Month = {month}\n    GROUP BY S.NAME, T.Year, T.Month\n    \"\"\"\n  \n    # I used .read_sql_querey to read the dtatbase datat and store it as Pandas DataFrame\n    df = pd.read_sql_query(query, conn)\n    \n    # filter stations with at least min_obs years of data value and group them by the station ID \n    df = df.groupby('Station_Name').filter(lambda x: x['Year'].count() &gt;= min_obs)\n    \n   \n     # Calculate the estimated yearly temperature increase for each station\n    yearly_changes = df.groupby('Station_Name').apply(\n        lambda group: calc_temp_coefficient(group)\n    ).reset_index(name='Estimated Yearly Increase (°C)')\n    \n    \n     # Create a Plotly Express scatter mapbox plot\n    fig = px.scatter_mapbox(\n        df,\n        lat='Latitude',\n        lon='Longitude',\n        hover_name='Station_Name',\n        color='Temp',  # Use the calculated column as color\n        height=600,\n        **kwargs\n    )\n    \n    # Set layout options and styling for the plot\n    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n    \n    # Return the figure\n    return fig\n\n\n\n\n\n\n\nfrom climate_database import query_climate_database\nfrom climate_database import calc_temp_coefficient\nfrom climate_database import temperature_coefficient_plot\n\nimport inspect\n#print(inspect.getsource(query_climate_database))\n#print(inspect.getsource(calc_temp_coefficient))\n#print(inspect.getsource(temperature_coefficient_plot))\n\n\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file=\"db_file\",country=\"India\",year_begin=1980, year_end=2020, month=1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map,\n                                   title=\"Estimates of the Yearly Temperature increase in India \")\n\n\nfig.show()\n\nValueError: Value of 'color' is not the name of a column in 'data_frame'. Expected one of ['Station_Name', 'Latitude', 'Longitude', 'Country', 'Year', 'Month', 'Temp'] but received: EYI\n\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file=\"db_file\",country=\"France\",year_begin=1980, year_end=2020, month=9, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map,\n                                   title=\"Estimates of the Yearly Temperature increase in France\")\n\nfig.show()"
  },
  {
    "objectID": "posts/Hw001/index.html#part-4-creating-two-additional-visuals",
    "href": "posts/Hw001/index.html#part-4-creating-two-additional-visuals",
    "title": "HW 1, Database",
    "section": "",
    "text": "from climate_database import analyze_seasonal_temperature_change\n\n\nimport inspect\nprint(inspect.getsource(analyze_seasonal_temperature_change))\n\n\ndb_file = \"db_file\"\n\n# Define the year range\nyear_range = (2000, 2020)\n\n# Define the country (optional)\ncountry = \"USA\"\n\n# Call the function\nanalyze_seasonal_temperature_change(db_file, year_range)\n\n\n\n\n\nimport plotly.express as px\n\ndef visualize_avg_monthly_temperature_by_country(df):\n    fig = px.line(df, x='Month', y='Avg_Temperature', title='Average Monthly Temperature by Country')\n    return fig\n\ndef get_data_from_database(db_file):\n    conn = sqlite3.connect(db_file)\n    query = \"SELECT * FROM temperatures\"  # Adjust your SQL query accordingly\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n    return df\n\n\nmy_df = get_data_from_database(\"db_file\")\nfig = visualize_avg_monthly_temperature_by_country(my_df)\n\n\ndef visualize_monthly_temperature_trends_for_station(df):\n    fig = px.line(df, x='Month', y='Temp', color='Year', title='Monthly Temperature Trends for a Specific Station')\n    return fig"
  },
  {
    "objectID": "posts/HW4/index.html",
    "href": "posts/HW4/index.html",
    "title": "HW 4, Heat Diffusion",
    "section": "",
    "text": "Below we set up and intalizes our N and epsilon variables as well as set up the main conditions for the heat difussion starting with the point in the middle\n\nN represents the size of the gride as well as the number of points that we will have on the dimension that the simulation is run on. Since we are working on a 2D gride the N gride will be N x N. The larger the vlaue of N is , the finer the gride will be. For this blog we will set up the N value to be 101\nEpsilon is the parameter that represents the stability constant or time step size in the simulation. Since we are working with heat difussion it can also be used to represent a scaling factor for the diffusion rate (this impacts the spead). Here we set the epsilon to be 0.2\n\n\nN = 101\nepsilon = 0.2\n\nBelow we imported some of the main packages we will need to use to both measure the time each method took and to present the heat difussion. we also constructed a gride with the initial condition to use as a starting point.\n\nimport time #so I could caluclate the time each method took\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# construct initial condition: 1 unit of heat at midpoint.\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n&lt;matplotlib.image.AxesImage at 0x7ec2cd0da6e0&gt;"
  },
  {
    "objectID": "posts/HW4/index.html#set-up",
    "href": "posts/HW4/index.html#set-up",
    "title": "HW 4, Heat Diffusion",
    "section": "",
    "text": "Below we set up and intalizes our N and epsilon variables as well as set up the main conditions for the heat difussion starting with the point in the middle\n\nN represents the size of the gride as well as the number of points that we will have on the dimension that the simulation is run on. Since we are working on a 2D gride the N gride will be N x N. The larger the vlaue of N is , the finer the gride will be. For this blog we will set up the N value to be 101\nEpsilon is the parameter that represents the stability constant or time step size in the simulation. Since we are working with heat difussion it can also be used to represent a scaling factor for the diffusion rate (this impacts the spead). Here we set the epsilon to be 0.2\n\n\nN = 101\nepsilon = 0.2\n\nBelow we imported some of the main packages we will need to use to both measure the time each method took and to present the heat difussion. we also constructed a gride with the initial condition to use as a starting point.\n\nimport time #so I could caluclate the time each method took\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# construct initial condition: 1 unit of heat at midpoint.\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n&lt;matplotlib.image.AxesImage at 0x7ec2cd0da6e0&gt;"
  },
  {
    "objectID": "posts/HW4/index.html#visualization",
    "href": "posts/HW4/index.html#visualization",
    "title": "HW 4, Heat Diffusion",
    "section": "Visualization",
    "text": "Visualization\nNow that we created out two functions we are finally ready to start visualizing our heat difussion maps.\n\nSet up before visualization\nBefore we can start using pyplot we need to creat snapshots of the gride state every 300 iteration to make it easier to graph.\n\n\nWe start by intializing the Gride state and Parameters\nwe use u0 for the gride state to use the intial state we created at the begining of the blog post. We also make sure to use the epsilon and N parameters that we established at the begining\n\n\nThen we construct the Laplace Operator Matrix A\nhere we call our get_A function to get our A matrix #### lastly, we run the simulation We created a loop following i to iterate 2700 times. Each time we iterate it represents a timestep in the simulation. It is importnat that we make sure to also update the gride state to represents the new iteration. We can update the gride by applying the matrix A to the current state u through the function advance_time_matvecmul. For the last part of this section, it is important that we store the intermediate solutions. Since 2700 is a lot of iterations we only want to keep track of every 300 iterations starting at 300 which we indicated in our if loop. We then append the iteration to our list ” solutions1 and we are now finally ready to start making cool heat difussions !!!\n\n# Initialize your grid state 'u' and stability constant 'epsilon'\nu = u0  #  initial grid state\nepsilon = 0.2  # stability constant\nN = 101\n\n# Get the matrix A using the function get_A(N)\nA = get_A(N)\n\n# Initialize a list to store the intermediate solutions\nsolutions1 = []\n\ns_time1=time.time()\n\n# Run the simulation for 2700 iterations\nfor i in range(2701):\n    u = advance_time_matvecmul(A, u, epsilon)\n\n    # Store the solution every 300 iterations\n    if i % 300 ==0 and i !=0:\n        solutions1.append(u.copy())\n#to calculate the total time it took to create it\ned_time1=time.time()\ntotal_time1=ed_time1- s_time1\n\n# Now, 'solutions' contains the grid state every 300 iterations\n\n\n\nGraphing\nWe will use matplot to create our visualizations. First, we create a 3x# grid of the subplots to show each of heat difussions that we captured at the diffrent iterations.\nWe start by looping throut our list with the iterations.\nThe row = i // 3 and col = i % 3 section calculate the row and column positions for the subplot that corresponds to the current solution. This arrangement distributes the plots across the 3x3 grid based on their index. // is the floor division operator, ensuring an integer result, and % gives the remainder, effectively wrapping the column index after every third plot.\nlastly, we use pyplot to graph everything :)\n\nimport matplotlib.pyplot as plt\n\n# Create a 3x3 grid of subplots\nfig, axs = plt.subplots(3, 3, figsize=(10, 10))\n\n\n# Loop over the solutions and plot each one\nfor i, solution in enumerate(solutions1):\n    # Calculate the row and column indices for the subplot\n    row = i // 3\n    col = i % 3\n\n    # Plot the solution on the subplot\n    axs[row, col].imshow(solution, cmap='viridis', interpolation='nearest')\n    axs[row, col].set_title(f\"Iteration {(i+1)*300}\")\n\n\n# Display the plot\nplt.show()\n\n\n\n\n\nWow look how pretty they all look!! To be able to compare the diffrent methods we also made sure to capture how long it took to creat the heat diffusions as can be seen right below\n\nprint (f\"Total time it took:{total_time1:0.2f}sec\")\n\nTotal time it took:217.09sec"
  },
  {
    "objectID": "posts/Index.html",
    "href": "posts/Index.html",
    "title": "HW 3, Webapp",
    "section": "",
    "text": "To first start I created three HTML files, base.html, submit.html, and view.html. each one of them fullfill a diffrent porpose that I will explain below. The app.py file is the application script file where is the main file that will handle the flask functions and any other function that we will use in the creation of the webapp.\n\n\n\nThis HTML code sets up the main page that opens up when you open the URL. Within the first page, you are displayed with the two links, to either see the message or to submit a message, as well as a list of random messages that have been submitted previously. The other HTML sites will essentially inherent from the base page.\n&lt;!doctype html&gt; \n\n{% block title %}{% endblock %} - PIC16B Website\n\n\n/* this section changes the diffrent colors of the title*/\n\n\n\nPlease follow the submit a message link to submit your message.\n\n\n\n/* this displays the two links on the main page, one to submit a message, and the second one to view the message*/\n\nSubmit a Message\n\n\nView Messages\n\n\n\n\n\n{% block header %}{% endblock %}\n\n{% block content %}{% endblock %}\n\n\n/* this is a secondary header that will show a list of the random messages */\n\n\n\n{% for message in messages %} \n\n{{ message[1] }}: {{ message[2] }}\n\n{% endfor %}\n\n\n\n\n\n\nThis html takes care of the page in which indvdiuals submit their message. They are asked to provide their name and submit a message. Once submitting, they can then also chose to see any previous message or be directed to main page.\n{% extends ‘base.html’ %}\n{% block header %}\n\n{% endblock %}\n{% block content %}  /* this sets up the two links for people to either view the message or go back to the first main page */\n\nReturn to main page\n\nView Messages\n\n /* this will change the header to blue and the submit botton to green*/\n\n/* this section prompts the users to write their name and leave a message, and creates the submit botton*/\n\n&lt;label for-“name”&gt; Your name.  Please write a message.  \n\n\n{% endblock %}\n\n\n\nThis last html presents the messages that have been submmited and orders them inside a table so it is easier to view past messages. At the bottom of the post I also included the code for the style.css file. For now, the pourpose of that file is to modify the webapp by changing the color of the page, the font, the location of the text, and any other stylistic changes you wish to make.\n&lt;!doctype html&gt;\n\n\n\n\n\n\n/* this for loop goes through all the messages and organizes them inside a table */ {% for message in messages %}\n\n{% endfor %}\n\n\n\n\nID\n\n\nName\n\n\nMessage\n\n\n\n{{ message[0] }}\n\n\n{{ message[1] }}\n\n\n{{ message[2] }}\n\n\n/* this creats a link that allows people to go back and submit another message*/\n\n\n\n\n\n\nWhen working on this project you want to make sure your folder with all the files is organized in a certain way. Inside your project directory you want to have three main folders called: app.py, templates, and static. Inside the app.py you will have your functions, inside the templates you will add your base.html,submit.html, and view.html files. And lastly, inside the static folder you will have your style.css.\n\n\nNow that we have everything organized and set up we are ready to start writing the functions !\n\n\n\nfrom flask import Flask, g, render_template, request\nimport sqlite3\n\n\nimport sklearn as sk\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pickle\nimport os\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\nfrom matplotlib.figure import Figure\n\nimport io\nimport base64\n\n\n\n\n\nFor this project we will need to use route functions. A route function is a URL patter that Flask is able to read and respond to. To ensure that a specific function is called and run when the URL is opned we use “@app.route” to indicate which function we want to use.\nTo create the flask web server instance run the following code:\n\napp = Flask(__name__)\n\nBelow are all the route functions we will need to use, after that I will go into the main functions that theses route functions are refering back to.\nFirst we start by creating a database and naming it.The base function retrives 3 random messages and renders the base.html templetes, which also passes the messages to be displayed on the main page. The submit function takes the POST commend to add the message into the database. The submit.html templet is then rendered to prompt users to submit a message. Lastly, the message function retrives all the messages that were added to the database amd renders the view.html template which displays the messages to the users.\n\n\n\"\"\"\nThis script contains routes and functions for a Flask web application.\n\nIt defines routes for the main page, message submission, and viewing messages.\n\"\"\"\nDATABASE = 'messages_db.sqlite'\n \"\"\"\n    Renders the main page of the web application.\n\n    Retrieves 3 random messages and renders the 'base.html' template with these messages.\n    \"\"\"\n@app.route('/')\ndef base():\n    messages = random_messages(3)  # Retrieve 3 random messages\n    return render_template('base.html', messages=messages)\n\n  \"\"\"\n    Handles message submission.\n\n    If the request method is POST, it inserts the message into the database.\n    Renders the 'submit.html' template.\n   \"\"\"\n@app.route('/submit', methods=['GET','POST'])\ndef submit():\n    if request.method == 'POST':\n        insert_message(request)\n    return render_template('submit.html')\n\n\"\"\"\n    Displays all messages stored in the database.\n\n    Retrieves all messages from the database and renders the 'view.html' template with these messages.\n\"\"\"\n@app.route('/message')\ndef message():\n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute(\"SELECT * FROM messages\")\n    messages = cursor.fetchall()\n    cursor.close()\n    return render_template('view.html', messages=messages)\n\n\nIndentationError: unindent does not match any outer indentation level (&lt;tokenize&gt;, line 17)\n\n\n\n\n\nThe pourpose of this function is to handle the messages that are being submitted by creating a database. The function first checks the SQL database ( which is called message_db) to see if the database exists. If it does not, it creates a new one and stores it in g.message_db. Once that is done it followes the SQL command to create the message table with the added message, and end the connection once done.\n\n# Function to get the message database\n\"\"\"\n    Retrieve the SQLite database connection for storing messages.\n\n    If the 'message_db' attribute is not present in the global 'g' object, \n    it creates a new connection to the SQLite database defined by the \n    'DATABASE' constant. It also creates a 'messages' table in the database \n    if it doesn't already exist.\n\n    Returns:\n    - sqlite3.Connection: The SQLite database connection object.\n    \"\"\"\ndef get_message_db():\n    if 'message_db' not in g:\n        g.message_db = sqlite3.connect(DATABASE)\n        cursor = g.message_db.cursor()\n        cursor.execute('''CREATE TABLE IF NOT EXISTS messages (\n                            id INTEGER PRIMARY KEY,\n                            handle TEXT,\n                            message TEXT\n                        )''')\n        g.message_db.commit()\n    return g.message_db\n    \n\n\nWe then need to create a function that inserts the message into the database.It retracts the name of the user and their message from the form data using the request.form[‘nm’] and request.form[‘message’].Then, it executes an SQL query to insert the name and message into the ‘messages’ table in the database.\n\n\n\nThis function than deals with storing the messages submitted by the users. it calles the get_message_db to connect back to the database, connect to the cursor and then followes the SQL command to add the message with the id ( the name inputted by the user) to the message table. Once that is done it commits the function and closes the database connection again\n\ndef insert_message(request):\n     \"\"\"\n    Insert a user message into the database.\n\n    Args:\n    - request (flask.request): The request object containing form data.\n\n    Returns:\n    - str: The message that was inserted into the database.\n    \"\"\"\n    db = get_message_db()\n    cursor = db.cursor()\n    name = request.form['nm']\n    message = request.form['message']\n    cursor.execute(\"INSERT INTO messages (handle, message) VALUES (?, ?)\", (name, message))\n    db.commit()\n    cursor.close()\n    return message\n \n\nIndentationError: unindent does not match any outer indentation level (&lt;tokenize&gt;, line 11)\n\n\n\n\n\nLastly, the last two functions display the page that presents the list of random messages and randomizes the message that are being presented.The functions are used to rabndomize five diffrent messages from the database. This will be seen in our main page. The function works by connecting to the cursor, executing SQL command by selecting rnaodm messages before closing the database connection again.\n\n@app.route('/view_messages')\ndef view_messages():\n     \"\"\"\n    Display a page showing a random selection of messages.\n\n    Returns:\n    - flask.render_template: HTML page displaying the random messages.\n    \"\"\"\n    messages = random_messages(5)  # You can change the number of messages to retrieve\n    return render_template('view.html', messages=messages)\n\n# Function to retrieve n random messages from the database\ndef random_messages(n):\n    \"\"\"\n    Retrieve n random messages from the database.\n\n    Args:\n    - n (int): Number of random messages to retrieve.\n\n    Returns:\n    - list: A list of tuples containing the retrieved messages.\n    \"\"\"\n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute(\"SELECT * FROM messages ORDER BY RANDOM() LIMIT ?\", (n,))\n    messages = cursor.fetchall()\n    cursor.close()\n    db.close()  # Close the database connection\n    return messages\n\n\n\n\n\n\n\nset FLASK_ENV=app.py\nflask run\n\n\n\n\n\nAs you can see there are two links: submit message and view message in the main page. This is were our rout functions come into play. We are essentially calling submit.html and view.html routes in order to be able to be directed to another page\n\n\n\nbetter front page.png\n\n\n\n\n\n\nOnce opening the submit a message url you are prompted to this page. Like I talked about before, this is where we need to utalize the Get and Post methods in order to be able to repsond to HTTP request and send the datat of the submitted messages.\n\n\n\nbetter submit page.png\n\n\n\n\n\nThe last part is to send the users to a page where they can view their messages. This is where the message route message function and view.html come into play. Since we are not transmitting any data we will only be using the GET method in this route function. Below I added the route message again so we can further examine it\nThe function goes into the database with the messages and uses .fetchall() to select all the messages submitted by the user.Once it is done it makes sure to close the sursor again and return our view.html templet. By doing so we are able to see the page depicted below , with some of the message I wrote.\n\n@app.route('/message')\ndef message():\n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute(\"SELECT * FROM messages\")\n    messages = cursor.fetchall()\n    cursor.close()\n    return render_template('view.html', messages=messages)\n\n\n\n\ncolor message listy.png\n\n\n\n\n\n\nhtml {\n    font-family: Times New Roman;\n    background: aliceblue;\n    padding: 1rem;\n}\n\nbody {\n    background:aliceblue;\n    font-family: Times New Roman;\n    max-width: 900px;\n    margin: 0 auto;\n}\n\nh1 {\n    color: rgb(0, 0, 0);\n    font-family: Times New Roman;\n    margin: 1rem 0;\n    text-align: center;\n}"
  },
  {
    "objectID": "posts/Index.html#important-note",
    "href": "posts/Index.html#important-note",
    "title": "HW 3, Webapp",
    "section": "",
    "text": "When working on this project you want to make sure your folder with all the files is organized in a certain way. Inside your project directory you want to have three main folders called: app.py, templates, and static. Inside the app.py you will have your functions, inside the templates you will add your base.html,submit.html, and view.html files. And lastly, inside the static folder you will have your style.css.\n\n\nNow that we have everything organized and set up we are ready to start writing the functions !\n\n\n\nfrom flask import Flask, g, render_template, request\nimport sqlite3\n\n\nimport sklearn as sk\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pickle\nimport os\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\nfrom matplotlib.figure import Figure\n\nimport io\nimport base64\n\n\n\n\n\nFor this project we will need to use route functions. A route function is a URL patter that Flask is able to read and respond to. To ensure that a specific function is called and run when the URL is opned we use “@app.route” to indicate which function we want to use.\nTo create the flask web server instance run the following code:\n\napp = Flask(__name__)\n\nBelow are all the route functions we will need to use, after that I will go into the main functions that theses route functions are refering back to.\nFirst we start by creating a database and naming it.The base function retrives 3 random messages and renders the base.html templetes, which also passes the messages to be displayed on the main page. The submit function takes the POST commend to add the message into the database. The submit.html templet is then rendered to prompt users to submit a message. Lastly, the message function retrives all the messages that were added to the database amd renders the view.html template which displays the messages to the users.\n\n\n\"\"\"\nThis script contains routes and functions for a Flask web application.\n\nIt defines routes for the main page, message submission, and viewing messages.\n\"\"\"\nDATABASE = 'messages_db.sqlite'\n \"\"\"\n    Renders the main page of the web application.\n\n    Retrieves 3 random messages and renders the 'base.html' template with these messages.\n    \"\"\"\n@app.route('/')\ndef base():\n    messages = random_messages(3)  # Retrieve 3 random messages\n    return render_template('base.html', messages=messages)\n\n  \"\"\"\n    Handles message submission.\n\n    If the request method is POST, it inserts the message into the database.\n    Renders the 'submit.html' template.\n   \"\"\"\n@app.route('/submit', methods=['GET','POST'])\ndef submit():\n    if request.method == 'POST':\n        insert_message(request)\n    return render_template('submit.html')\n\n\"\"\"\n    Displays all messages stored in the database.\n\n    Retrieves all messages from the database and renders the 'view.html' template with these messages.\n\"\"\"\n@app.route('/message')\ndef message():\n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute(\"SELECT * FROM messages\")\n    messages = cursor.fetchall()\n    cursor.close()\n    return render_template('view.html', messages=messages)\n\n\nIndentationError: unindent does not match any outer indentation level (&lt;tokenize&gt;, line 17)\n\n\n\n\n\nThe pourpose of this function is to handle the messages that are being submitted by creating a database. The function first checks the SQL database ( which is called message_db) to see if the database exists. If it does not, it creates a new one and stores it in g.message_db. Once that is done it followes the SQL command to create the message table with the added message, and end the connection once done.\n\n# Function to get the message database\n\"\"\"\n    Retrieve the SQLite database connection for storing messages.\n\n    If the 'message_db' attribute is not present in the global 'g' object, \n    it creates a new connection to the SQLite database defined by the \n    'DATABASE' constant. It also creates a 'messages' table in the database \n    if it doesn't already exist.\n\n    Returns:\n    - sqlite3.Connection: The SQLite database connection object.\n    \"\"\"\ndef get_message_db():\n    if 'message_db' not in g:\n        g.message_db = sqlite3.connect(DATABASE)\n        cursor = g.message_db.cursor()\n        cursor.execute('''CREATE TABLE IF NOT EXISTS messages (\n                            id INTEGER PRIMARY KEY,\n                            handle TEXT,\n                            message TEXT\n                        )''')\n        g.message_db.commit()\n    return g.message_db\n    \n\n\nWe then need to create a function that inserts the message into the database.It retracts the name of the user and their message from the form data using the request.form[‘nm’] and request.form[‘message’].Then, it executes an SQL query to insert the name and message into the ‘messages’ table in the database.\n\n\n\nThis function than deals with storing the messages submitted by the users. it calles the get_message_db to connect back to the database, connect to the cursor and then followes the SQL command to add the message with the id ( the name inputted by the user) to the message table. Once that is done it commits the function and closes the database connection again\n\ndef insert_message(request):\n     \"\"\"\n    Insert a user message into the database.\n\n    Args:\n    - request (flask.request): The request object containing form data.\n\n    Returns:\n    - str: The message that was inserted into the database.\n    \"\"\"\n    db = get_message_db()\n    cursor = db.cursor()\n    name = request.form['nm']\n    message = request.form['message']\n    cursor.execute(\"INSERT INTO messages (handle, message) VALUES (?, ?)\", (name, message))\n    db.commit()\n    cursor.close()\n    return message\n \n\nIndentationError: unindent does not match any outer indentation level (&lt;tokenize&gt;, line 11)\n\n\n\n\n\nLastly, the last two functions display the page that presents the list of random messages and randomizes the message that are being presented.The functions are used to rabndomize five diffrent messages from the database. This will be seen in our main page. The function works by connecting to the cursor, executing SQL command by selecting rnaodm messages before closing the database connection again.\n\n@app.route('/view_messages')\ndef view_messages():\n     \"\"\"\n    Display a page showing a random selection of messages.\n\n    Returns:\n    - flask.render_template: HTML page displaying the random messages.\n    \"\"\"\n    messages = random_messages(5)  # You can change the number of messages to retrieve\n    return render_template('view.html', messages=messages)\n\n# Function to retrieve n random messages from the database\ndef random_messages(n):\n    \"\"\"\n    Retrieve n random messages from the database.\n\n    Args:\n    - n (int): Number of random messages to retrieve.\n\n    Returns:\n    - list: A list of tuples containing the retrieved messages.\n    \"\"\"\n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute(\"SELECT * FROM messages ORDER BY RANDOM() LIMIT ?\", (n,))\n    messages = cursor.fetchall()\n    cursor.close()\n    db.close()  # Close the database connection\n    return messages"
  },
  {
    "objectID": "posts/Index.html#now-is-the-fun-part",
    "href": "posts/Index.html#now-is-the-fun-part",
    "title": "HW 3, Webapp",
    "section": "",
    "text": "set FLASK_ENV=app.py\nflask run\n\n\n\n\n\nAs you can see there are two links: submit message and view message in the main page. This is were our rout functions come into play. We are essentially calling submit.html and view.html routes in order to be able to be directed to another page\n\n\n\nbetter front page.png\n\n\n\n\n\n\nOnce opening the submit a message url you are prompted to this page. Like I talked about before, this is where we need to utalize the Get and Post methods in order to be able to repsond to HTTP request and send the datat of the submitted messages.\n\n\n\nbetter submit page.png\n\n\n\n\n\nThe last part is to send the users to a page where they can view their messages. This is where the message route message function and view.html come into play. Since we are not transmitting any data we will only be using the GET method in this route function. Below I added the route message again so we can further examine it\nThe function goes into the database with the messages and uses .fetchall() to select all the messages submitted by the user.Once it is done it makes sure to close the sursor again and return our view.html templet. By doing so we are able to see the page depicted below , with some of the message I wrote.\n\n@app.route('/message')\ndef message():\n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute(\"SELECT * FROM messages\")\n    messages = cursor.fetchall()\n    cursor.close()\n    return render_template('view.html', messages=messages)\n\n\n\n\ncolor message listy.png\n\n\n\n\n\n\nhtml {\n    font-family: Times New Roman;\n    background: aliceblue;\n    padding: 1rem;\n}\n\nbody {\n    background:aliceblue;\n    font-family: Times New Roman;\n    max-width: 900px;\n    margin: 0 auto;\n}\n\nh1 {\n    color: rgb(0, 0, 0);\n    font-family: Times New Roman;\n    margin: 1rem 0;\n    text-align: center;\n}"
  },
  {
    "objectID": "posts/Project/index.html",
    "href": "posts/Project/index.html",
    "title": "",
    "section": "",
    "text": "---\ntitle: \"Final Project: Recipe Generator Web App - Eat Your Veggies!\"\nauthor: \"Gianna Pedroza, Neomi Goodman, Narayanan Kannan\"\ndate: \"2024-3-21\"\ncategories: [final project]\n---"
  },
  {
    "objectID": "posts/Project/index.html#i.-recipes",
    "href": "posts/Project/index.html#i.-recipes",
    "title": "",
    "section": "I. Recipes",
    "text": "I. Recipes\nOne of the first steps for this project is scraping a a dataset of recipes in order to recommend recipes to the user of the web app. We chose to scrape the recipes from AllRecipes.com because it specifically has a cuisines page that lists 49 different cuisines, and each recipe page has a detailed list of ingredients. One downfall of scraping this page that we noticed later on was that some common cuisines are missing from the main page, such as Mexican cuisine. However, we chose to keep the database to only these recipes in order to simplify the recommendation system since the other pages do not order the recipes by cuisine which we wanted to be one of the determining factors for recipe recommendation.\nFurthermore, choosing an appropriate webscraper was also important for this task. We used BeautifulSoup which is a Python library that is specifically designed for parsing HTML and XML documents, making it ideal for web scraping. Additionally, it provides simple methods and code for navigating, searching, and modifying a parse tree. BeautifulSoup also allows for direct scraping within a notebook rather than needing to define separate Python file, like with Proxy. Working within a notebook simplifies debugging and makes it easier to run the code.\nThis is a simple overview of how the web scraper function scrape_allREcipes_cuisines() scrapes the page: - Step 1: Scrape through the main cuisine page and make a dataframe of all of the cuisines and their URLs. - Step 2: Loop through the list and scrape the cuisine URLs for recipe URLs and make a new dataframe with the cuisine type, recipe name, and recipe page URL. - Step 3: Loop through the new list and scrape the recipe pages for the ingredient lists and add them to the list defined in the previous step.\n\n\ndef scrape_allRecipes_cuisines():\n  \"\"\"\n  Scrapes recipe data from Allrecipes.com based on different cuisines.\n\n  Returns:\n  pandas.DataFrame: DataFrame containing the scraped recipe information including Name, URL, Cuisine, and Ingredients.\n  \"\"\"\n\n  # URL of the page listing all cuisines on Allrecipes.com\n  url = 'https://www.allrecipes.com/cuisine-a-z-6740455'\n  result = requests.get(url)\n  doc = BeautifulSoup(result.text, \"html.parser\")\n  cuisines = doc.select('ul.loc.mntl-link-list a')\n\n  # Send a GET request to the page listing all cuisines\n  cuisine_dict = {}\n  for link in cuisines:\n      cuisine = link.get_text(strip=True)\n      url = link['href']\n      cuisine_dict[cuisine] = url\n\n  # Parse the HTML content of the page\n  df = pd.DataFrame(list(cuisine_dict.items()), columns=['Cuisine', 'URL'])\n\n  # Create an empty list to store recipe information\n  recipes_data = []\n\n  # Iterate over rows in the cuisine DataFrame\n  for index, row in df.iterrows():\n      cuisine_url = row['URL']\n      cuisine = df['Cuisine'][index]\n      result = requests.get(cuisine_url)\n      doc = BeautifulSoup(result.text, 'html.parser')\n\n      # {'class': 'comp mntl-card-list-items mntl-document-card mntl-card card--image-top card card--no-image'}\n      # Extract information for each recipe\n      recipe_info1 = doc.find_all('a', {'class': 'comp mntl-card-list-items mntl-document-card mntl-card card card--no-image'})\n      recipe_info2 = doc.find_all('a', {'class': 'comp mntl-card-list-items mntl-document-card mntl-card card--image-top card card--no-image'})\n      recipe_info = recipe_info1 + recipe_info2\n\n      # Iterate over each recipe and extract relevant information\n      for recipe_card in recipe_info:\n          name = recipe_card.find('span', {'class': 'card__title-text'}).text.strip()\n          url = recipe_card['href']\n\n          # Extract information from individual recipe URLs\n          if not pd.isna(url):\n              result2 = requests.get(url)\n              doc2 = BeautifulSoup(result2.text, 'html.parser')\n\n              # Create a list to store ingredients\n              ingredients_list = []\n              ingredients_container = doc2.find('div', {'class': 'mntl-lrs-ingredients'})\n\n              # Check if the container is found\n              if ingredients_container:\n                  # Find the list of ingredients\n                  ingredients_list_element = ingredients_container.find('ul', {'class': 'mntl-structured-ingredients__list'})\n\n                  # Check if the list of ingredients is found\n                  if ingredients_list_element:\n                      # Extract and append each ingredient to the list\n                      for ingredient_item in ingredients_list_element.find_all('li', {'class': 'mntl-structured-ingredients__list-item'}):\n                          ingredient = ingredient_item.find('span', {'data-ingredient-name': 'true'})\n                          quantity = ingredient_item.find('span', {'data-ingredient-quantity': 'true'})\n                          unit = ingredient_item.find('span', {'data-ingredient-unit': 'true'})\n\n                          if ingredient and quantity and unit:\n                              ingredient_text = f\"{quantity.text.strip()} {unit.text.strip()} {ingredient.text.strip()}\"\n                              ingredients_list.append(ingredient_text)\n\n              # Append recipe information to the list\n              recipes_data.append({\n                  'Name': name,\n                  'URL': url,\n                  'Cuisine': cuisine,\n                  'Ingredients': ingredients_list,\n              })\n\n  # Create a DataFrame from the list of recipes\n  recipes_df = pd.DataFrame(recipes_data)\n  return recipes_df\n\n\n\nThe function scrape_allRecipes_cuisines() is designed to gather recipe data from Allrecipes.com cuisine page. It operates by first accessing a URL listing various cuisines on the website. It then extracts the names and corresponding URLs of these cuisines. Subsequently, for each cuisine, the function accesses its specific URL to retrieve recipe information. It identifies recipe details such as name and URL by parsing the HTML content. Upon obtaining a recipe’s URL, it accesses the individual recipe page to extract the ingredients list. The function then compiles all gathered data into a structured format, returning a DataFrame containing recipe names, URLs, associated cuisines, and ingredients lists.\nIt moves through the pages as shown in the order bellow:\n\nAll Cuisines Page:\n\n\n\n\n\n\n\nSpecific Cuisine Page\n\n\n\n\n\n\n\nRecipe Page:\n\n\n\n\n\n\nBy running this function we produce a dataset like this:\n\n\n\n  \n    \n\n\n\n\n\n\nName\nURL\nCuisine\nIngredients\n\n\n\n\n0\nBest Vinegar Coleslaw\nhttps://www.allrecipes.com/recipe/59318/amish-...\nAmish and Mennonite\n['1 large head cabbage, cored and finely shred...\n\n\n1\nPennsylvania Dutch Pickled Beets and Eggs\nhttps://www.allrecipes.com/recipe/13743/pennsy...\nAmish and Mennonite\n['8 large eggs', '2 (15 ounce) cans whole pick...\n\n\n2\nAmish Macaroni Salad\nhttps://www.allrecipes.com/recipe/74915/amish-...\nAmish and Mennonite\n['2 cups uncooked elbow macaroni', '3 large ha...\n\n\n3\nAmish Friendship Bread Starter\nhttps://www.allrecipes.com/recipe/7063/amish-f...\nAmish and Mennonite\n['1 (.25 ounce) package active dry yeast', '¼ ...\n\n\n4\nMy Amish Friend's Caramel Corn\nhttps://www.allrecipes.com/recipe/74950/my-ami...\nAmish and Mennonite\n['7 quarts plain popped popcorn', '2 cups dry ...\n\n\n...\n...\n...\n...\n...\n\n\n2316\nVietnamese Grilled Pork Skewers\nhttps://www.allrecipes.com/recipe/261122/vietn...\nVietnamese\n['1 pound pork belly, cubed', '1 fresh red ch...\n\n\n2317\nGoi Ga (Vietnamese Chicken and Cabbage Salad)\nhttps://www.allrecipes.com/recipe/271155/goi-g...\nVietnamese\n['4 skinless cooked chicken breasts, shredded...\n\n\n2318\nVietnamese Fresh Spring Rolls\nhttps://www.allrecipes.com/recipe/24239/vietna...\nVietnamese\n['2 ounces rice vermicelli', '8 rice wrappers...\n\n\n2319\nPho (Vietnamese Noodle Soup)\nhttps://www.allrecipes.com/recipe/228443/authe...\nVietnamese\n['4 pounds beef soup bones (shank and knee)', ...\n\n\n2320\nVietnamese Grilled Lemongrass Chicken\nhttps://www.allrecipes.com/recipe/241607/vietn...\nVietnamese\n['2 tablespoons canola oil', '2 tablespoons fi...\n\n\n\n\n\n2321 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nUnfortunately the ingrediants lists are not organized in a good way, so we need to isolate key ingrediants that are common in many recipes. For the machine learning model we are trying to concentrate on produce, so this list will contain more produce than anything else. Additionally, to simplify the machine learning model we will not be including the measurements of the ingrediants because it is a lot harder to train a model to also recognize how much of an ingredient is in an image.\nFirst, we need to remove any punctuation from the string and then seacrch for the key words. To do all of this we will be using the functions remove_punctuation(text) and find_key_ingredients(df, key_ingredients). find_key_ingredients compares the each item from the key_ingredients list to the string to find if each word is in the string. For some of the ingredients there can be errors, such as ‘corn starch’ gets recognized as only ‘corn,’ so some if statements are implemented to try and prevent this mistake.\n\n\ndef remove_punctuation(text):\n  '''\n  - Removes the punctuation from the string of recipe ingredients\n  - Input: string of ingredients\n  - Output: the cleaned string\n  '''\n  translator = str.maketrans('', '', string.punctuation)\n  return text.translate(translator)\n\ndef find_key_ingredients(df, key_ingredients):\n  '''\n  - Compares the lists of ingredients and creates a new column in the dataframe\n    with a list of key ingredients for each recipe\n  - Input: dataframe of recipes, list of key ingredients\n  - Output: dataframe with new column of key ingredients\n  '''\n\n  # Function to check if any key ingredient is found in a list of ingredients\n  def find_word_in_string(text):\n    '''\n    - Finds key ingredients in ingredient string\n    - Input: text from ingredient column of df\n    - Output: List of key ingredients found in ingredient column\n    '''\n    R = []\n    for i in key_ingredients:\n      if i in text:\n        # a lot of recipes use corn starch and corn flour\n        if i == 'corn' and ('corn starch' in text or 'corn flour' in text):\n          break;\n        R.append(i)\n        # Sometimes only need one egg\n        if i == 'eggs' and 'egg' in text and 'eggs' not in R:\n          R.append(i)\n    return R\n\n  # Remove punctuation\n  df['Ingredients'] = df['Ingredients'].apply(remove_punctuation)\n\n  # Find key ingredients in each row\n  df['key_ingredients'] = df['Ingredients'].apply(find_word_in_string)\n  return df\n\n\n\nAfter running these function on the recipe dataframe we get a new dataframe with a column for key ingredients:\n\ncleaned_df\n\n\n  \n    \n\n\n\n\n\n\nName\nURL\nCuisine\nIngredients\nkey_ingredients\n\n\n\n\n0\nBest Vinegar Coleslaw\nhttps://www.allrecipes.com/recipe/59318/amish-...\nAmish and Mennonite\n1 large head cabbage cored and finely shredded...\n['cabbage', 'celery', 'onion', 'sugar']\n\n\n1\nPennsylvania Dutch Pickled Beets and Eggs\nhttps://www.allrecipes.com/recipe/13743/pennsy...\nAmish and Mennonite\n8 large eggs 2 15 ounce cans whole pickled bee...\n['onion', 'beet', 'eggs', 'sugar']\n\n\n2\nAmish Macaroni Salad\nhttps://www.allrecipes.com/recipe/74915/amish-...\nAmish and Mennonite\n2 cups uncooked elbow macaroni 3 large hardcoo...\n['celery', 'onion', 'bell pepper', 'eggs', 'su...\n\n\n3\nAmish Friendship Bread Starter\nhttps://www.allrecipes.com/recipe/7063/amish-f...\nAmish and Mennonite\n1 25 ounce package active dry yeast ¼ cup warm...\n['milk', 'flour', 'sugar']\n\n\n4\nMy Amish Friend's Caramel Corn\nhttps://www.allrecipes.com/recipe/74950/my-ami...\nAmish and Mennonite\n7 quarts plain popped popcorn 2 cups dry roast...\n['corn', 'sugar']\n\n\n...\n...\n...\n...\n...\n...\n\n\n2316\nVietnamese Grilled Pork Skewers\nhttps://www.allrecipes.com/recipe/261122/vietn...\nVietnamese\n1 pound pork belly cubed 1 fresh red chile pe...\n['garlic', 'pork', 'sugar']\n\n\n2317\nGoi Ga (Vietnamese Chicken and Cabbage Salad)\nhttps://www.allrecipes.com/recipe/271155/goi-g...\nVietnamese\n4 skinless cooked chicken breasts shredded 1 ...\n['cabbage', 'cilantro', 'onion', 'carrot', 'ch...\n\n\n2318\nVietnamese Fresh Spring Rolls\nhttps://www.allrecipes.com/recipe/24239/vietna...\nVietnamese\n2 ounces rice vermicelli 8 rice wrappers 85 i...\n['cilantro', 'lettuce', 'rice']\n\n\n2319\nPho (Vietnamese Noodle Soup)\nhttps://www.allrecipes.com/recipe/228443/authe...\nVietnamese\n4 pounds beef soup bones shank and knee 1 medi...\n['cilantro', 'garlic', 'onion', 'rice', 'beef']\n\n\n2320\nVietnamese Grilled Lemongrass Chicken\nhttps://www.allrecipes.com/recipe/241607/vietn...\nVietnamese\n2 tablespoons canola oil 2 tablespoons finely ...\n['garlic', 'chicken', 'sugar']\n\n\n\n\n\n2321 rows × 5 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nThis is our final dataset that we will be using to compare with the results of the model and to recommend recipes. The functions used in this section can be found in recipe_manip.py."
  },
  {
    "objectID": "posts/Project/index.html#ii.-images-for-ingredients",
    "href": "posts/Project/index.html#ii.-images-for-ingredients",
    "title": "",
    "section": "II. Images for Ingredients",
    "text": "II. Images for Ingredients\nThe web scraping model I created to gather images of ingredients from Google Images uses both BeautifulSoup and Selenium, which are used for parsing HTML and automating web interactions. By using Selenium, and web driver like geckodriver for FireFox of chromedriver for Google Chrome, the script is able to load pages by scrolling to the end, ensuring that all images are loaded before the HTML is parsed by BeautifulSoup. Additionally, Selenium allows you to interact with elements on the page, which I used to clicking on each thumbnail to access the URLs of the corresponding images.\nThe main function initializes a web driver object and prompts the user for inputs including a list of keywords, the desired number of images to download for each keyword, and the name of the file to save the downloaded images. It also creates a new file if the specified one does not already exist in the directory. Subsequently, for each keyword provided by the user, the script generates a Google Images URL and calls the scrape_images function to extract the URLs of the images before proceeding to download them using the download_image function.\nThe scrape_images function is responsible for extracting image URLs from the provided Google Images URL. It utilizes Selenium to scroll through the webpage, ensuring all images are loaded, and then parses the HTML using BeautifulSoup. Thumbnails of images are identified and clicked to access the full-sized image URLs. These URLs are collected and returned as a set.\nThe download_image function handles the downloading and saving of images. It makes a request to the provided image URL and retrieves the image content. The content is then processed, ensuring it is in a usable format. The image is then saved as a JPEG file in the specified download path with the provided file name. This ensures that the images are successfully downloaded and stored in files designated to the ingredient classes for classification with the model later on.\nTo be able to run the code in the terminal or in a notebook, a web driver exicutable file must be stored within the same directory as the py file. This is also where the ingredient files will be downloaded.\n\n\ndef main():\n    wd = webdriver.Chrome()\n    data = input('Enter your search keyword: ')\n    data_list = data.split(\", \")\n    \n    num_images = int(input('Enter the number of images you want: '))\n    file_name = input('Enter file location: ')\n    \n    if not os.path.exists(file_name):\n        os.mkdir(file_name)\n    \n    count = 0\n    for i in data_list:\n        search_url = Google_Image + 'q=' + i #'q=' because its a query\n        print(search_url)\n        urls = scrape_images(search_url, wd, 1, num_images)\n        print(f\"Found {len(urls)}\")\n    \n        for j, url in enumerate(urls):\n            download_image(file_name + \"/\", url, str(count) + \".jpg\")\n            count += 1\n        \n        print(\"Success!\")\n            \n    print(\"Complete!\")\n    wd.quit()\n\ndef scrape_images(initial_url, wd, delay, max_images):\n    url = initial_url\n    wd.get(url)\n    \n    # scroll height of webpage\n    last_height = wd.execute_script(\"return document.body.scrollHeight\") \n    while True:\n        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")    # The driver scrolls down the webpage here.\n        time.sleep(8)\n        new_height = wd.execute_script(\"return document.body.scrollHeight\")\n        if new_height == last_height:   # Breaks here if the driver reached the bottom of the webpage (when it cannot scroll down anymore).\n            break\n        last_height = new_height        # If the driver failed to scroll to the bottom of the webpage, the current scroll height is recorded. Following, it is compared to the scroll height in the next iteration of the loop to decide if the driver reached the bottom of the webpage or not.\n    wd.execute_script(\"window.scrollTo(0, 0);\")\n    \n    page_html = wd.page_source  # The HTML of the webpage is obtained by the driver.\n    pageSoup = bs4.BeautifulSoup(page_html, 'html.parser')  # Using Beautiful Soup to parse the HTML of the webpage.\n    thumbnails = wd.find_elements(By.CLASS_NAME, \"Q4LuWd\")  # This class name is obtained from the thumbnail of each image in Google Image Search.\n    time.sleep(3)\n    \n    len_thumbnails = len(thumbnails)    # The number of images found is recorded and printed.\n    print(\"Found %s image candidates\"%(len_thumbnails))\n    \n    image_urls = set()\n        \n    for img in thumbnails[len(image_urls): max_images]: # Loops through the images of the webpage to obtain and store the number of image URLs requested.\n        try: \n            img.click()\n            time.sleep(0.5)\n\n        except:\n            continue\n\n        images = wd.find_elements(By.CLASS_NAME, \"iPVvYb\") # This class name is obtained from the actual image and not its thumbnail in Google Image Search.\n        for image in images:\n            if image.get_attribute('src') in image_urls: # Prevents an image URL from being stored if it is already there.\n                max_images += 1 # Accounts for a duplicate image URL by ensuring that the function still returns the number of image URLs requested.\n                break\n\n            if image.get_attribute('src') and 'http' in image.get_attribute('src'): # Checks if an image URL is usable.\n                    image_urls.add(image.get_attribute('src')) # Stores an image URL if it is usable.\n                    #print(f\"Found {len(image_urls)}\")\n        \n    return image_urls # Returns all of the usable image URLs.\n\ndef download_image(download_path, url, file_name):\n\n    try:\n        image_content = requests.get(url).content\n    except requests.exceptions.RequestException as e:\n        print(f\"Failed to download image from {url} - {e}\")\n        return  # Return without attempting to process the image further\n\n    try:\n        image_file = io.BytesIO(image_content)\n        image = Image.open(image_file)\n    except (IOError, OSError) as e:\n        print(f\"Failed to open image file - {e}\")\n        return  # Return without attempting to process the image further\n\n    # Convert image to RGB mode\n    image = image.convert('RGB')\n\n    file_path = os.path.join(download_path, file_name)\n    try:\n        with open(file_path, \"wb\") as f:\n            image.save(f, \"JPEG\")\n        print(f\"Downloaded: {file_name}\")\n    except Exception as e:\n        print(f\"Failed to save image - {e}\")\n\n\n\nUnfortunately, despite Selenium allowing us to scroll to the bottom of the page, the Google Images page only loads around 400 images for a given search word. Therefore, to get a decent amount of images for training we need to input multiple key words for the same ingredient.\nFor example, for the ingredient onion, there are many different types of onions so we can make a list of these and input them when prompted: yellow onion, white onion, sweet onion, red onion, shallots, onion raw. Additionally, for certain produce or ingredients often when you look them up they come up cooked, to prevent this with many of the broader ingredients you should add ‘raw’ to the key frase.\n\n\n\n\n\nAlternatively, name is defined in the py file so you can run this in the terminal by calling “python image_scraper.py”\nBelow is what shows up during each step of the web scraping of the image URLs: 1. Scrolling to the end of the page and parsing HTML:\n\n\n\n\n\n\nClicking on each thumbnail and parsing HTML for image URLs:\n\n\n\n\n\n\nIn total, we were able to web scrape around 1000 images for 20 ingredients: Tomatoes, Potatoes, Zucchini, Broccoli, Atrichoke, Beans, Onion, Asparagus, Green Onion, Cabbage, Lentils, Garlic, Bell Peppers, Chili Peppers, Corn, Carrots, Mushrooms, Cherries, Apples, Rice.\nWe were not able to scrape more than this due to a few different issues. In order to fully load each page we had to allow time for it to pause between clicking and scrolling, so it took a long time to download a good amount of images for each ingredient. Additionally, the webscraper would often stop scraping randomly, most likely due to connection issues with the web driver and sometimes it would find zero items on the page."
  },
  {
    "objectID": "posts/Project/index.html#i.-implementation-of-the-model",
    "href": "posts/Project/index.html#i.-implementation-of-the-model",
    "title": "",
    "section": "I. Implementation of the Model",
    "text": "I. Implementation of the Model\nThe machine learning aspect of our project entailed the implementation of a CNN in pytorch. After researching many different models, and weighing the pros and cons of each architecture, we settled on using a Resnet to carry out image classification. While we had the option to utilize a pretrained model taken from the torchvision database, we felt that it would be better to implement and train our own rendition of the network. This allowed for minute adjustments to layer functions, such as changing the filter size in a convolution, or removing negligible operations to counteract overfitting. The code itself is quite complex. Let’s walk through the actual Resnet class:\nclass ResNet(nn.Module):\n    def __init__(self, block, num_block, num_classes=100,num_channel=3):\n        \"\"\"Initializes the ResNet model.\n\n        Args:\n            block: Basic block or bottleneck block.\n            num_block (list): List containing the number of blocks for each layer.\n            num_classes (int): Number of output classes.\n            num_channel (int): Number of input channels, default is 3.\n        \"\"\"\n        super().__init__()\n        self.in_channels = DIM\n        self.conv1 = nn.Sequential(\n          nn.Conv2d(num_channel, DIM, kernel_size=3, padding=1, bias=False),\n          nn.BatchNorm2d(DIM),\n          nn.ReLU(inplace=True))\n        self.conv2_x = self._make_layer(block, DIM, num_block[0], 1)\n        self.conv3_x = self._make_layer(block, DIM*2, num_block[1], 2)\n        self.conv4_x = self._make_layer(block, DIM*4, num_block[2], 2)\n        self.conv5_x = self._make_layer(block, DIM*8, num_block[3], 2)\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(DIM*8 * block.expansion, num_classes)\n        self.sigmoid = nn.Sigmoid()\nThis is just a snippet of the code that goes into creating a Resnet. The model starts out with a simple 2D convolution followed by a batch normalization and ReLU activation. Here, num_channels refers to the number of color channels in the input image (three for all our data) and DIM the number of output channels after the convolution is processed (DIM is variable, but we find a value of 64 works best). Then, we proceed to the meat of the architecture, utilizing a complex function called _make_layer() (definition not pictured) that is used to create a model “layer” consisting of a variable number of convolutions, batch normalizations, max pools, and activations, all depending on the type and number of “blocks” specified. Currently, we don’t need to worry about the explicit definition of a block – it’s quite involved, so it suffices to understand that they’re used as the framework of a layer in the model. After this constructor, model operations are put into motion using a feed-forward function:\ndef forward(self, x):\n  \"\"\"Forward pass of the ResNet model.\n\n    Args:\n      x (torch.Tensor): Input tensor.\n\n    Returns:\n      torch.Tensor: Output tensor.\n  \"\"\"\n  output = self.conv1(x)\n  output = self.conv2_x(output)\n  output = self.conv3_x(output)\n  output = self.conv4_x(output)\n  output = self.conv5_x(output)\n  output = self.avg_pool(output)\n  output = output.view(output.size(0), -1)\n  output = self.fc(output)\n\n  return self.sigmoid(output)\nData is fed through each layer (as outlined above) and its class probability distribution is returned using the sigmoid activation function. We use a Resnet18:\ndef resnet18(num_classes,num_channel):\n  \"\"\" returns a ResNet 18 object\n  \"\"\"\n  return ResNet(BasicBlock, [2, 2, 2, 2],num_classes,num_channel)\nas our main model, but by adjusting the block type and number, we can also possibly return a Resnet34, or Resnet50, all the way up to a Resnet152 (here, the number associated with a Resnet can be thought of as the number of layers in the model). This concludes the construction of our model – now we’re ready to create our dataset!"
  },
  {
    "objectID": "posts/Project/index.html#ii-dataset-creation",
    "href": "posts/Project/index.html#ii-dataset-creation",
    "title": "",
    "section": "II: Dataset Creation",
    "text": "II: Dataset Creation\nIn order to train, we must first create a dataset from which our model can learn from. Depending on how you store your training images, you can write a funciton to retrieve data in the form of numpy arrays. Our data was stored in a directory containing subdirectories of a given class to partition the set of different tyes of ingredients. To create numpy arrays of the images and their corresponding labels, we utilized the following:\ndef load_images_and_labels(parent_folder, target_size=(150,150)):\n  \"\"\"Returns image data in form of a numpy array\n    \n  args:\n    parent_folder: folder containing classes of images to move through sequentially\n    target_size: size of resulting images in image array\n  \"\"\"\n  images = []\n  labels = []\n  class_mapping = {\"beans\" : 0, \"bell_pepper\" : 1, \"potato\" : 2, \"tomato\" : 3}\n\n  for class_folder in os.listdir(parent_folder):\n    class_path = os.path.join(parent_folder, class_folder)\n    if os.path.isdir(class_path) and class_folder in class_mapping:\n      label = class_mapping[class_folder]\n      for image_file in os.listdir(class_path):\n        image_path = os.path.join(class_path, image_file)\n        try:\n          image = PIL.Image.open(image_path)\n          if image is not None:\n            if image.mode != 'RGB':\n              image = image.convert('RGB')\n            image = image.resize(target_size)\n            images.append(image)\n            labels.append(label)\n            except Exception as e:\n              print(f'Error reading image {image_path}: {e}')\n\n  return np.array(images), np.array(labels)\nThis function iterates through each file and adds the images to python using PIL, then appends each image to a list called images, as well as its label (corresponding to the stated mapping) to a list called labels. At the end, a numpy array version of both these lists is returned. Using these, we can then create a pytorch dataset: ```python class Ingredients(Dataset): “““Class creating pytorch dataset from images and their labels.\nArgs:\n    images (list): List of images.\n    labels (list): List of corresponding labels.\n    transform (callable, optional): Augmentation of the images.\n\nAttributes:\n    images (list): List of images.\n    labels (list): List of corresponding labels.\n    transform (callable, optional): Augmentation of the images.\n\nMethods:\n    __len__(self): Returns the number of samples in the dataset.\n    __getitem__(self, idx): Retrieves the item at the given index.\n\n\"\"\"\ndef __init__(self, images, labels, transform=None):\n    \"\"\"Initializes the Ingredients dataset.\n\n    Args:\n        images (list): List of images.\n        labels (list): List of corresponding labels.\n        transform (callable, optional): Augmentation of the images.\n    \"\"\"\n    self.images = images\n    self.labels = labels\n    self.transform = transform\n\ndef __len__(self):\n    \"\"\"Returns the number of samples in the dataset.\n\n    Returns:\n        int: Number of samples in the dataset.\n    \"\"\"\n    return len(self.images)\n\ndef __getitem__(self, idx):\n    \"\"\"Retrieves the item at the given index.\n\n    Args:\n        idx (int): Index of the item to retrieve.\n\n    Returns:\n        tuple: A tuple containing the image and its corresponding label.\n    \"\"\"\n    image = self.images[idx]\n    image = PIL.Image.fromarray(image)\n    label = self.labels[idx]\n    if self.transform:\n        image = self.transform(image)\n    return image, label\nEvery pytorch dataset is a class inheriting from the dataset class (which you'll have to import through ```torchvision```) and needs to define the following three functions: a constructor (```__init__```), ```__len__()```, and ```__getitem__()```. The constructor records the data, ```__len__``` returns the length of the dataset, and ```__getitem__``` returns an image-label combo at an index of the set. Once we've created our dataset, there are multiple steps we would like to take to process our data. For one, we'd like to change the pixel values of our images to live in the range [-1,1] from the standard [0,255], done through the following:\n```python\ndef preprocess(images_arr):\n  \"\"\"Preprocess an array of images.\n\n    Args:\n        images_arr (numpy.ndarray): Array of images to be preprocessed.\n\n    Returns:\n        numpy.ndarray: Preprocessed array of images, pixel values between -1 and 1.\n  \"\"\"\n  images_arr = images_arr.astype(np.float32)\n  images_arr = images_arr/255.0\n  images_arr = (images_arr * 2.0) - 1.0\n\n  return images_arr\nIn this function, we convert each image to a 32-bit float and then dilate the channels to fit within the desired interval. Additionally, we have written a function to find the mean and standard deviation across each channel:\ndef normalization_vals(images_arr):\n  \"\"\"Calculate mean and standard deviation values for normalizing an array of images.\n\n    Args:\n        images_arr (numpy.ndarray): Array of images for which to calculate normalization values.\n\n    Returns:\n        tuple: A tuple containing the mean and standard deviation values for each channel.\n  \"\"\"\n  means = np.mean(images_arr, axis=(0,1,2))\n  stds = np.std(images_arr, axis=(0,1,2))\n  means = tuple(means)\n  stds = tuple(stds)\n  return means, stds\nThese values will be used to normalize the pixel values of our training set. With all this, we are now ready to begin training."
  },
  {
    "objectID": "posts/Project/index.html#iii.-training-the-model",
    "href": "posts/Project/index.html#iii.-training-the-model",
    "title": "",
    "section": "III. Training the Model",
    "text": "III. Training the Model\nThis part is relatively simple. The whole process is contained in one file, main.py, which imports useful functions as defined in other files. Here is main:\nimport torch\nimport torch.optim as optim\n# from torchinfo import summary\n\nfrom logger import Logger\nfrom train import run_epoch\nfrom resnet import resnet14, resnet18, resnet34, resnet50\nfrom data_loader import get_data\nfrom test import run_test\n\ntorch.manual_seed(0)\n\nEPOCHS = 200\nEARLY_STOP = 5\nLR = 0.1\nMOMENTUM = 0.9\nWEIGHT_DECAY = 0.0005 # 0.0005 test\nVAL_SPLIT = 0.1\n\nmodel_save_path = f'./model_logs/Ingredients1'\n\nlog = Logger()\nlog.set_logger(f'{model_save_path}.log')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntrain_loader, val_loader, test_loader, num_classes, num_channels = get_data(VAL_SPLIT)\nmodel = resnet18(num_classes, num_channels)\noptimizer = optim.SGD(model.parameters(), LR, MOMENTUM, weight_decay=WEIGHT_DECAY)\n# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n\nrun_epoch(model, train_loader, val_loader, optimizer, scheduler, EPOCHS, EARLY_STOP, f'{model_save_path}.pth', device, log)\naccuracy = run_test(model, test_loader, device, f\"{model_save_path}.pth\")\nlog.logger.info(\"Accuracy: {:.10f}\".format(accuracy))\nOf note are get_data(), run_epoch(), and run_test(). The former is defined in another file, data_loader.py, and is as follows:\ndef get_data(val_split=0.5):\n    \"\"\"Prepares DataLoader instances for training, validation, and testing splits of an image dataset.\n    \n    Parameters:\n    - val_split (float, optional): Proportion of the training set to use for validation. Default is 0.5.\n\n    Returns:\n    - Tuple[DataLoader, DataLoader, DataLoader, int, int]: A tuple containing DataLoader instances for the\n      training, validation, and test datasets, the number of unique labels in the dataset, and the number 3,\n      whose specific meaning may depend on context (e.g., number of color channels).\n    \"\"\"\n    parent_folder = \"/content/drive/MyDrive/proj_files/ingredients/ingredients\"\n    images, labels = load_images_and_labels(parent_folder)\n    preprocess(images)\n    transform = transforms.Compose([\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomResizedCrop(32),\n        transforms.ToTensor(),\n        transforms.Normalize((0.38046584, 0.10854615, -0.13485776), (0.5249659, 0.59474176, 0.6634378))\n    ])\n\n    dataset = Ingredients(images, labels, transform)\n        \n    batch_size = 64 # change to appropriate value for dataset in use\n\n    dataset_size = len(dataset)\n    indices = list(range(dataset_size))\n    train_test_split = int(np.floor(0.8 * dataset_size))\n    train_val_split = int(np.floor(val_split * dataset_size))\n\n    np.random.shuffle(indices)\n\n    train_idx, test_idx = indices[train_test_split:], indices[:train_test_split]\n    val_idx = train_idx[:train_val_split]\n\n    train_loader = DataLoader(dataset,\n                              batch_size=batch_size,\n                              sampler=SubsetRandomSampler(train_idx),\n                              num_workers=4)\n    \n    test_loader = DataLoader(dataset,\n                             batch_size=batch_size,\n                             sampler=SubsetRandomSampler(test_idx),\n                             num_workers=4)\n\n    val_loader = DataLoader(dataset,\n                            batch_size=batch_size,\n                            sampler=SubsetRandomSampler(val_idx),\n                            num_workers=4)\n    \n    return train_loader, val_loader, test_loader, len(np.unique(labels)), 3\nIt begins by identifying the parent directory in which all our data is located (you will have to change this to depending on where your images are located). Then it utilizes the preprocessing functions we defined above. Next, we define a dataset based on the preprocessed image data, and split this dataset into a training set, validation set, and test set. We then create an iterator of sorts called a DataLoader that allows us to effectively load data in batches to train the model. We create three separate data loaders, one for our training set, one for our valiadation set, and one for our test set. These, in addition to the number of classes and color channels in the data, are returned and further used in our main function. Thus, the data pipeline portion of the process has been completed. Now let’s define the actual training algorithm. The function run_epoch() is as follows:\ndef run_epoch(model:torch.nn.Module, train_loader:torch.utils.data.DataLoader, val_loader:torch.utils.data.DataLoader, optimizer, scheduler, epochs:int, early_stop:int, model_save_path:str, device:torch.device, log, loading=False):\n    \"\"\"Run training and validation for the given number of epochs.\n\n    Args:\n        model (torch.nn.Module): The model.\n        train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n        val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n        optimizer: The optimizer to update the model's parameters.\n        scheduler: Learning rate scheduler.\n        epochs (int): Number of epochs for training.\n        early_stop (int): Number of epochs to wait before early stopping if validation loss doesn't improve.\n        model_save_path (str): Path to save the best model.\n        device (torch.device): Device to be used for training (cuda or cpu).\n        log: Logger object for logging.\n        loading (bool): Flag indicating whether to load a pre-trained model. Default is False.\n\n    Returns:\n        None\n    \"\"\"\n    train_losses = []\n    val_losses = []\n    if loading==True:\n        model.load_state_dict(torch.load(model_save_path))\n        log.logger.info(\"-------------Model Loaded------------\")\n        \n    best_loss=0\n    #curr_early_stop = early_stop\n    model.to(device)\n    for epoch in range(epochs):\n        train_loss = train(model, train_loader, optimizer, device)\n        val_loss = val(model, val_loader, device)\n        scheduler.step()\n       \n        log.logger.info((f\"Epoch: {epoch+1} - loss: {train_loss:.10f} - test_loss: {val_loss:.10f}\"))\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        \n        if epoch == 0:\n            best_loss=val_loss\n        if val_loss&lt;=best_loss:\n            torch.save(model.state_dict(), model_save_path)    \n            best_loss=val_loss\n            log.logger.info(\"-------- Save Best Model! --------\")\nThis function utilizes both our training and validation data loaders. We defined functions for training and validation (train and val, respectively) that implement the classic backpropogation process. Additionally, we include a learning rate scheduler to help boost training results. Logger is used to create files that streamline the access and view of our results. Using validation loss as our metric, we save every model that is an improvement over the current best model. If needed, one could easily implement an early stopping system (in fact, our own early stop code is commented out of the original file on github), but based off our research, which involved looking into the processes outlined in the original Resnet research paper, it seemed best to evaluate the model of a full period of 200 epochs. Lastly, we look at the run_test() function, which is again, relatively simple:\ndef run_test(model, test_loader, device, model_save_path):\n    \"\"\"Run testing on the test data using the trained model.\n\n    Args:\n        model (torch.nn.Module): The trained model.\n        test_loader (torch.utils.data.DataLoader): DataLoader containing the test data.\n        device (torch.device): The device to be used for testing (cuda or cpu).\n        model_save_path (str): Path to the saved model.\n\n    Returns:\n        float: Accuracy achieved by the model on the test data.\n    \"\"\"\n    model.load_state_dict(torch.load(model_save_path))\n    model.to(device)\n    model.eval()\n\n    all_labels = []\n    all_predictions = []\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predictions = torch.max(outputs, 1)\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predictions.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_predictions)\n\n    return accuracy\nHere, we use the test_loader to run through the dataset one more time, evaluating it using our final, fully trained model and calculating accuracy using scikit-learn’s accuray algorithm. We trained our model on a small subset of ingredients, which resulted in an accuracy score of around 70%. This score should improve immensley when a large number and variety of images is used as training data, unfortunately, we were unable to achieve this premier model due to a lack of resources. Our dataset creation function is quite itensive. Thus, when trying to load our set, both google colab and jupyter lab would run out of GPU RAM. Given the proper resources, it would be more than possible to produce a fully trained model on an expansive dataset. Still, we produced a more than adequate network using only a small subset. It’s training results are visualized below:\n\n\n\nimage.png\n\n\nAs we proceed through the cycle of epochs, the loss continues to decrease. There is not much overfitting present, thought the values start to stagnate near the end of the training process. Here, we achieved a 68% accuracy, but tuning hyperparameters further should increase this. Additionally, as mentioned before, a larger, more diverse dataset would do wonders for performance. Unfortunately, that’s just not possible with our currently available resources. Now with this, the training process is done. We proceed to put everything together in the form of a webapp made in Flask."
  },
  {
    "objectID": "posts/Project/index.html#i.-home-page",
    "href": "posts/Project/index.html#i.-home-page",
    "title": "",
    "section": "I. Home Page",
    "text": "I. Home Page\nWhile creating the home page itself simple involved design we also first had to establish the HTML code to set up the Web APP. To start off we created a a base.html and a main.html. Main pourpose of the base.html is to provides a structured layout with a navigation bar and placeholders for dynamic content. The navigation bar ks able to naviage between the three pages”:\n\nHome page: Links to the main page of the application. The url_for(‘main’) function dynamically generates the URL to the view function named main.\nGenerate a Recipe: Links to the page where users can generate recipes. The url_for(‘generate’) generates the URL to the generate view function.\nRecipe List: Links to a page displaying a list of recipes. The url_for(‘receipe_page’) (note the typo in ‘recipe’) generates the URL to the receipe_page view function. !\n\n\n&lt;!doctype html&gt;\n\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n&lt;title&gt;{% block title %}{% endblock %} - Recipe Generator&lt;/title&gt;\n&lt;nav&gt;\n  &lt;h1&gt;Recipe Generator&lt;/h1&gt;\n  &lt;!-- &lt;b&gt;Navigation:&lt;/b&gt; --&gt;\n  &lt;ul&gt;\n    &lt;li&gt;&lt;a href=\"{{ url_for('main') }}\"&gt;Home page&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=\"{{ url_for('generate')}}\"&gt;Generate a Recipe&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;a href=\"{{ url_for('receipe_page')}}\"&gt;Recipe List&lt;/a&gt;&lt;/li&gt;\n    &lt;!-- &lt;li&gt;&lt;a href=\"{{ url_for('recipe_results')}}\"&gt;Recipes&lt;/a&gt;&lt;/li&gt; --&gt;\n  &lt;/ul&gt;\n&lt;/nav&gt;\n&lt;section class=\"content\"&gt;\n  &lt;header&gt;\n    {% block header %}{% endblock %}\n  &lt;/header&gt;\n  {% block content %}{% endblock %}\n&lt;/section&gt;\n\nBelow the main.html sets up the structure of the mian page. we used img src to present three images and imporve the appreace of our model but most importantly we added a button called “Generate a Receipe”. The style of the button is added at style.css. The picture below depics how our home page looks like. As you can see you can navigate to the generate a recipe page both through the button in the middle or through the navigation bar. Below the image we will dive into how we formulated the generate function.\n\n\n {% extends 'base.html' %}\n\n{% block header %}\n  &lt;h1&gt;{% block title %} Eat Your Veggies{% endblock %}&lt;/h1&gt;\n{% endblock %}\n\n{% block content %}\n\n&lt;div class=\"centered-container\"&gt;\n  &lt;h2&gt;Get a List of Receipes Tailored For You and Your Ingredients&lt;/h2&gt;\n\n  &lt;p&gt; Welcome to our recipe generator! Have leftover vegetables? Not sure what to do with them? Do not fret we got you covered! It's simple: upload photos of your available vegetables and other ingredients, select your preferred cuisine type, and voilà! We'll provide you with a curated list of recipes tailored just for you. Let's make cooking at home easy, fun, and absolutely delightful. Start your culinary adventure with us today!&lt;/p&gt;\n\n  &lt;p&gt; Simply click on the generate button below or explore our recipes in the tab above&lt;/p&gt;\n&lt;a href=\"/generate\" class=\"myButton\"&gt;Generate a Recipe&lt;/a&gt;\n&lt;p&gt;The best seasoning is hunger!&lt;/p&gt;\n&lt;/div&gt;\n\n&lt;div class=\"image-container\"&gt;\n&lt;img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ-IZ1xJCPpfvhKZeLbKinyBX6Zvilg_RULXA&usqp=CAU.jpg\" alt=\"Image1\"&gt;\n&lt;img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQqE81NwU51B4OED72TNNtVQHk1QJN2GZNZSw&usqp=CAU.jpg\" alt=\"Image 2\"&gt;\n&lt;img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSq_n1J37kfrfyPyt7lLnjGB_ZaAFVa95Om_A&usqp=CAU.jpg\" alt=\"Image 3\"&gt;\n&lt;/div&gt;\n\n{% endblock %}"
  },
  {
    "objectID": "posts/Project/index.html#ii.-list-of-recipes",
    "href": "posts/Project/index.html#ii.-list-of-recipes",
    "title": "",
    "section": "II. List of recipes",
    "text": "II. List of recipes\nAnother component of the website is the recipe list page. On this page, participants are simply able to explore the different cuisine and the recipes within them regardless of the ingredients they have.\nThe code from the app.py file below will present how the receipe list page work and how we navigate to the correct url of the recipe\nThe def filter_ recipe takes in the user selected cuisine and filter through the data farme to find the matching receipes. Lets break down the decorator and the function.\nfirst the @app.route(‘/filter-recipes’, methods=[‘POST’]) decorator tells Flask to use the filter_recipes function as the handler for requests to the URL /filter-recipes when the HTTP method is POST.\nWhen the create the function which runs when the POST request is made for filter-recipes. The “cuisine = request.form.get(‘cuisine’)” code extracts the value of cuisine from the form data submitted with the POST request. If cuisine is not present in the form data, None is returned instead. The next line “ingredients = None: Initializes ingredients with None” is added so when we are generating receipes we can also incldue ingredients howevere in this case users only select a cusine.\nThe function then calls the function gen_recipe with cuisine and ingredients as arguments. This function isreturns a collection of recommended recipes based on the provided cuisine (and potentially ingredients, if implemented).\nWe then convert the recommended_recipes into a list of dictionaries. Each dictionary represents a recipe, and the orient=‘records’ parameter means each row in the DataFrame is converted to a dictionary, with column names as keys.\nLastly we render the receipe result page.\n\n@app.route('/filter-recipes', methods=['POST'])\ndef filter_recipes():\n    \"\"\"Filter recipes based on selected cuisine and ingredients.\n\n    Retrieves the selected cuisine and ingredients from the request form, then generates\n    recommended recipes using the 'gen_recipe' function. Converts the recommended recipes\n    into a dictionary format and renders the 'recipe_results.html' template, passing the\n    recipes data.\n\n    Returns:\n        Response: Rendered template 'recipe_results.html' with recipes data.\n    \"\"\"\n    cuisine = request.form.get('cuisine')\n    ingredients = None\n\n    recommended_recipes = gen_recipe(cuisine, ingredients)\n    recipes_data = recommended_recipes.to_dict(orient='records')\n\n    # Render the recipe_results.html template, passing the recipes data\n    return render_template('recipe_results.html', recipes=recipes_data)\n\n\nNow that we are at the recipe results page lets examine the recipe_resulst function:\nWhen a user accesses the /recipe_results/ URL, the application can respond to both GET and POST requests. For GET requests, it’s expected that the user has passed information through the URL’s query parameters, specifying both a type of cuisine and ingredients they’re interested in. The application checks if both pieces of information are provided; if not, it displays a message asking for both the cuisine and ingredients. If the information is present, it uses a function gen_recipe from earlier to fetch recommended recipes matching the specified criteria, converts this data into a format that can be easily used in the HTML template, and then displays it to the user on the recipe_results.html page. For POST requests, which are not explicitly handled with form data in this code, it simply displays a message encouraging the user to select a cuisine to view recipes, suggesting that the main interaction mode expected is through GET requests\n\n@app.route('/recipe_results/', methods=['GET', 'POST'])\ndef recipe_results():\n    \"\"\"Display recipe results based on selected cuisine and ingredients.\n\n    If the request method is GET, retrieves the selected cuisine and ingredients from the request arguments,\n    generates recommended recipes using the 'gen_recipe' function, converts the recommended recipes into a\n    dictionary format, and renders the 'recipe_results.html' template, passing the recipes data. If either\n    cuisine or ingredients is not provided, displays an error message.\n\n    If the request method is not GET, displays a message prompting the user to select a cuisine to view recipes.\n\n    Returns:\n        Response: Rendered template 'recipe_results.html' with recipes data or an error message.\n    \"\"\"\n    if request.method == 'GET':\n        # fetch cuisine and ingredients passed form generate\n        cuisine = request.args.get('cuisine')\n        ingredients = request.args.get('ingredients')\n        if cuisine is None or ingredients is None:\n            message = \"Provide both a cuisine and ingredients.\"\n            return render_template('recipe_results.html', message=message)\n        # print(cuisine + ' ' + ingredients)\n        recommended_recipes = gen_recipe(cuisine, ingredients)\n        # print(recommended_recipes)\n        recipes_data = recommended_recipes.to_dict(orient='records')\n        print(recipes_data)  # Debugging line to check data\n        return render_template('recipe_results.html', recipes=recipes_data)\n    else:\n        message = \"Please select a cuisine to view recipes.\"\n        return render_template('recipe_results.html', message=message)\n\nif __name__ == '__main__':\n    app.run(debug=True)\n\nThe code below is from the receipe_results.html, it dynamically display a list of recipes on a webpage. If recipes are available, each one is presented with its name (as a clickable link to the recipe’s page) and its key ingredients. If no recipes are available, a predefined message is displayed instead.\n\n\nLets break down how it works and what each part does:\nThe h1 simply displays the header of the page.\nThe div id=“recipes”&gt;…&lt;/div” encloses the entire section of the code that deals with displaying the recipes or a message in case there are no recipes to display. It is important to note that the id=“recipes” attribute is used to uniquely identify this division in the document, which can be useful for styling or scripting purposes.\nNext up % if recipes % is a Jinja2 conditional statement that checks if there is a recipes variable available and it has a truthy value (e.g., a non-empty list). If recipes is truthy, the block of code inside this condition is executed.\nIf the recipes variable exists and contains items than this lines % for recipe in recipes % , loop iterates over each item in recipes, note each item in the loop is referred to as recipe.\nInside the loop, a div class=“recipe”&gt;…&lt;/div is defined for each recipe. Within this divsion we displays the recipe’s name and links to the recipe’s URL. The recipe.Name and recipe.URL are placeholders that get replaced with the actual name and URL of the recipe, respectively, as provided by the recipes variable. A paragraph (p) with the class “key-ingredients” that displays the key ingredients of the recipe. The recipe.key_ingredients placeholder is replaced with the actual key ingredients data. Lastly , else is executed if recipes is falsy (e.g., an empty list or None). It renders a paragraph (p) displaying a message, which is provided by the message variable. To end the if-else conditional look we use endif\n\n\n\n\n\nHere is what the recipe list page looks like. Once users submit preferences they will be redirected to the same page as the recipe page from above. Again when clicking a recipe they will be redirected to the recipe website.\n\n\n\nfilter.png\n\n\nFinally, the recommended recipes are rendered on the recipe result page for the user to explore. This page allows users to explore the recipes from different cuisines without needing to upload any images, in the case that they do not have any ingredients."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]