[
  {
    "objectID": "TMDB_scraper/hw2blog.html",
    "href": "TMDB_scraper/hw2blog.html",
    "title": "HW 2, Web Scarpping",
    "section": "",
    "text": "In this blog we will go through on how to scrap a website. In this example we will formulate a list of all the actors who acted in the first Hunger Game movie and create a data fram with all the other movies in which they acted in to help recomand new movies to others.\n\n\nLocate the Url that you would be scrapping information from. For this blog we will use this website https://www.themoviedb.org/ and will scarpe a list of actors from the Hunger Game to create recomandtion system that gives users a list of the other movies the actors played in. \nWe First want to make sure that we are in the right enviroment and create a TMDB_scraper folder with all the files we will need by running the following lines in your terminal.\n\nPIC16B-24W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\n\n\n\n\n#### Part 1 We create a new file inside the spider directory and named it tmdb_spider.py and run the following lines\nTo run the code in the terminal we need to run this line with the correct added subdir for the movie page we chose\n“scrapy crawl tmdb_spider -o movies.csv -a subdir=”\n\n\nNext we will create and implemnt three diffrent parsing methods to get the url for each actor page and then to scrape their page and collect a list of all the movies they have acted in\nOnce we have create the intial function to be able to explore the movie we chose for we need to add the correct subdir .\nWhen going to the hunger game movie page the url is as follow : https://www.themoviedb.org/movie/70160-the-hunger-games When running the code in our terminal we can also just write 70160 for the subdir instead of also inclduing the movie name\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n    '''\n     Initialize the TmdbSpider instance.\n     '''\n    \n\nIn this parse method we create a way to navigate to our cast and crew page by adding /cast to our orginal url. This can be more easily visualize by going to the movie webiste. Once you are there select the full cast and crew page and you will get redirected to a new page. If you look at the url you would notice that it now changed to :https://www.themoviedb.org/movie/70160-the-hunger-games/cast\n\ndef parse(self, response):\n    ''''\n    We start on a movie page, and then  navigate to the Full Cast & Crew page\n    ''''\n            # Navigate to the Full Cast & Crew page\n            full_cast_url = f\"{response.url + '/cast/'}\"\n            # Yield a scrapy.Request for the page of each actor listed on the Full Cast & Crew page\n            yield scrapy.Request(full_cast_url, callback=self.parse_full_credits)\n            \n            \n        \n  \n\nNow that we are at the full cast and crew page the nect step is to go to the actors main page where we can get a list of all the Movies and TV shows they acted in. In order to understand how to find all the information we need we need to inspect the webpage we are at. When we inspect it we are able to use css or xpath to explore and navigate to the right urls. To locate the cast members we will use css to search within the h3 aspect. We will then go through the list of actors by examining ol. To get the actors url we ill examine the href attribute under the div class info\n\n  def parse_full_credits(self, response):\n        '''\n        Following the previous parse we start off at the main page with all the list of the cast and crew \n        \n        We will navigate to get a list of all the actors in The Hunger Games movie \n        \n        This will yiled a request that will get us to each of the actors main page where we \n        could get a list of the movies and TV shows they have been in \n        '''\n        actors_url=response.css('h3:contains(\"Cast\") + ol')\n        #actor_name=actors_url.css(\":: text\")\n        list_a=actors_url.css(\"div.info a::attr(href)\").getall()\n        \n        for url in list_a:\n            full_url = response.urljoin(url)\n            \n            yield scrapy.Request(full_url,callback=self.parse_actor_page) \n           \n\nNow that we are at the actors webapge we will use css to get a list of all the movies they were in. We also used the h2 element to collect all the actors name that were in the title of each of their webpage. Once we collect all the information we created a for loop that yoield a dictionary with the name of the actor and the movies and TV shows that they have been in.\n\ndef parse_actor_page(self, response):\n        \n        '''\n        In this section we assume that we are starting in the actore webapge\n    \n        We will be grabbing and formulating a list of all the movies they were in \n        \n        '''\n        # get the actor name, in this case we located in the title of the page \n        actor_name=response.css('h2.title &gt; a::text').get()\n        # this lines grabs the list of all the movies they have been in \n    \n        card_credit=response.xpath('//h3[text()=\"Acting\"]/following-sibling::table[1]')\n        movie_id=card_credit.css('td.role.true.account_adult_false.item_adult_false &gt; a.tooltip &gt; bdi::text').getall()\n        \n         #This will go through the list of all the movies we have collected and create a dictionary with all the movies and actors \n       \n        for movie in movie_id:\n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie}\n       \n\n\n\n\nOnce we have finished writing all the parsing for the class TmdbSpider we will run the following line with the correct subdir to create a .csv file for all of our actors and moves/ TV they have been in.\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=70160-the-hunger-games"
  },
  {
    "objectID": "TMDB_scraper/hw2blog.html#first-step",
    "href": "TMDB_scraper/hw2blog.html#first-step",
    "title": "HW 2, Web Scarpping",
    "section": "",
    "text": "Locate the Url that you would be scrapping information from. For this blog we will use this website https://www.themoviedb.org/ and will scarpe a list of actors from the Hunger Game to create recomandtion system that gives users a list of the other movies the actors played in. \nWe First want to make sure that we are in the right enviroment and create a TMDB_scraper folder with all the files we will need by running the following lines in your terminal.\n\nPIC16B-24W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper"
  },
  {
    "objectID": "TMDB_scraper/hw2blog.html#second-step",
    "href": "TMDB_scraper/hw2blog.html#second-step",
    "title": "HW 2, Web Scarpping",
    "section": "",
    "text": "#### Part 1 We create a new file inside the spider directory and named it tmdb_spider.py and run the following lines\nTo run the code in the terminal we need to run this line with the correct added subdir for the movie page we chose\n“scrapy crawl tmdb_spider -o movies.csv -a subdir=”\n\n\nNext we will create and implemnt three diffrent parsing methods to get the url for each actor page and then to scrape their page and collect a list of all the movies they have acted in\nOnce we have create the intial function to be able to explore the movie we chose for we need to add the correct subdir .\nWhen going to the hunger game movie page the url is as follow : https://www.themoviedb.org/movie/70160-the-hunger-games When running the code in our terminal we can also just write 70160 for the subdir instead of also inclduing the movie name\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n    '''\n     Initialize the TmdbSpider instance.\n     '''\n    \n\nIn this parse method we create a way to navigate to our cast and crew page by adding /cast to our orginal url. This can be more easily visualize by going to the movie webiste. Once you are there select the full cast and crew page and you will get redirected to a new page. If you look at the url you would notice that it now changed to :https://www.themoviedb.org/movie/70160-the-hunger-games/cast\n\ndef parse(self, response):\n    ''''\n    We start on a movie page, and then  navigate to the Full Cast & Crew page\n    ''''\n            # Navigate to the Full Cast & Crew page\n            full_cast_url = f\"{response.url + '/cast/'}\"\n            # Yield a scrapy.Request for the page of each actor listed on the Full Cast & Crew page\n            yield scrapy.Request(full_cast_url, callback=self.parse_full_credits)\n            \n            \n        \n  \n\nNow that we are at the full cast and crew page the nect step is to go to the actors main page where we can get a list of all the Movies and TV shows they acted in. In order to understand how to find all the information we need we need to inspect the webpage we are at. When we inspect it we are able to use css or xpath to explore and navigate to the right urls. To locate the cast members we will use css to search within the h3 aspect. We will then go through the list of actors by examining ol. To get the actors url we ill examine the href attribute under the div class info\n\n  def parse_full_credits(self, response):\n        '''\n        Following the previous parse we start off at the main page with all the list of the cast and crew \n        \n        We will navigate to get a list of all the actors in The Hunger Games movie \n        \n        This will yiled a request that will get us to each of the actors main page where we \n        could get a list of the movies and TV shows they have been in \n        '''\n        actors_url=response.css('h3:contains(\"Cast\") + ol')\n        #actor_name=actors_url.css(\":: text\")\n        list_a=actors_url.css(\"div.info a::attr(href)\").getall()\n        \n        for url in list_a:\n            full_url = response.urljoin(url)\n            \n            yield scrapy.Request(full_url,callback=self.parse_actor_page) \n           \n\nNow that we are at the actors webapge we will use css to get a list of all the movies they were in. We also used the h2 element to collect all the actors name that were in the title of each of their webpage. Once we collect all the information we created a for loop that yoield a dictionary with the name of the actor and the movies and TV shows that they have been in.\n\ndef parse_actor_page(self, response):\n        \n        '''\n        In this section we assume that we are starting in the actore webapge\n    \n        We will be grabbing and formulating a list of all the movies they were in \n        \n        '''\n        # get the actor name, in this case we located in the title of the page \n        actor_name=response.css('h2.title &gt; a::text').get()\n        # this lines grabs the list of all the movies they have been in \n    \n        card_credit=response.xpath('//h3[text()=\"Acting\"]/following-sibling::table[1]')\n        movie_id=card_credit.css('td.role.true.account_adult_false.item_adult_false &gt; a.tooltip &gt; bdi::text').getall()\n        \n         #This will go through the list of all the movies we have collected and create a dictionary with all the movies and actors \n       \n        for movie in movie_id:\n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie}\n       \n\n\n\n\nOnce we have finished writing all the parsing for the class TmdbSpider we will run the following line with the correct subdir to create a .csv file for all of our actors and moves/ TV they have been in.\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=70160-the-hunger-games"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/Hw1/Untitled1.html",
    "href": "posts/Hw1/Untitled1.html",
    "title": "Myblog",
    "section": "",
    "text": "import sqlite3\nimport pandas as pd\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    conn=sqlite3.connect(db_file) \n    c = conn.cursor()\n\n    # Construct the SQL query using f-strings\n    sql_query = f\"\"\"\n        SELECT S.NAME AS Name, S.LATITUDE AS Latitude, S.LONGITUDE AS Longitude, C.Name AS Name, \n               T.Year, T.Month,T.Temp\n        FROM temperatures T\n        LEFT JOIN stations S ON T.ID = S.ID\n        LEFT JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.'FIPS 10-4'\n        WHERE C.Name = '{country}'\n          AND T.Year BETWEEN {year_begin} AND {year_end}\n          AND T.Month = {month}\n        GROUP BY S.NAME, T.Year, T.Month\n    \"\"\"\n\n    # Execute the SQL query and fetch the results into a DataFrame\n    df = pd.read_sql_query(sql_query, conn)\n\n    # Close the database connection\n    conn.close()\n\n    return df\n\n\nquery_climate_database(db_file = \"your_database_file.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\nDatabaseError: Execution failed on sql '\n        SELECT S.NAME AS Name, S.LATITUDE AS Latitude, S.LONGITUDE AS Longitude, C.Name AS Name, \n               T.Year, T.Month,T.Temp\n        FROM temperatures T\n        LEFT JOIN stations S ON T.ID = S.ID\n        LEFT JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.'FIPS 10-4'\n        WHERE C.Name = 'India'\n          AND T.Year BETWEEN 1980 AND 2020\n          AND T.Month = 1\n        GROUP BY S.NAME, T.Year, T.Month\n    ': no such table: temperatures"
  },
  {
    "objectID": "posts/HW 3/Index.html",
    "href": "posts/HW 3/Index.html",
    "title": "HW 3, Webapp",
    "section": "",
    "text": "HW3: Creating a Webapp\n\nIn this blog post I will go over how to develop and create your own webapp\n\n\nTo start I created a github repository where I placed all my files, so I could easily commit and push any changes I have made\n\n\nStep 1: Creating the HTML files\n\nTo first start I created three HTML files, base.html, submit.html, and view.html. each one of them fullfill a diffrent porpose that I will explain below.\n\n\nBase.html\nThis HTML code sets up the main page that opens up when you open the URL. Within the first page, you are displayed with the two links, to either see the message or to submit a message, as well as a list of random messages that have been submitted previously.\n&lt;!doctype html&gt; \n\n{% block title %}{% endblock %} - PIC16B Website\n\n\n/* this section changes the diffrent colors of the title*/\n\n\nWelcome to My Webapp!\n\n\nPlease follow the submit a message link to submit your message.\n\n\n\n/* this displays the two links on the main page, one to submit a message, and the second one to view the message*/\n\nSubmit a Message\n\n\nView Messages\n\n\n\n\n\n{% block header %}{% endblock %}\n\n{% block content %}{% endblock %}\n\n\n/* this is a secondary header that will show a list of the random messages */\n\nRandom Messages:\n\n\n\n{% for message in messages %} \n\n{{ message[1] }}: {{ message[2] }}\n\n{% endfor %}\n\n\n\n\n\nSubmit.html\nThis html takes care of the page in which indvdiuals submit their message. They are asked to provide their name and submit a message. Once submitting, they can then also chose to see any previous message or be directed to main page.\n{% extends ‘base.html’ %}\n{% block header %}\n\n{% block title %}Submit a Message{% endblock %}\n\n{% endblock %}\n{% block content %}  /* this sets up the two links for people to either view the message or go back to the first main page */\n\nReturn to main page\n\nView Messages\n\n /* this will change the header to blue and the submit botton to green*/\n\n/* this section prompts the users to write their name and leave a message, and creates the submit botton*/\n\n&lt;label for-“name”&gt; Your name.  Please write a message.  \n\n\n{% endblock %}\n\n\nView.html\nThis last html presents the messages that have been submmited and orders them inside a table so it is easier to view past messages.\n&lt;!doctype html&gt;\n\n\n\nBelow are your messages:\n\n\n\n\n/* this for loop goes through all the messages and organizes them inside a table */ {% for message in messages %}\n\n{% endfor %}\n\n\n\n\nID\n\n\nName\n\n\nMessage\n\n\n\n{{ message[0] }}\n\n\n{{ message[1] }}\n\n\n{{ message[2] }}\n\n\n/* this creats a link that allows people to go back and submit another message*/\n\nsubmit another message\n\n\n\n\n\n\nStep 2: Creat functions for data base mangment\n\nFirst we need to import all the packages we might be using\nfrom flask import Flask, g, render_template, request import sqlite3\nimport sklearn as sk import matplotlib.pyplot as plt import numpy as np import pickle import os from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas from matplotlib.figure import Figure\nimport io import base64\nFirst we start by creating a datavase and naming it as well as intializing flask applications. The base function retrives 3 random messages and renders the base.html templetes, which also passes the messages to be displayed on the main page. The submit function tkaes the POST commend to add the message into the database. The submit.html templet is then rendered to prompt users to submit a message. Lastly, the message function retrives all the messages that were added to the database amd renders the view.html template which displays the messages to the users.\n““” This script contains routes and functions for a Flask web application.\nIt defines routes for the main page, message submission, and viewing messages. ““”\nDATABASE = ‘messages_db.sqlite’ app = Flask(name)\n““” Renders the main page of the web application.\nRetrieves 3 random messages and renders the 'base.html' template with these messages.\n\"\"\"\n@app.route(‘/’) def base(): messages = random_messages(3) # Retrieve 3 random messages return render_template(‘base.html’, messages=messages)\n““” Handles message submission.\nIf the request method is POST, it inserts the message into the database.\nRenders the 'submit.html' template.\n““” @app.route(‘/submit’, methods=[‘GET’,‘POST’]) def submit(): if request.method == ‘POST’: insert_message(request) return render_template(‘submit.html’)\n““” Displays all messages stored in the database.\nRetrieves all messages from the database and renders the 'view.html' template with these messages.\n““” @app.route(‘/message’) def message(): db = get_message_db() cursor = db.cursor() cursor.execute(“SELECT * FROM messages”) messages = cursor.fetchall() cursor.close() return render_template(‘view.html’, messages=messages)\nThis function then retrives trhe SQLite database connection used for storing the messages that were submitted by the users.\n\n# Function to get the message database\n\"\"\"\n    Retrieve the SQLite database connection for storing messages.\n\n    If the 'message_db' attribute is not present in the global 'g' object, \n    it creates a new connection to the SQLite database defined by the \n    'DATABASE' constant. It also creates a 'messages' table in the database \n    if it doesn't already exist.\n\n    Returns:\n    - sqlite3.Connection: The SQLite database connection object.\n    \"\"\"\ndef get_message_db():\n    if 'message_db' not in g:\n        g.message_db = sqlite3.connect(DATABASE)\n        cursor = g.message_db.cursor()\n        cursor.execute('''CREATE TABLE IF NOT EXISTS messages (\n                            id INTEGER PRIMARY KEY,\n                            handle TEXT,\n                            message TEXT\n                        )''')\n        g.message_db.commit()\n    return g.message_db\n    \n\n\nWe then need to create a function that inserts the message into the database.It retracts the name of the user and their message from the form data using the request.form[‘nm’] and request.form[‘message’].Then, it executes an SQL query to insert the name and message into the ‘messages’ table in the database.\n\n\n\n\nFunction to insert a user message into the database\ndef insert_message(request): ““” Insert a user message into the database.\nArgs:\n- request (flask.request): The request object containing form data.\n\nReturns:\n- str: The message that was inserted into the database.\n\"\"\"\ndb = get_message_db()\ncursor = db.cursor()\nname = request.form['nm']\nmessage = request.form['message']\ncursor.execute(\"INSERT INTO messages (handle, message) VALUES (?, ?)\", (name, message))\ndb.commit()\ncursor.close()\nreturn message\nLastly, the last two functions display the page that presents the list of random messages and randomizes the message that are being presented.\n\n@app.route('/view_messages')\ndef view_messages():\n     \"\"\"\n    Display a page showing a random selection of messages.\n\n    Returns:\n    - flask.render_template: HTML page displaying the random messages.\n    \"\"\"\n    messages = random_messages(5)  # You can change the number of messages to retrieve\n    return render_template('view.html', messages=messages)\n\n# Function to retrieve n random messages from the database\ndef random_messages(n):\n    \"\"\"\n    Retrieve n random messages from the database.\n\n    Args:\n    - n (int): Number of random messages to retrieve.\n\n    Returns:\n    - list: A list of tuples containing the retrieved messages.\n    \"\"\"\n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute(\"SELECT * FROM messages ORDER BY RANDOM() LIMIT ?\", (n,))\n    messages = cursor.fetchall()\n    cursor.close()\n    db.close()  # Close the database connection\n    return messages\n\n\nOnce we have wrote all the code and saved all the files we then run the following code in the terminal to publish the webapp and get the url for it\nset FLASK_ENV=app.py\nflask run\n\n\nHere is what my webapp looks like\n\nFirst is the main page the user sees when opening the URL\n\n\n\ncolor main.png\n\n\nOnce opening the submit a message url you are prompted to this page\n\n\n\ncolor messahe.png\n\n\nlastly, here is the list of message you can view once opining the view messages link\n\n\n\ncolor message listy.png"
  },
  {
    "objectID": "posts/bruin/index.html",
    "href": "posts/bruin/index.html",
    "title": "Creating posts",
    "section": "",
    "text": "### Import all the data and plotly libary\n\nimport plotly.express as px \nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\ndf= penguins \n# changed the name of data frame to make it easier to work with\n\n\n# in order to isnpect the data I printed out the first five rows so I could see the variables I need to work with \ndf.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\ndf=df[df.Sex.notnull()]\n\ncolumes=(['Species','Region', 'Island', 'Sex', 'Culmen Length (mm)','Culmen Depth (mm)','Flipper Length (mm)', 'Body Mass (g)'])\ndfnew=df[columes]\n# this creats a new data fram with the selected columes that I want to be analyzing in my visualization \n\n\ndfnew.head()\n\n\n\n\n\n\n\n\nSpecies\nRegion\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfig=px.scatter(df,x='Flipper Length (mm)',\n               y='Body Mass (g)',\n              color='Species',\n              title='Flipper Length vs Body Mass')\n# this creats a scatter plot that represents the X and Y varaibles as well as distingues speices by color \nfig.show()"
  },
  {
    "objectID": "posts/bruin/index.html#hw0",
    "href": "posts/bruin/index.html#hw0",
    "title": "Creating posts",
    "section": "",
    "text": "### Import all the data and plotly libary\n\nimport plotly.express as px \nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\ndf= penguins \n# changed the name of data frame to make it easier to work with\n\n\n# in order to isnpect the data I printed out the first five rows so I could see the variables I need to work with \ndf.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\ndf=df[df.Sex.notnull()]\n\ncolumes=(['Species','Region', 'Island', 'Sex', 'Culmen Length (mm)','Culmen Depth (mm)','Flipper Length (mm)', 'Body Mass (g)'])\ndfnew=df[columes]\n# this creats a new data fram with the selected columes that I want to be analyzing in my visualization \n\n\ndfnew.head()\n\n\n\n\n\n\n\n\nSpecies\nRegion\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfig=px.scatter(df,x='Flipper Length (mm)',\n               y='Body Mass (g)',\n              color='Species',\n              title='Flipper Length vs Body Mass')\n# this creats a scatter plot that represents the X and Y varaibles as well as distingues speices by color \nfig.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Myblog",
    "section": "",
    "text": "HW 3, Webapp\n\n\n\n\n\n\n\nweek 6\n\n\nHW 3\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2024\n\n\nNeomi Goodman\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nNeomi Goodman\n\n\n\n\n\n\n  \n\n\n\n\nHW 2, Web Scarpping\n\n\n\n\n\n\n\nweek 5\n\n\nHW 2\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nNeomi Goodman\n\n\n\n\n\n\n  \n\n\n\n\nHW 1, Database\n\n\n\n\n\n\n\nweek 4\n\n\nHW 1\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2023\n\n\nNeomi Goodman\n\n\n\n\n\n\n  \n\n\n\n\nCreating posts\n\n\n\n\n\n\n\nweek 0\n\n\nHW 0\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2023\n\n\nNeomi Goodman\n\n\n\n\n\n\n  \n\n\n\n\nCreating posts\n\n\n\n\n\n\n\nweek 0\n\n\nHW 0\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2023\n\n\nNeomi Goodman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/bruin/Index1.html",
    "href": "posts/bruin/Index1.html",
    "title": "Creating posts",
    "section": "",
    "text": "import plotly.express as px \nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\ndf= penguins \n# changed the name of data frame to make it easier to work with\n\n\n# in order to isnpect the data I printed out the first five rows so I could see the variables I need to work with \ndf.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf=df[df.Sex.notnull()]\n\ncolumes=(['Species','Region', 'Island', 'Sex', 'Culmen Length (mm)','Culmen Depth (mm)','Flipper Length (mm)', 'Body Mass (g)'])\ndfnew=df[columes]\n# this creats a new data fram with the selected columes that I want to be analyzing in my visualization \n\n\ndfnew.head()\n\n\n\n\n\n\n\n\nSpecies\nRegion\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfig=px.scatter(df,x='Flipper Length (mm)',\n               y='Body Mass (g)',\n              color='Species',\n              title='Flipper Length vs Body Mass')\n# this creats a scatter plot that represents the X and Y varaibles as well as distingues speices by color \nfig.show()"
  },
  {
    "objectID": "posts/bruin/Index1.html#hw0",
    "href": "posts/bruin/Index1.html#hw0",
    "title": "Creating posts",
    "section": "",
    "text": "import plotly.express as px \nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\ndf= penguins \n# changed the name of data frame to make it easier to work with\n\n\n# in order to isnpect the data I printed out the first five rows so I could see the variables I need to work with \ndf.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf=df[df.Sex.notnull()]\n\ncolumes=(['Species','Region', 'Island', 'Sex', 'Culmen Length (mm)','Culmen Depth (mm)','Flipper Length (mm)', 'Body Mass (g)'])\ndfnew=df[columes]\n# this creats a new data fram with the selected columes that I want to be analyzing in my visualization \n\n\ndfnew.head()\n\n\n\n\n\n\n\n\nSpecies\nRegion\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfig=px.scatter(df,x='Flipper Length (mm)',\n               y='Body Mass (g)',\n              color='Species',\n              title='Flipper Length vs Body Mass')\n# this creats a scatter plot that represents the X and Y varaibles as well as distingues speices by color \nfig.show()"
  },
  {
    "objectID": "posts/Hw001/index.html",
    "href": "posts/Hw001/index.html",
    "title": "HW 1, Database",
    "section": "",
    "text": "# import all packages I might need \nimport pandas as pd \nimport sqlite3\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom plotly.io import write_html\nimport plotly.express as px\n\n\n\n\n\n# create a database called db_file \nconn= sqlite3.connect(\"db_file\")\n\n\n\n\n\ndf = pd.read_csv(\"temps.csv\")\ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\n\n\n#clean and prepare the data \ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\n\ndf = prepare_df(df)\n\n\n#creat a table with the temperature data\nfor i, df in enumerate(df_iter):\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\n\n#creats a table with all the stations \nurl = \"station-metadata.csv\"\nstations = pd.read_csv(url)\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n\n27585\n\n\n\n#creats a table with all the countries \ncountries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\ncountries= pd.read_csv(countries_url)\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n279\n\n\n\n\n\n\n#to make sure we have created the database with the three tables we wanted we print a list with our tables\ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('stations',), ('countries',)]\n\n\n\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\n\n\n\n\ncmd = \\\n\"\"\"\nSELECT T.id, T.month, T.temp\nFROM temperatures T\nLEFT JOIN countries C ON SUBSTR(T.id, 1, 2) = SUBSTR(C.\"FIPS 10-4\", 1, 2)\nLEFT JOIN stations S ON SUBSTR(T.id, 1, 2) = SUBSTR(S.id, 1, 2)\nWHERE C.Name IS NOT NULL\n\n\"\"\"\ncursor = conn.cursor()\ncursor.execute(cmd)\nresult = [cursor.fetchone() for i in range(10)]  # get just the first 10 results\nresult\n\n[('AFM00040911', 1, 2.99),\n ('AFM00040911', 1, 2.99),\n ('AFM00040911', 1, 2.99),\n ('AFM00040911', 1, 2.99),\n ('AFM00040911', 1, 2.99),\n ('AFM00040911', 1, 2.99),\n ('AFM00040911', 2, 8.09),\n ('AFM00040911', 2, 8.09),\n ('AFM00040911', 2, 8.09),\n ('AFM00040911', 2, 8.09)]\n\n\n\nconn.close()\n\n\n\n\n\n\n\n\n\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \n   # this connects it to the database we created earlier \n    conn= sqlite3.connect(db_file)\n    \n    query = f\"\"\"\n    SELECT S.NAME AS NAME, S.LATITUDE AS LATITUDE, S.LONGITUDE AS LONGITUDE, C.Name AS Country,\n           T.Year, T.Month, AVG(T.Temp) AS Temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.ID = S.ID\n    LEFT JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.'FIPS 10-4'\n    WHERE C.Name = '{country}'\n          AND T.Year BETWEEN {year_begin} AND {year_end}\n          AND T.Month = {month}\n   \"\"\"\n    \n    df = pd.read_sql_query(query, conn)\n    # this closes the database connction \n    conn.close()\n   \n    return df\n\n\nquery_climate_database(db_file = \"db_file\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n19.765926\n\n\n\n\n\n\n\n\n\n\n\n\n\n# I first created a function to calculate the yearly temp coefficient so I will not have to do this calculation within my next function and the work would be presented more clearly \n\ndef calc_temp_coefficient(data_group):\n    x = data_group[['Year']].values\n    y = data_group['Temp'].values\n    if len(x) &lt; 2:\n        return np.nan  # this is in case there is not enough data to caluclate the regression \n    lr = LinearRegression()\n    lr.fit(x, y)\n    return round(lr.coef_[0], 5) \n\n# a function that shows how the average yearly change in temperature vary within a given country\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs=10 ,**kwargs): \n    conn=sqlite3.connect(\"db_file\")\n    \n    # SQL query to get the temperature data for a the specified country in a year range and month\n    query = f\"\"\"\n    SELECT S.NAME AS Station_Name, S.LATITUDE AS Latitude, S.LONGITUDE AS Longitude,\n           C.Name AS Country, T.Year, T.Month, AVG(T.Temp) AS Temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.ID = S.ID\n    LEFT JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.'FIPS 10-4'\n    WHERE C.Name = '{country}'\n          AND T.Year BETWEEN {year_begin} AND {year_end}\n          AND T.Month = {month}\n    GROUP BY S.NAME, T.Year, T.Month\n    \"\"\"\n  \n    # I used .read_sql_querey to read the dtatbase datat and store it as Pandas DataFrame\n    df = pd.read_sql_query(query, conn)\n    \n    # filter stations with at least min_obs years of data value and group them by the station ID \n    df = df.groupby('Station_Name').filter(lambda x: x['Year'].count() &gt;= min_obs)\n    \n   \n     # Calculate the estimated yearly temperature increase for each station\n    yearly_changes = df.groupby('Station_Name').apply(\n        lambda group: calc_temp_coefficient(group)\n    ).reset_index(name='Estimated Yearly Increase (°C)')\n    \n    \n     # Create a Plotly Express scatter mapbox plot\n    fig = px.scatter_mapbox(\n        df,\n        lat='Latitude',\n        lon='Longitude',\n        hover_name='Station_Name',\n        color='Temp',  # Use the calculated column as color\n        height=600,\n        **kwargs\n    )\n    \n    # Set layout options and styling for the plot\n    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n    \n    # Return the figure\n    return fig\n\n\n\n\n\n\n\nfrom climate_database import query_climate_database\nfrom climate_database import calc_temp_coefficient\nfrom climate_database import temperature_coefficient_plot\n\nimport inspect\n#print(inspect.getsource(query_climate_database))\n#print(inspect.getsource(calc_temp_coefficient))\n#print(inspect.getsource(temperature_coefficient_plot))\n\n\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file=\"db_file\",country=\"India\",year_begin=1980, year_end=2020, month=1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map,\n                                   title=\"Estimates of the Yearly Temperature increase in India \")\n\n\nfig.show()\n\nValueError: Value of 'color' is not the name of a column in 'data_frame'. Expected one of ['Station_Name', 'Latitude', 'Longitude', 'Country', 'Year', 'Month', 'Temp'] but received: EYI\n\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file=\"db_file\",country=\"France\",year_begin=1980, year_end=2020, month=9, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map,\n                                   title=\"Estimates of the Yearly Temperature increase in France\")\n\nfig.show()\n\n\n\n\n\n\n\n\n\nfrom climate_database import analyze_seasonal_temperature_change\n\n\nimport inspect\nprint(inspect.getsource(analyze_seasonal_temperature_change))\n\n\ndb_file = \"db_file\"\n\n# Define the year range\nyear_range = (2000, 2020)\n\n# Define the country (optional)\ncountry = \"USA\"\n\n# Call the function\nanalyze_seasonal_temperature_change(db_file, year_range)\n\n\n\n\n\nimport plotly.express as px\n\ndef visualize_avg_monthly_temperature_by_country(df):\n    fig = px.line(df, x='Month', y='Avg_Temperature', title='Average Monthly Temperature by Country')\n    return fig\n\ndef get_data_from_database(db_file):\n    conn = sqlite3.connect(db_file)\n    query = \"SELECT * FROM temperatures\"  # Adjust your SQL query accordingly\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n    return df\n\n\nmy_df = get_data_from_database(\"db_file\")\nfig = visualize_avg_monthly_temperature_by_country(my_df)\n\n\ndef visualize_monthly_temperature_trends_for_station(df):\n    fig = px.line(df, x='Month', y='Temp', color='Year', title='Monthly Temperature Trends for a Specific Station')\n    return fig"
  },
  {
    "objectID": "posts/Hw001/index.html#part-2-create-a-query-function",
    "href": "posts/Hw001/index.html#part-2-create-a-query-function",
    "title": "HW 1, Database",
    "section": "",
    "text": "def query_climate_database(db_file, country, year_begin, year_end, month):\n    \n   # this connects it to the database we created earlier \n    conn= sqlite3.connect(db_file)\n    \n    query = f\"\"\"\n    SELECT S.NAME AS NAME, S.LATITUDE AS LATITUDE, S.LONGITUDE AS LONGITUDE, C.Name AS Country,\n           T.Year, T.Month, AVG(T.Temp) AS Temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.ID = S.ID\n    LEFT JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.'FIPS 10-4'\n    WHERE C.Name = '{country}'\n          AND T.Year BETWEEN {year_begin} AND {year_end}\n          AND T.Month = {month}\n   \"\"\"\n    \n    df = pd.read_sql_query(query, conn)\n    # this closes the database connction \n    conn.close()\n   \n    return df\n\n\nquery_climate_database(db_file = \"db_file\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n19.765926"
  },
  {
    "objectID": "posts/Hw001/index.html#part-3-create-an-interactive-viusalization",
    "href": "posts/Hw001/index.html#part-3-create-an-interactive-viusalization",
    "title": "HW 1, Database",
    "section": "",
    "text": "# I first created a function to calculate the yearly temp coefficient so I will not have to do this calculation within my next function and the work would be presented more clearly \n\ndef calc_temp_coefficient(data_group):\n    x = data_group[['Year']].values\n    y = data_group['Temp'].values\n    if len(x) &lt; 2:\n        return np.nan  # this is in case there is not enough data to caluclate the regression \n    lr = LinearRegression()\n    lr.fit(x, y)\n    return round(lr.coef_[0], 5) \n\n# a function that shows how the average yearly change in temperature vary within a given country\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs=10 ,**kwargs): \n    conn=sqlite3.connect(\"db_file\")\n    \n    # SQL query to get the temperature data for a the specified country in a year range and month\n    query = f\"\"\"\n    SELECT S.NAME AS Station_Name, S.LATITUDE AS Latitude, S.LONGITUDE AS Longitude,\n           C.Name AS Country, T.Year, T.Month, AVG(T.Temp) AS Temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.ID = S.ID\n    LEFT JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.'FIPS 10-4'\n    WHERE C.Name = '{country}'\n          AND T.Year BETWEEN {year_begin} AND {year_end}\n          AND T.Month = {month}\n    GROUP BY S.NAME, T.Year, T.Month\n    \"\"\"\n  \n    # I used .read_sql_querey to read the dtatbase datat and store it as Pandas DataFrame\n    df = pd.read_sql_query(query, conn)\n    \n    # filter stations with at least min_obs years of data value and group them by the station ID \n    df = df.groupby('Station_Name').filter(lambda x: x['Year'].count() &gt;= min_obs)\n    \n   \n     # Calculate the estimated yearly temperature increase for each station\n    yearly_changes = df.groupby('Station_Name').apply(\n        lambda group: calc_temp_coefficient(group)\n    ).reset_index(name='Estimated Yearly Increase (°C)')\n    \n    \n     # Create a Plotly Express scatter mapbox plot\n    fig = px.scatter_mapbox(\n        df,\n        lat='Latitude',\n        lon='Longitude',\n        hover_name='Station_Name',\n        color='Temp',  # Use the calculated column as color\n        height=600,\n        **kwargs\n    )\n    \n    # Set layout options and styling for the plot\n    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n    \n    # Return the figure\n    return fig\n\n\n\n\n\n\n\nfrom climate_database import query_climate_database\nfrom climate_database import calc_temp_coefficient\nfrom climate_database import temperature_coefficient_plot\n\nimport inspect\n#print(inspect.getsource(query_climate_database))\n#print(inspect.getsource(calc_temp_coefficient))\n#print(inspect.getsource(temperature_coefficient_plot))\n\n\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file=\"db_file\",country=\"India\",year_begin=1980, year_end=2020, month=1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map,\n                                   title=\"Estimates of the Yearly Temperature increase in India \")\n\n\nfig.show()\n\nValueError: Value of 'color' is not the name of a column in 'data_frame'. Expected one of ['Station_Name', 'Latitude', 'Longitude', 'Country', 'Year', 'Month', 'Temp'] but received: EYI\n\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file=\"db_file\",country=\"France\",year_begin=1980, year_end=2020, month=9, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map,\n                                   title=\"Estimates of the Yearly Temperature increase in France\")\n\nfig.show()"
  },
  {
    "objectID": "posts/Hw001/index.html#part-4-creating-two-additional-visuals",
    "href": "posts/Hw001/index.html#part-4-creating-two-additional-visuals",
    "title": "HW 1, Database",
    "section": "",
    "text": "from climate_database import analyze_seasonal_temperature_change\n\n\nimport inspect\nprint(inspect.getsource(analyze_seasonal_temperature_change))\n\n\ndb_file = \"db_file\"\n\n# Define the year range\nyear_range = (2000, 2020)\n\n# Define the country (optional)\ncountry = \"USA\"\n\n# Call the function\nanalyze_seasonal_temperature_change(db_file, year_range)\n\n\n\n\n\nimport plotly.express as px\n\ndef visualize_avg_monthly_temperature_by_country(df):\n    fig = px.line(df, x='Month', y='Avg_Temperature', title='Average Monthly Temperature by Country')\n    return fig\n\ndef get_data_from_database(db_file):\n    conn = sqlite3.connect(db_file)\n    query = \"SELECT * FROM temperatures\"  # Adjust your SQL query accordingly\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n    return df\n\n\nmy_df = get_data_from_database(\"db_file\")\nfig = visualize_avg_monthly_temperature_by_country(my_df)\n\n\ndef visualize_monthly_temperature_trends_for_station(df):\n    fig = px.line(df, x='Month', y='Temp', color='Year', title='Monthly Temperature Trends for a Specific Station')\n    return fig"
  },
  {
    "objectID": "posts/HW2/index.html",
    "href": "posts/HW2/index.html",
    "title": "HW 2, Web Scarpping",
    "section": "",
    "text": "In this blog we will go through on how to scrap a website. In this example we will formulate a list of all the actors who acted in the first Hunger Game movie and create a data fram with all the other movies in which they acted in to help recomand new movies to others.\n\n\nLocate the Url that you would be scrapping information from. For this blog we will use this website https://www.themoviedb.org/ and will scarpe a list of actors from the Hunger Game to create recomandtion system that gives users a list of the other movies the actors played in. \nWe First want to make sure that we are in the right enviroment and create a TMDB_scraper folder with all the files we will need by running the following lines in your terminal.\n\nPIC16B-24W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\n\n\n\n\n#### Part 1 We create a new file inside the spider directory and named it tmdb_spider.py and run the following lines\nTo run the code in the terminal we need to run this line with the correct added subdir for the movie page we chose\n“scrapy crawl tmdb_spider -o movies.csv -a subdir=”\n\n\nNext we will create and implemnt three diffrent parsing methods to get the url for each actor page and then to scrape their page and collect a list of all the movies they have acted in\nOnce we have create the intial function to be able to explore the movie we chose for we need to add the correct subdir .\nWhen going to the hunger game movie page the url is as follow : https://www.themoviedb.org/movie/70160-the-hunger-games When running the code in our terminal we can also just write 70160 for the subdir instead of also inclduing the movie name\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n    '''\n     Initialize the TmdbSpider instance.\n     '''\n    \n\nIn this parse method we create a way to navigate to our cast and crew page by adding /cast to our orginal url. This can be more easily visualize by going to the movie webiste. Once you are there select the full cast and crew page and you will get redirected to a new page. If you look at the url you would notice that it now changed to :https://www.themoviedb.org/movie/70160-the-hunger-games/cast\n\ndef parse(self, response):\n    ''''\n    We start on a movie page, and then  navigate to the Full Cast & Crew page\n    ''''\n            # Navigate to the Full Cast & Crew page\n            full_cast_url = f\"{response.url + '/cast/'}\"\n            # Yield a scrapy.Request for the page of each actor listed on the Full Cast & Crew page\n            yield scrapy.Request(full_cast_url, callback=self.parse_full_credits)\n            \n            \n        \n  \n\nNow that we are at the full cast and crew page the nect step is to go to the actors main page where we can get a list of all the Movies and TV shows they acted in. In order to understand how to find all the information we need we need to inspect the webpage we are at. When we inspect it we are able to use css or xpath to explore and navigate to the right urls. To locate the cast members we will use css to search within the h3 aspect. We will then go through the list of actors by examining ol. To get the actors url we ill examine the href attribute under the div class info\n\n  def parse_full_credits(self, response):\n        '''\n        Following the previous parse we start off at the main page with all the list of the cast and crew \n        \n        We will navigate to get a list of all the actors in The Hunger Games movie \n        \n        This will yiled a request that will get us to each of the actors main page where we \n        could get a list of the movies and TV shows they have been in \n        '''\n        actors_url=response.css('h3:contains(\"Cast\") + ol')\n        #actor_name=actors_url.css(\":: text\")\n        list_a=actors_url.css(\"div.info a::attr(href)\").getall()\n        \n        for url in list_a:\n            full_url = response.urljoin(url)\n            \n            yield scrapy.Request(full_url,callback=self.parse_actor_page) \n           \n\nNow that we are at the actors webapge we will use css to get a list of all the movies they were in. We also used the h2 element to collect all the actors name that were in the title of each of their webpage. Once we collect all the information we created a for loop that yoield a dictionary with the name of the actor and the movies and TV shows that they have been in.\n\ndef parse_actor_page(self, response):\n        \n        '''\n        In this section we assume that we are starting in the actore webapge\n    \n        We will be grabbing and formulating a list of all the movies they were in \n        \n        '''\n        # get the actor name, in this case we located in the title of the page \n        actor_name=response.css('h2.title &gt; a::text').get()\n        # this lines grabs the list of all the movies they have been in \n    \n        card_credit=response.xpath('//h3[text()=\"Acting\"]/following-sibling::table[1]')\n        movie_id=card_credit.css('td.role.true.account_adult_false.item_adult_false &gt; a.tooltip &gt; bdi::text').getall()\n        \n         #This will go through the list of all the movies we have collected and create a dictionary with all the movies and actors \n       \n        for movie in movie_id:\n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie}\n       \n\n\n\n\nOnce we have finished writing all the parsing for the class TmdbSpider we will run the following line with the correct subdir to create a .csv file for all of our actors and moves/ TV they have been in.\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=70160-the-hunger-games"
  },
  {
    "objectID": "posts/HW2/index.html#first-step",
    "href": "posts/HW2/index.html#first-step",
    "title": "HW 2, Web Scarpping",
    "section": "",
    "text": "Locate the Url that you would be scrapping information from. For this blog we will use this website https://www.themoviedb.org/ and will scarpe a list of actors from the Hunger Game to create recomandtion system that gives users a list of the other movies the actors played in. \nWe First want to make sure that we are in the right enviroment and create a TMDB_scraper folder with all the files we will need by running the following lines in your terminal.\n\nPIC16B-24W\nscrapy startproject TMDB_scraper\ncd TMDB_scraper"
  },
  {
    "objectID": "posts/HW2/index.html#second-step",
    "href": "posts/HW2/index.html#second-step",
    "title": "HW 2, Web Scarpping",
    "section": "",
    "text": "#### Part 1 We create a new file inside the spider directory and named it tmdb_spider.py and run the following lines\nTo run the code in the terminal we need to run this line with the correct added subdir for the movie page we chose\n“scrapy crawl tmdb_spider -o movies.csv -a subdir=”\n\n\nNext we will create and implemnt three diffrent parsing methods to get the url for each actor page and then to scrape their page and collect a list of all the movies they have acted in\nOnce we have create the intial function to be able to explore the movie we chose for we need to add the correct subdir .\nWhen going to the hunger game movie page the url is as follow : https://www.themoviedb.org/movie/70160-the-hunger-games When running the code in our terminal we can also just write 70160 for the subdir instead of also inclduing the movie name\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n    '''\n     Initialize the TmdbSpider instance.\n     '''\n    \n\nIn this parse method we create a way to navigate to our cast and crew page by adding /cast to our orginal url. This can be more easily visualize by going to the movie webiste. Once you are there select the full cast and crew page and you will get redirected to a new page. If you look at the url you would notice that it now changed to :https://www.themoviedb.org/movie/70160-the-hunger-games/cast\n\ndef parse(self, response):\n    ''''\n    We start on a movie page, and then  navigate to the Full Cast & Crew page\n    ''''\n            # Navigate to the Full Cast & Crew page\n            full_cast_url = f\"{response.url + '/cast/'}\"\n            # Yield a scrapy.Request for the page of each actor listed on the Full Cast & Crew page\n            yield scrapy.Request(full_cast_url, callback=self.parse_full_credits)\n            \n            \n        \n  \n\nNow that we are at the full cast and crew page the nect step is to go to the actors main page where we can get a list of all the Movies and TV shows they acted in. In order to understand how to find all the information we need we need to inspect the webpage we are at. When we inspect it we are able to use css or xpath to explore and navigate to the right urls. To locate the cast members we will use css to search within the h3 aspect. We will then go through the list of actors by examining ol. To get the actors url we ill examine the href attribute under the div class info\n\n  def parse_full_credits(self, response):\n        '''\n        Following the previous parse we start off at the main page with all the list of the cast and crew \n        \n        We will navigate to get a list of all the actors in The Hunger Games movie \n        \n        This will yiled a request that will get us to each of the actors main page where we \n        could get a list of the movies and TV shows they have been in \n        '''\n        actors_url=response.css('h3:contains(\"Cast\") + ol')\n        #actor_name=actors_url.css(\":: text\")\n        list_a=actors_url.css(\"div.info a::attr(href)\").getall()\n        \n        for url in list_a:\n            full_url = response.urljoin(url)\n            \n            yield scrapy.Request(full_url,callback=self.parse_actor_page) \n           \n\nNow that we are at the actors webapge we will use css to get a list of all the movies they were in. We also used the h2 element to collect all the actors name that were in the title of each of their webpage. Once we collect all the information we created a for loop that yoield a dictionary with the name of the actor and the movies and TV shows that they have been in.\n\ndef parse_actor_page(self, response):\n        \n        '''\n        In this section we assume that we are starting in the actore webapge\n    \n        We will be grabbing and formulating a list of all the movies they were in \n        \n        '''\n        # get the actor name, in this case we located in the title of the page \n        actor_name=response.css('h2.title &gt; a::text').get()\n        # this lines grabs the list of all the movies they have been in \n    \n        card_credit=response.xpath('//h3[text()=\"Acting\"]/following-sibling::table[1]')\n        movie_id=card_credit.css('td.role.true.account_adult_false.item_adult_false &gt; a.tooltip &gt; bdi::text').getall()\n        \n         #This will go through the list of all the movies we have collected and create a dictionary with all the movies and actors \n       \n        for movie in movie_id:\n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie}\n       \n\n\n\n\nOnce we have finished writing all the parsing for the class TmdbSpider we will run the following line with the correct subdir to create a .csv file for all of our actors and moves/ TV they have been in.\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=70160-the-hunger-games"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]